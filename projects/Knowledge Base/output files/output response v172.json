[
    {
        "items": [
            {
                "file": "PartVII Selected Topics/PartVII Introduction.md",
                "contents": "# Introduction\n\nThis part contains a selection of algorithmic topics that extend and complement earlier material in this book. Some chapters introduce new models of computation such as circuits or parallel computers. Others cover specialized domains such as matrices or number theory. The last two chapters discuss some of the known limitations to the design of efficient algorithms and introduce techniques for coping with those limitations.\n\nChapter 26 presents an algorithmic model for parallel computing based on task-parallel computing, and more specifically, fork-join parallelism. The chapter introduces the basics of the model, showing how to quantify parallelism in terms of the measures of work and span. It then investigates several interesting fork-join algorithms, including algorithms for matrix multiplication and merge sorting.\n\nAn algorithm that receives its input over time, rather than having the entire input available at the start, is called an \"online\" algorithm. Chapter 27 examines techniques used in online algorithms, starting with the \"toy\" problem of how long to wait for an elevator before taking the stairs. It then studies the \"move-to-front\" heuristic for maintaining a linked list and finishes with the online version of the caching problem we saw back in Section 15.4. The analyses of these online algorithms are remarkable in that they prove that these algorithms, which do not know their future inputs, perform within a constant factor of optimal algorithms that know the future inputs.\n\nChapter 28 studies efficient algorithms for operating on matrices. It presents two general methods\u2014LU decomposition and LUP decomposition\u2014for solving linear equations by Gaussian elimination in $O(n^3)$ time. It also shows that matrix inversion and matrix multiplication can be performed equally fast. The chapter concludes by showing how to compute a least-squares approximate solution when a set of linear equations has no exact solution.\n\nChapter 29 studies how to model problems as linear programs, where the goal is to maximize or minimize an objective, given limited resources and competing constraints. Linear programming arises in a variety of practical application areas. The chapter also addresses the concept of \"duality\" which, by establishing that a maximization problem and minimization problem have the same objective value, helps to show that solutions to each are optimal.\n\nChapter 30 studies operations on polynomials and shows how to use a well-known signal-processing technique\u2014the fast Fourier transform (FFT)\u2014to multiply two degree-n polynomials in $O(n \\lg n)$ time. It also derives a parallel circuit to compute the FFT.\n\nChapter 31 presents number-theoretic algorithms. After reviewing elementary number theory, it presents Euclid\u2019s algorithm for computing greatest common divisors. Next, it studies algorithms for solving modular linear equations and for raising one number to a power modulo another number. Then, it explores an important application of number-theoretic algorithms: the RSA public-key cryptosystem. This cryptosystem can be used not only to encrypt messages so that an adversary cannot read them, but also to provide digital signatures. The chapter finishes with the Miller-Rabin randomized primality test, which enables finding large primes efficiently\u2014an essential requirement for the RSA system.\n\nChapter 32 studies the problem of finding all occurrences of a given pattern string in a given text string, a problem that arises frequently in text-editing programs. After examining the naive approach, the chapter presents an elegant approach due to Rabin and Karp. Then, after showing an efficient solution based on finite automata, the chapter presents the Knuth-Morris-Pratt algorithm, which modifies the automaton-based algorithm to save space by cleverly preprocessing the pattern. The chapter finishes by studying suffix arrays, which can not only find a pattern in a text string, but can do quite a bit more, such as finding the longest repeated substring in a text and finding the longest common substring appearing in two texts.\n\nChapter 33 examines three algorithms within the expansive field of machine learning. Machine-learning algorithms are designed to take in vast amounts of data, devise hypotheses about patterns in the data, and test these hypotheses. The chapter starts with k-means clustering, which groups data elements into k classes based on how similar they are to each other. It then shows how to use the technique of multiplicative weights to make predictions accurately based on a set of \"experts\" of varying quality. Perhaps surprisingly, even without knowing which experts are reliable and which are not, you can predict almost as accurately as the most reliable expert. The chapter finishes with gradient descent, an optimization technique that finds a local minimum value for a function. Gradient descent has many applications, including finding parameter settings for many machine-learning models.\n\nChapter 34 concerns NP-complete problems. Many interesting computational problems are NP-complete, but no polynomial-time algorithm is known for solving any of them. This chapter presents techniques for determining when a problem is NP-complete, using them to prove several classic problems NP-complete: determining whether a graph has a hamiltonian cycle (a cycle that includes every vertex), determining whether a boolean formula is satisfiable (whether there exists an assignment of boolean values to its variables that causes the formula to evaluate to TRUE), and determining whether a given set of numbers has a subset that adds up to a given target value. The chapter also proves that the famous traveling-salesperson problem (find a shortest route that starts and ends at the same location and visits each of a set of locations once) is NP-complete.\n\nChapter 35 shows how to find approximate solutions to NP-complete problems efficiently by using approximation algorithms. For some NP-complete problems, approximate solutions that are near optimal are quite easy to produce, but for others even the best approximation algorithms known work progressively more poorly as the problem size increases. Then, there are some problems for which investing increasing amounts of computation time yields increasingly better approximate solutions. This chapter illustrates these possibilities with the vertex-cover problem (unweighted and weighted versions), an optimization version of 3-CNF satisfiability, the traveling-salesperson problem, the set-covering problem, and the subset-sum problem."
            },
            {
                "file": "PartVII Selected Topics Algorithms.md",
                "contents": "# C26 Parallel Algorithms\n\n## C26.1 FIB\n```\nFIB(n)\n1  if n \u2264 1\n2      return n\n3  else x = FIB(n - 1)\n4       y = FIB(n - 2)\n5       return x + y\n```\n\n## C26.1 P-FIB\n```\nP-FIB(n)\n1  if n \u2264 1\n2      return n\n3  else x = spawn P-FIB(n - 1) // don't wait for subroutine to return\n4       y = P-FIB(n - 2)     // in parallel with spawned subroutine\n5       sync                   // wait for spawned subroutine to finish\n6       return x + y\n```\n\n## C26.1 P-MAT-VEC\n```\nP-MAT-VEC(A, x, y, n)\n1  parallel for i = 1 to n // parallel loop\n2      for j = 1 to n       // serial loop\n3          yi = yi + A[i,j] * xj\n```\n\n## C26.1 P-MAT-VEC-RECURSIVE\n```\nP-MAT-VEC-RECURSIVE(A, x, y, n, i, i')\n1  if i == i'                         // just one iteration to do?\n2      for j = 1 to n                 // mimic P-MAT-VEC serial loop\n3          y[i] = y[i] + A[i,j] * x[j]\n4  else mid = floor((i + i') / 2)     // parallel divide-and-conquer\n5       spawn P-MAT-VEC-RECURSIVE(A, x, y, n, i, mid)\n6       P-MAT-VEC-RECURSIVE(A, x, y, n, mid + 1, i')\n7       sync\n```\n\n## C26.1 RACE-EXAMPLE\n```\nRACE-EXAMPLE()\n1  x = 0\n2  parallel for i = 1 to 2\n3      x = x + 1 // determinacy race\n4  print x\n```\n\n## C26.1 P-MAT-VEC-WRONG\n```\nP-MAT-VEC-WRONG(A, x, y, n)\n1  parallel for i = 1 to n\n2      parallel for j = 1 to n\n3          y[i] = y[i] + A[i,j] * x[j] // determinacy race\n```\n\n## C26 Exercises P-TRANSPOSE\n```\nP-TRANSPOSE(A, n)\n1  parallel for j = 2 to n\n2      parallel for i = 1 to j - 1\n3          exchange A[i,j] with A[j,i]\n```\n\n## C26.2 P-MATRIX-MULTIPLY\n```\nP-MATRIX-MULTIPLY(A, B, C, n)\n1  parallel for i = 1 to n // compute entries in each of n rows\n2      parallel for j = 1 to n // compute n entries in row i\n3          for k = 1 to n\n4              C[i,j] = C[i,j] + A[i,k] * B[k,j] // add in another term of equation (4.1)\n```\n\n## C26.2 P-MATRIX-MULTIPLY-RECURSIVE\n```\nP-MATRIX-MULTIPLY-RECURSIVE(A, B, C, n)\n1  if n == 1                                  // just one element in each matrix?\n2      C[1,1] = C[1,1] + A[1,1] * B[1,1]\n3      return\n4  let D be a new n x n matrix                  // temporary matrix\n5  parallel for i = 1 to n                    // set D = 0\n6      parallel for j = 1 to n\n7          D[i,j] = 0\n8  partition A, B, C, and D into n/2 x n/2 submatrices\n     A11, A12, A21, A22; B11, B12, B21, B22; C11, C12, C21, C22;\n     and D11, D12, D21, D22; respectively\n9  spawn P-MATRIX-MULTIPLY-RECURSIVE(A11, B11, C11, n/2)\n10 spawn P-MATRIX-MULTIPLY-RECURSIVE(A11, B12, C12, n/2)\n11 spawn P-MATRIX-MULTIPLY-RECURSIVE(A21, B11, C21, n/2)\n12 spawn P-MATRIX-MULTIPLY-RECURSIVE(A21, B12, C22, n/2)\n13 spawn P-MATRIX-MULTIPLY-RECURSIVE(A12, B21, D11, n/2)\n14 spawn P-MATRIX-MULTIPLY-RECURSIVE(A12, B22, D12, n/2)\n15 spawn P-MATRIX-MULTIPLY-RECURSIVE(A22, B21, D21, n/2)\n16 spawn P-MATRIX-MULTIPLY-RECURSIVE(A22, B22, D22, n/2)\n17 sync                                       // wait for spawned submatrix products\n18 parallel for i = 1 to n                    // update C = C + D\n19     parallel for j = 1 to n\n20         C[i,j] = C[i,j] + D[i,j]\n```\n\n## C26.3 P-MERGE-SORT\n```\nP-MERGE-SORT(A, p, r)\n1  if p \u2265 r                           // zero or one element?\n2      return\n3  q = floor((p + r) / 2)             // midpoint of A[p:r]\n4  // Recursively sort A[p:q] in parallel.\n5  spawn P-MERGE-SORT(A, p, q)\n6  // Recursively sort A[q+1:r] in parallel.\n7  spawn P-MERGE-SORT(A, q + 1, r)\n8  sync                             // wait for spawns\n9  // Merge A[p:q] and A[q+1:r] into A[p:r].\n10 P-MERGE(A, p, q, r)\n```\n\n## C26.3 FIND-SPLIT-POINT\n```\nFIND-SPLIT-POINT(A, p, r, x)\n1  low = p                          // low end of search range\n2  high = r + 1                     // high end of search range\n3  while low < high                 // more than one element?\n4      mid = floor((low + high) / 2) // midpoint of range\n5      if x \u2264 A[mid]                  // is answer q \u2264 mid?\n6          high = mid                 // narrow search to A[low:mid]\n7      else low = mid + 1             // narrow search to A[mid+1:high]\n8  return low\n```\n\n## C26.3 P-MERGE\n```\nP-MERGE(A, p, q, r)\n1  let B[p:r] be a new array          // allocate scratch array\n2  P-MERGE-AUX(A, p, q, q + 1, r, B, p) // merge from A into B\n3  parallel for i = p to r            // copy B back to A in parallel\n4      A[i] = B[i]\n```\n\n## C26.3 P-MERGE-AUX\n```\nP-MERGE-AUX(A, p1, r1, p2, r2, B, p3)\n1  if p1 > r1 and p2 > r2             // are both subarrays empty?\n2      return\n3  if r1 - p1 < r2 - p2                 // second subarray bigger?\n4      exchange p1 with p2            // swap subarray roles\n5      exchange r1 with r2\n6  q1 = floor((p1 + r1) / 2)          // midpoint of A[p1:r1]\n7  x = A[q1]                          // median of A[p1:r1] is pivot x\n8  q2 = FIND-SPLIT-POINT(A, p2, r2, x) // split A[p2:r2] around x\n9  q3 = p3 + (q1 - p1) + (q2 - p2)    // where x belongs in B ...\n10 B[q3] = x                           // ... put it there\n11 // Recursively merge A[p1:q1-1] and A[p2:q2-1] into B[p3:q3-1].\n12 spawn P-MERGE-AUX(A, p1, q1 - 1, p2, q2 - 1, B, p3)\n13 // Recursively merge A[q1+1:r1] and A[q2:r2] into B[q3+1:r3].\n14 spawn P-MERGE-AUX(A, q1 + 1, r1, q2, r2, B, q3 + 1)\n15 sync                               // wait for spawns\n```\n\n## C26 Problem 26-1 SUM-ARRAYS\n```\nSUM-ARRAYS(A, B, C, n)\n1  parallel for i = 1 to n\n2      C[i] = A[i] + B[i]\n```\n\n## C26 Problem 26-1 SUM-ARRAYS'\n```\nSUM-ARRAYS'(A, B, C, n)\n1  grain-size = ? // to be determined\n2  r = ceil(n / grain-size)\n3  for k = 0 to r - 1\n4      spawn ADD-SUBARRAY(A, B, C, k * grain-size + 1, min((k + 1) * grain-size, n))\n5  sync\n```\n\n## C26 Problem 26-1 ADD-SUBARRAY\n```\nADD-SUBARRAY(A, B, C, i, j)\n1  for k = i to j\n2      C[k] = A[k] + B[k]\n```\n\n## C26 Problem 26-4 REDUCE\n```\nREDUCE(x, i, j)\n1  y = x[i]\n2  for k = i + 1 to j\n3      y = y \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 x[k]\n4  return y\n```\n\n## C26 Problem 26-4 SCAN\n```\nSCAN(x, n)\n1  let y[1:n] be a new array\n2  y[1] = x[1]\n3  for i = 2 to n\n4      y[i] = y[i-1] \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 x[i]\n5  return y\n```\n\n## C26 Problem 26-4 P-SCAN-1\n```\nP-SCAN-1(x, n)\n1  let y[1:n] be a new array\n2  P-SCAN-1-AUX(x, y, 1, n)\n3  return y\n```\n\n## C26 Problem 26-4 P-SCAN-1-AUX\n```\nP-SCAN-1-AUX(x, y, i, j)\n1  parallel for l = i to j\n2      y[l] = P-REDUCE(x, 1, l)\n```\n\n## C26 Problem 26-4 P-SCAN-2\n```\nP-SCAN-2(x, n)\n1  let y[1:n] be a new array\n2  P-SCAN-2-AUX(x, y, 1, n)\n3  return y\n```\n\n## C26 Problem 26-4 P-SCAN-2-AUX\n```\nP-SCAN-2-AUX(x, y, i, j)\n1  if i == j\n2      y[i] = x[i]\n3  else k = floor((i + j) / 2)\n4       spawn P-SCAN-2-AUX(x, y, i, k)\n5       P-SCAN-2-AUX(x, y, k + 1, j)\n6       sync\n7       parallel for l = k + 1 to j\n8           y[l] = y[k] \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 y[l]\n```\n\n## C26 Problem 26-4 P-SCAN-3\n```\nP-SCAN-3(x, n)\n1  let y[1:n] and t[1:n] be new arrays\n2  y[1] = x[1]\n3  if n > 1\n4      P-SCAN-UP(x, t, 2, n)\n5      P-SCAN-DOWN(x[1], x, t, y, 2, n)\n6  return y\n```\n\n## C26 Problem 26-4 P-SCAN-UP\n```\nP-SCAN-UP(x, t, i, j)\n1  if i == j\n2      return x[i]\n3  else\n4      k = floor((i + j) / 2)\n5      t[k] = spawn P-SCAN-UP(x, t, i, k)\n6      right = P-SCAN-UP(x, t, k + 1, j)\n7      sync\n8      return // fill in the blank (t[k] \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 right)\n```\n\n## C26 Problem 26-4 P-SCAN-DOWN\n```\nP-SCAN-DOWN(v, x, t, y, i, j)\n1  if i == j\n2      y[i] = v \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 x[i]\n3  else\n4      k = floor((i + j) / 2)\n5      spawn P-SCAN-DOWN(// fill in the blank (v), x, t, y, i, k)\n6      P-SCAN-DOWN(// fill in the blank (v \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 t[k]), x, t, y, k + 1, j)\n7      sync\n```"
            },
            {
                "file": "PartVII Selected Topics/C26 Parallel Algorithms.md",
                "contents": "# 26 Parallel Algorithms\n\nThe vast majority of algorithms in this book are **serial algorithms** suitable for running on a uniprocessor computer that executes only one instruction at a time. This chapter extends our algorithmic model to encompass **parallel algorithms**, where multiple instructions can execute simultaneously. Specifically, we\u2019ll explore the elegant model of task-parallel algorithms, which are amenable to algorithmic design and analysis. Our study focuses on fork-join parallel algorithms, the most basic and best understood kind of task-parallel algorithm. Fork-join parallel algorithms can be expressed cleanly using simple linguistic extensions to ordinary serial code. Moreover, they can be implemented efficiently in practice.\n\nParallel computers\u2014computers with multiple processing units\u2014are ubiquitous. Handheld, laptop, desktop, and cloud machines are all **multicore computers**, or simply, **multicores**, containing multiple processing \u201ccores.\u201d Each processing core is a full-fledged processor that can directly access any location in a common **shared memory**. Multicores can be aggregated into larger systems, such as clusters, by using a network to interconnect them. These multicore clusters usually have a **distributed memory**, where one multicore\u2019s memory cannot be accessed directly by a processor in another multicore. Instead, the processor must explicitly send a message over the cluster network to a processor in the remote multicore to request any data it requires. The most powerful clusters are supercomputers, comprising many thousands of multicores. But since shared-memory programming tends to be conceptually easier than distributed-memory programming, and multicore machines are widely available, this chapter focuses on parallel algorithms for multicores.\n\nOne approach to programming multicores is **thread parallelism**. This processor-centric parallel-programming model employs a software abstraction of \u201cvirtual processors,\u201d or **threads** that share a common memory. Each thread maintains its own program counter and can execute code independently of the other threads. The operating system loads a thread onto a processing core for execution and switches it out when another thread needs to run.\n\nUnfortunately, programming a shared-memory parallel computer using threads tends to be difficult and error-prone. One reason is that it can be complicated to dynamically partition the work among the threads so that each thread receives approximately the same load. For any but the simplest of applications, the programmer must use complex communication protocols to implement a scheduler that load-balances the work.\n\n## 26.1 The basics of fork-join parallelism\n\nOur exploration of parallel programming begins with the problem of computing Fibonacci numbers recursively in parallel. We\u2019ll look at a straightforward serial Fibonacci calculation, which, although inefficient, serves as a good illustration of how to express parallelism in pseudocode.\nRecall that the Fibonacci numbers are defined by equation (3.31) on page 69 of the book:\n$F_i = \\begin{cases} 0 & \\text{if } i = 0, \\\\ 1 & \\text{if } i = 1, \\\\ F_{i-1} + F_{i-2} & \\text{if } i \\ge 2. \\end{cases}$\nTo calculate the $n$th Fibonacci number recursively, you could use the ordinary serial algorithm in the procedure FIB [[PartVII Selected Topics Algorithms.md#C26.1 FIB]].\nTo analyze this algorithm, let $T(n)$ denote the running time of `FIB(n)`. Since `FIB(n)` contains two recursive calls plus a constant amount of extra work, we obtain the recurrence $T(n) = T(n-1) + T(n-2) + \\Theta(1)$. This recurrence has solution $T(n) = \\Theta(F_n)$, which can be established by using the substitution method. Since $F_n = \\Theta(\\phi^n)$, where $\\phi = (1 + \\sqrt{5})/2$ is the golden ratio, it follows that $T(n) = \\Theta(\\phi^n)$ (Equation 26.1).\n\n### Parallel keywords\nThe `P-FIB` procedure [[PartVII Selected Topics Algorithms.md#C26.1 P-FIB]] computes Fibonacci numbers using the parallel keywords **spawn** and **sync**.\nIf the keywords `spawn` and `sync` are deleted from `P-FIB`, the resulting pseudocode text is identical to `FIB` (other than renaming). We define the **serial projection** of a parallel algorithm to be the serial algorithm that results from ignoring the parallel directives.\n\n### Semantics of parallel keywords\n**Spawning** occurs when the keyword `spawn` precedes a procedure call. The semantics of a `spawn` differs from an ordinary procedure call in that the procedure instance that executes the spawn\u2014the **parent**\u2014may continue to execute in parallel with the spawned subroutine\u2014its **child**\u2014instead of waiting for the child to finish. The keyword `spawn` does not say that a procedure *must* execute in parallel with its spawned children, only that it *may*. The parallel keywords express the **logical parallelism** of the computation. At runtime, a **scheduler** determines which subcomputations actually run in parallel.\nA procedure cannot safely use the values returned by its spawned children until after it executes a **sync** statement. The keyword `sync` indicates that the procedure must wait as necessary for all its spawned children to finish before proceeding to the statement after the `sync`\u2014the \u201cjoin\u201d of a fork-join parallel computation. It is convenient to assume that every procedure executes a `sync` implicitly before it returns.\n\n### A graph model for parallel execution\nThe execution of a parallel computation can be viewed as a directed acyclic graph $G = (V, E)$, called a (parallel) **trace**. Vertices in $V$ are executed instructions, and edges in $E$ represent dependencies. If a chain of instructions contains no parallel or procedural control (no `spawn`, `sync`, procedure call, or `return`), we group the entire chain into a single **strand**. Strands do not include instructions that involve parallel or procedural control; these control dependencies must be represented as edges in the trace.\nIf $G$ contains a directed path from strand $u$ to strand $v$, the two strands are (logically) **in series**. If there is no path in $G$ either from $u$ to $v$ or from $v$ to $u$, the strands are (logically) **in parallel**.\nAnalyses generally assume an **ideal parallel computer**: a set of processors and a **sequentially consistent** shared memory. Memory is accessed by **load instructions** and **store instructions**. Sequential consistency means that even if multiple processors attempt to access memory simultaneously, the shared memory behaves as if exactly one instruction from one of the processors is executed at a time, in some global linear order that preserves individual processor orders.\n\n### Performance measures\nTheoretical efficiency of a task-parallel algorithm is gauged using **work/span analysis**, based on **work** and **span**.\n**Work** of a task-parallel computation is the total time to execute the entire computation on one processor. It's the sum of the times taken by each strand. If $T_P$ is the running time on $P$ processors, work is $T_1$.\n**Span** is the fastest possible time to execute the computation on an unlimited number of processors. It corresponds to the sum of the times taken by strands along a longest path (weighted by execution time) in the trace. Such a longest path is the **critical path**. Span is denoted $T_\\infty$.\nWork and span provide lower bounds on running time $T_P$:\n-   Work law: $T_P \\ge T_1/P$ (Equation 26.2)\n-   Span law: $T_P \\ge T_\\infty$ (Equation 26.3)\n\n**Speedup** on $P$ processors is $T_1/T_P$. It's at most $P$. When $T_1/T_P = \\Theta(P)$, it's **linear speedup**. **Perfect linear speedup** is when $T_1/T_P = P$.\n**Parallelism** is $T_1/T_\\infty$. It denotes the average amount of work per step along the critical path, the maximum possible speedup, and a limit on attaining perfect linear speedup (if $P > T_1/T_\\infty$, perfect linear speedup is impossible).\n(Parallel) **slackness** is $(T_1/T_\\infty)/P = T_1/(P T_\\infty)$. If slackness < 1, perfect linear speedup is impossible.\n\n### Scheduling\nA task-parallel scheduler maps the dynamically unfolding computation to individual processors. We analyze **greedy schedulers**, which assign as many strands to processors as possible in each time step.\nA step is a **complete step** if at least $P$ strands are ready. A greedy scheduler assigns any $P$ ready strands.\nA step is an **incomplete step** if fewer than $P$ strands are ready. A greedy scheduler assigns each ready strand to its own processor.\n**Theorem 26.1:** On an ideal parallel computer with $P$ processors, a greedy scheduler executes a task-parallel computation with work $T_1$ and span $T_\\infty$ in time $T_P \\le T_1/P + T_\\infty$ (Equation 26.4).\n**Corollary 26.2:** Running time $T_P$ scheduled by a greedy scheduler is within a factor of 2 of optimal.\n**Corollary 26.3:** If $P \\ll T_1/T_\\infty$ (slackness is much greater than 1), then $T_P \\approx T_1/P$ (near-perfect linear speedup).\n\n### Analyzing parallel algorithms\nWork of `P-FIB(n)` is $T_1(n) = \\Theta(\\phi^n)$ [[PartVII Selected Topics Algorithms.md#C26.1 P-FIB]].\nSpan of `P-FIB(n)`: The spawned call `P-FIB(n-1)` runs in parallel with `P-FIB(n-2)`. The recurrence is $T_\\infty(n) = \\max(T_\\infty(n-1), T_\\infty(n-2)) + \\Theta(1) = T_\\infty(n-1) + \\Theta(1)$, which has solution $T_\\infty(n) = \\Theta(n)$.\nParallelism of `P-FIB(n)` is $T_1(n)/T_\\infty(n) = \\Theta(\\phi^n/n)$.\n\n### Parallel loops\nPseudocode uses the **parallel** keyword before `for` for loops where iterations can operate in parallel. Compilers can implement `parallel for` loops using recursive spawning. For example, `P-MAT-VEC` [[PartVII Selected Topics Algorithms.md#C26.1 P-MAT-VEC]] uses `parallel for`. It can be implemented by `P-MAT-VEC-RECURSIVE` [[PartVII Selected Topics Algorithms.md#C26.1 P-MAT-VEC-RECURSIVE]].\nWork of `P-MAT-VEC` is $T_1(n) = \\Theta(n^2)$ for an $n \\times n$ matrix.\nSpan of a parallel loop with $n$ iterations, where iteration $i$ has span $iter_\\infty(i)$, is $T_\\infty(n) = \\Theta(\\lg n) + \\max_i \\{iter_\\infty(i)\\}$.\nSpan of `P-MAT-VEC`'s doubly nested loops: The outer `parallel for` control is $\\Theta(\\lg n)$. Each inner serial `for` loop has span $\\Theta(n)$. So, $T_\\infty(n) = \\Theta(n) + \\Theta(\\lg n) = \\Theta(n)$.\nParallelism of `P-MAT-VEC` is $\\Theta(n^2)/\\Theta(n) = \\Theta(n)$.\n\n### Race conditions\nA parallel algorithm is **deterministic** if it always does the same thing on the same input. It is **nondeterministic** if its behavior might vary.\nA **determinacy race** occurs when two logically parallel instructions access the same memory location and at least one modifies the value. Example: `RACE-EXAMPLE` [[PartVII Selected Topics Algorithms.md#C26.1 RACE-EXAMPLE]]. Incrementing a shared variable `x` involves load, increment register, store. If interleaved, updates can be lost.\nTo ensure algorithms are deterministic, any two strands operating in parallel should be **mutually noninterfering**: they only read, and do not modify, any memory locations accessed by both. Index variables of `parallel for` loops usually do not cause races.\n`P-MAT-VEC-WRONG` [[PartVII Selected Topics Algorithms.md#C26.1 P-MAT-VEC-WRONG]] is incorrect due to determinacy races when updating $y_i$.\n\n### A chess lesson\nWork/span analysis can be superior to measured running times for extrapolating scalability. An optimization that reduced $T_{32}$ from 65s to 40s for a chess program was abandoned. Original: $T_1 = 2048s, T_\\infty = 1s$. $T_P = T_1/P + T_\\infty$. $T_{32} = 2048/32 + 1 = 65s$. $T_{512} = 2048/512 + 1 = 5s$. Optimized: $T'_1 = 1024s, T'_\\infty = 8s$. $T'_{32} = 1024/32 + 8 = 40s$. $T'_{512} = 1024/512 + 8 = 10s$. The optimization scaled poorly.\n\n## 26.2 Parallel matrix multiplication\n\nThis section explores parallelizing three matrix-multiplication algorithms.\n\n### A parallel algorithm for matrix multiplication using parallel loops\nThe `P-MATRIX-MULTIPLY` procedure [[PartVII Selected Topics Algorithms.md#C26.2 P-MATRIX-MULTIPLY]] parallelizes the two outer loops of the standard `MATRIX-MULTIPLY`.\nWork: $T_1(n) = \\Theta(n^3)$.\nSpan: Two nested `parallel for` loops contribute $\\Theta(\\lg n)$ each for their recursion trees, and the innermost serial `for` loop contributes $\\Theta(n)$. So, $T_\\infty(n) = \\Theta(\\lg n) + \\Theta(\\lg n) + \\Theta(n) = \\Theta(n)$.\nParallelism: $\\Theta(n^3)/\\Theta(n) = \\Theta(n^2)$.\n\n### A parallel divide-and-conquer algorithm for matrix multiplication\n`P-MATRIX-MULTIPLY-RECURSIVE` [[PartVII Selected Topics Algorithms.md#C26.2 P-MATRIX-MULTIPLY-RECURSIVE]] parallelizes the recursive matrix multiplication from Section 4.1 (page 83 of the book) using spawning for the eight submatrix multiplications. It uses a temporary matrix $D$ to avoid determinacy races.\nWork: $M_1(n) = 8M_1(n/2) + \\Theta(n^2) = \\Theta(n^3)$ (by master theorem case 1).\nSpan: The eight parallel recursive spawns operate on matrices of the same size. The span for zeroing matrix $D$ (lines 5-7) and adding $D$ to $C$ (lines 18-20) using nested `parallel for` loops is $\\Theta(\\lg n)$. Matrix partitioning is $\\Theta(1)$. Recurrence: $M_\\infty(n) = M_\\infty(n/2) + \\Theta(\\lg n)$ (Equation 26.6). By master theorem case 2 (with $k=1$ for $\\lg^k n$), $M_\\infty(n) = \\Theta(\\lg^2 n)$.\nParallelism: $M_1(n)/M_\\infty(n) = \\Theta(n^3 / \\lg^2 n)$.\n\n### Parallelizing Strassen\u2019s method\nStrassen's algorithm can be parallelized using spawning. Steps involve partitioning, creating $S_i$ and $P_i$ matrices, 7 recursive spawns, and combining results.\n1. Partitioning: $\\Theta(1)$ work and span.\n2. Creating $S_i$ and $P_i$ matrices (17 total, $P_i$ zeroed): Nested `parallel for` loops give $\\Theta(n^2)$ work and $\\Theta(\\lg n)$ span.\n3. 7 recursive spawns: $7T_1(n/2)$ work and $T_\\infty(n/2)$ span (max of 7 parallel).\n4. Updating $C_{ij}$ submatrices: Nested `parallel for` loops give $\\Theta(n^2)$ work and $\\Theta(\\lg n)$ span.\nWork: Same as serial Strassen, $T_1(n) = \\Theta(n^{\\lg 7})$.\nSpan: Same recurrence as `P-MATRIX-MULTIPLY-RECURSIVE`, $T_\\infty(n) = T_\\infty(n/2) + \\Theta(\\lg n)$, so $T_\\infty(n) = \\Theta(\\lg^2 n)$.\nParallelism: $\\Theta(n^{\\lg 7} / \\lg^2 n)$.\n\n## 26.3 Parallel merge sort\n\nSerial merge sort is $\\Theta(n \\lg n)$. The `P-MERGE-SORT` procedure [[PartVII Selected Topics Algorithms.md#C26.3 P-MERGE-SORT]] modifies merge sort to spawn the first recursive call. (The description notes that it spawns both recursive calls, lines 5 and 7).\nIf a serial `MERGE` procedure ($\\Theta(n)$ work and span) is used (`P-NAIVE-MERGE-SORT`): \nWork $T_1(n) = \\Theta(n \\lg n)$.\nSpan $T_\\infty(n) = T_\\infty(n/2) + \\Theta(n) = \\Theta(n)$ (master theorem case 1).\nParallelism $T_1(n)/T_\\infty(n) = \\Theta(\\lg n)$, which is not impressive.\nThe bottleneck is the serial `MERGE` procedure. We need a parallel merging algorithm.\n\n`P-MERGE` [[PartVII Selected Topics Algorithms.md#C26.3 P-MERGE]] uses an auxiliary recursive procedure `P-MERGE-AUX` [[PartVII Selected Topics Algorithms.md#C26.3 P-MERGE-AUX]].\nThe key idea for `P-MERGE-AUX` merging $A[p_1:r_1]$ and $A[p_2:r_2]$ into $B[p_3:r_3]$:\n1. Assume $A[p_1:r_1]$ is the larger subarray.\n2. Pick pivot $x = A[q_1]$ where $q_1 = \\lfloor (p_1+r_1)/2 \\rfloor$.\n3. Find split point $q_2$ in $A[p_2:r_2]$ using binary search (`FIND-SPLIT-POINT` [[PartVII Selected Topics Algorithms.md#C26.3 FIND-SPLIT-POINT]]) such that elements before $q_2$ are $\\le x$ and elements at/after $q_2$ are $\\ge x$. This takes $\\Theta(\\lg n)$ time.\n4. Determine index $q_3$ in $B$ where $x$ belongs.\n5. Place $x$ into $B[q_3]$.\n6. Recursively spawn to merge $A[p_1:q_1-1]$ with $A[p_2:q_2-1]$ into $B[p_3:q_3-1]$.\n7. Recursively spawn to merge $A[q_1+1:r_1]$ with $A[q_2:r_2]$ into $B[q_3+1:r_3]$.\n`FIND-SPLIT-POINT(A, p, r, x)` runs in $\\Theta(\\lg n)$ work and span for a subarray of size $n=r-p+1$.\n\n### Work/span analysis of parallel merging\nFor `P-MERGE-AUX` on $n$ total elements:\nSpan $T_\\infty(n)$: The call to `FIND-SPLIT-POINT` contributes $\\Theta(\\lg n)$. Recursive spawns operate in parallel. Neither recursive invocation operates on more than $3n/4$ elements. So, $T_\\infty(n) = T_\\infty(3n/4) + \\Theta(\\lg n)$ (Equation 26.7). By master theorem case 2 (with $k=1$), $T_\\infty(n) = \\Theta(\\lg^2 n)$.\nWork $T_1(n)$: Binary search costs $\\Theta(\\lg n)$. Recursive spawns together merge at most $n-1$ elements. Recurrence: $T_1(n) = T_1(\\alpha n) + T_1((1-\\alpha)n) + \\Theta(\\lg n)$ for $1/4 \\le \\alpha \\le 3/4$ (Equation 26.8). Using substitution, $T_1(n) = O(n)$. Since it's $\\Omega(n)$ (copies elements), $T_1(n) = \\Theta(n)$.\nThe `P-MERGE` wrapper adds a `parallel for` to copy $B$ to $A$, which has $\\Theta(\\lg n)$ span and $\\Theta(n)$ work. So, overall span for `P-MERGE` is $\\Theta(\\lg^2 n)$ and work is $\\Theta(n)$.\n\n### Analysis of parallel merge sort\nUsing the parallel `P-MERGE`:\nWork of `P-MERGE-SORT`: $T_1(n) = 2T_1(n/2) + \\Theta(n) = \\Theta(n \\lg n)$ (master theorem case 2).\nSpan of `P-MERGE-SORT`: $T_\\infty(n) = T_\\infty(n/2) + \\Theta(\\lg^2 n)$. By master theorem case 2 ($k=2$), $T_\\infty(n) = \\Theta(\\lg^3 n)$.\nParallelism: $T_1(n)/T_\\infty(n) = \\Theta(n \\lg n / \\lg^3 n) = \\Theta(n / \\lg^2 n)$. This is much better than $\\Theta(\\lg n)$.\n\n## Chapter notes\n\nParallel computers and algorithmic models for parallel programming have been around in various forms for years. Prior editions of CLRS included material on sorting networks and the PRAM (Parallel Random-Access Machine) model. The data-parallel model is another popular algorithmic programming model, which features operations on vectors and matrices as primitives. The notion of sequential consistency is due to Lamport [275].\n\nGraham [197] and Brent [71] showed that there exist schedulers achieving the bound of Theorem 26.1. Eager, Zahorjan, and Lazowska [129] showed that any greedy scheduler achieves this bound and proposed the methodology of using work and span (although not by those names) to analyze parallel algorithms. Blelloch [57] developed an algorithmic programming model based on work and span (which he called \u201cdepth\u201d) for data-parallel programming. Blumofe and Leiserson [63] gave a distributed scheduling algorithm for task-parallel computations based on randomized \u201cwork-stealing\u201d and showed that it achieves the bound $E[T_P] \\le T_1/P + O(T_\\infty)$. Arora, Blumofe, and Plaxton [20] and Blelloch, Gibbons, and Matias [61] also provided provably good algorithms for scheduling task-parallel computations.\n\nThe parallel pseudocode and programming model were influenced by Cilk [290, 291, 383, 396]. The open-source project OpenCilk (www.opencilk.org) provides Cilk programming as an extension to C and C++.\n\nConcerns about nondeterministic parallel programs were expressed by Lee [281] and Bocchino, Adve, Adve, and Snir [64]. The algorithms literature contains many algorithmic strategies for detecting races and extending the fork-join model to avoid or safely embrace various kinds of nondeterminism. Blelloch, Fineman, Gibbons, and Shun [59] showed that deterministic parallel algorithms can often be as fast as, or even faster than, their nondeterministic counterparts.\n\nSeveral of the parallel algorithms in this chapter appeared in unpublished lecture notes by C. E. Leiserson and H. Prokop and were originally implemented in Cilk. The parallel merge-sorting algorithm was inspired by an algorithm due to Akl [12]."
            }
        ]
    }
]