Let me analyze the code and create the resumes as requested. I'll structure them from most concise to most detailed.

# Software Developer Resume (Broad)
- Developed multiple AI chatbots using Claude 3.7, Gemini 2.0, and DeepSeek-R1 models
- Built parallel document processing system using Python multiprocessing
- Implemented error handling, logging, and monitoring
- Created modular, reusable components for configuring AI models, managing API credentials, and processing inputs/outputs

# Cloud Engineer Resume  
- Architected and deployed AI applications on AWS using Bedrock, IAM roles, and S3 for secure model hosting
- Built AWS Lambda functions to handle AI model inference at scale
- Implemented AWS credential management and role-based access control for AI services
- Designed parallel processing system using AWS infrastructure to handle batch AI inference jobs
- Created logging and monitoring system for AI applications using AWS services
- Experience with AWS SDK (boto3) for programmatic infrastructure management

# AI/ML Engineer Resume
- Developed production applications using state-of-the-art LLMs including Claude 3.7, Gemini Pro, DeepSeek-R1
- Built parallel inference pipeline to process documents and images using AI models at scale
- Implemented context management system for efficient handling of large text and PDF inputs
- Created batching system to optimize model inference performance and costs
- Designed modular architecture for running multiple AI models with different configurations
- Experience with prompt engineering and model output processing for production use cases
- Built logging and monitoring system to track model performance and errors

# Detailed Reference Resume
## AI Application Development
### Chatbot Applications
- Developed CLI chatbots powered by Claude 3.7, Gemini 2.0, and DeepSeek-R1 models
- Implemented multiple interaction modes:
  - Pure text chat interface
  - Image context mode for loading and analyzing images
  - File context mode for document analysis
- Built robust system for managing conversation history and context
- Implemented proper error handling for API rate limits and failures

### Parallel Document Processing System
- Created scalable system for batch processing documents using AI models
- Implemented multiprocessing to parallelize inference across CPU cores
- Built modular pipeline for handling text, PDF, and image inputs
- Designed aggregation system to combine and process model outputs
- Added token counting and monitoring capabilities
- Created logging system for tracking processing status and errors

### AWS Integration
- Deployed models on AWS Bedrock with proper IAM roles and permissions
- Implemented S3 storage for batch processing inputs/outputs
- Built credential management system for multiple API keys
- Created Lambda functions for serverless inference
- Added proper error handling for AWS service interactions

### Technical Implementation Details
- Used Python multiprocessing and concurrent.futures for parallelization
- Implemented robust logging using Python logging module
- Built modular configuration system for different models and API keys
- Created clear separation of concerns between components
- Added proper exception handling throughout the system
- Implemented interface for both sync and async operations
- Built monitoring system for tracking model performance

### Best Practices
- Followed clean code principles with proper documentation
- Implemented comprehensive error handling and logging
- Built modular, reusable components
- Added proper credential and secrets management 
- Created clear interfaces between components
- Added proper input validation and sanitization
- Implemented monitoring and observability

### Skills Demonstrated
- Production AI application development
- Cloud architecture and deployment
- Parallel processing and optimization
- Error handling and monitoring
- API integration and management
- Clean code and best practices
- Python development expertise
