[
    {
        "items": [
            {
                "file": "foundations overview.md",
                "contents": "# Part I: Foundations Overview\n\nThis document provides an overview of core foundational concepts from Part I of \"Introduction to Algorithms.\"\n\n# 1 Chapter 1: The Role of Algorithms in Computing\n\n## 1.1 Core Concepts\nAn *algorithm* is any well-defined computational procedure that takes some value, or set of values, as *input* and produces some value, or set of values, as *output* in a finite amount of time. An algorithm is thus a sequence of computational steps that transform the input into the output.\nA *computational problem* specifies the desired input/output relationship. An *instance of a problem* consists of the input (satisfying constraints in the problem statement) needed to compute a solution.\nAn algorithm is *correct* if, for every problem instance provided as input, it halts with the correct output.\nA *data structure* is a way to store and organize data to facilitate access and modifications. No single data structure works well for all purposes.\n\n## 1.2 Problem Types\nAlgorithms solve diverse problems, including sorting, searching, graph problems (shortest paths, topological sort), numerical problems (matrix multiplication, FFT), and problems in areas like computational biology, cryptography, and resource allocation (linear programming).\n*Hard problems*, such as NP-complete problems (e.g., traveling-salesperson problem), are those for which no known efficient algorithm exists. For these, approximation algorithms are often sought.\n\n# 2 Chapter 2: Getting Started\n\n## 2.1 Incremental Method\nExample: *Insertion sort*. It sorts an array by iteratively taking an element from the unsorted part and inserting it into its correct position in the already sorted part.\n\n## 2.2 Loop Invariants\nA tool to prove algorithm correctness. For a loop invariant, three properties must be shown:\n- *Initialization*: It is true prior to the first iteration of the loop.\n- *Maintenance*: If it is true before an iteration of the loop, it remains true before the next iteration.\n- *Termination*: When the loop terminates, the invariant (usually along with the reason for termination) gives a useful property that helps show the algorithm is correct.\n\n## 2.3 Analyzing Algorithms\n*Input size* ($n$) depends on the problem (e.g., number of items for sorting, total bits for multiplying integers).\n*Running time* $T(n)$ is the number of primitive operations or \"steps\" executed on a particular input of size $n$. Assumed model is a generic one-processor, *random-access machine (RAM)* model where instructions execute sequentially and basic operations take constant time.\n\n### 2.3.1 Analysis Types\n- *Worst-case analysis*: Longest running time for any input of size $n$. Provides an upper bound and is often the focus because it's a guarantee, the worst case occurs often for some problems, and the average case is frequently as bad as the worst case.\n- *Average-case analysis*: Expected running time over all inputs of size $n$, assuming a certain probability distribution of inputs.\n- *Order of growth*: Focuses on the leading term of the running time formula and ignores constant coefficients, as it best characterizes efficiency for large inputs.\n\n## 2.4 Divide-and-Conquer Method\nA common recursive algorithm design technique:\n- *Divide* the problem into a number of subproblems that are smaller instances of the same problem.\n- *Conquer* the subproblems by solving them recursively. If subproblem sizes are small enough (base case), solve them directly.\n- *Combine* the solutions to the subproblems into the solution for the original problem.\nExample: *Merge sort*. Sorts an array $A[p..r]$ by:\n1.  Divide: If $p<r$, find index $q = \\lfloor(p+r)/2\\rfloor$ to split $A[p..r]$ into $A[p..q]$ and $A[q+1..r]$.\n2.  Conquer: Recursively sort $A[p..q]$ and $A[q+1..r]$ using merge sort.\n3.  Combine: Merge the two sorted subarrays $A[p..q]$ and $A[q+1..r]$ to produce a single sorted subarray $A[p..r]$. The MERGE procedure takes $\\Theta(n)$ time for $n$ elements.\n\n### 2.4.1 Recurrence Relations for Divide-and-Conquer\nThe running time $T(n)$ of a divide-and-conquer algorithm is often described by a recurrence. For merge sort:\n$T(n) = \\begin{cases} \\Theta(1) & \\text{if } n=1 \\\\ 2T(n/2) + \\Theta(n) & \\text{if } n > 1 \\end{cases}$\nThis recurrence solves to $T(n) = \\Theta(n \\lg n)$.\n\n# 3 Chapter 3: Characterizing Running Times\n\n## 3.1 Asymptotic Notation\nUsed to describe the behavior of functions in the limit, focusing on order of growth.\n\n### 3.1.1 $\\Theta$-notation (Theta-notation)\nDefines an asymptotically tight bound.\n$\\Theta(g(n)) = \\{f(n) : \\text{there exist positive constants } c_1, c_2, \\text{ and } n_0 \\text{ such that } 0 \\le c_1 g(n) \\le f(n) \\le c_2 g(n) \\text{ for all } n \\ge n_0\\}$.\n\n### 3.1.2 $O$-notation (Big-O notation)\nDefines an asymptotic upper bound.\n$O(g(n)) = \\{f(n) : \\text{there exist positive constants } c \\text{ and } n_0 \\text{ such that } 0 \\le f(n) \\le c g(n) \\text{ for all } n \\ge n_0\\}$.\n\n### 3.1.3 $\\Omega$-notation (Big-Omega notation)\nDefines an asymptotic lower bound.\n$\\Omega(g(n)) = \\{f(n) : \\text{there exist positive constants } c \\text{ and } n_0 \\text{ such that } 0 \\le c g(n) \\le f(n) \\text{ for all } n \\ge n_0\\}$.\n\n### 3.1.4 $o$-notation (Little-o notation)\nDefines an upper bound that is not asymptotically tight.\n$o(g(n)) = \\{f(n) : \\text{for any positive constant } c > 0, \\text{ there exists a constant } n_0 > 0 \\text{ such that } 0 \\le f(n) < c g(n) \\text{ for all } n \\ge n_0\\}$.\nEquivalently, $\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = 0$.\n\n### 3.1.5 $\\omega$-notation (Little-omega notation)\nDefines a lower bound that is not asymptotically tight.\n$\\omega(g(n)) = \\{f(n) : \\text{for any positive constant } c > 0, \\text{ there exists a constant } n_0 > 0 \\text{ such that } 0 \\le c g(n) < f(n) \\text{ for all } n \\ge n_0\\}$.\nEquivalently, $\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = \\infty$.\n\n# 4 Chapter 4: Divide-and-Conquer (Overview based on Part I Introduction)\nChapter 4 delves further into the divide-and-conquer method, providing additional examples such as Strassen's algorithm for matrix multiplication. It also covers methods for solving recurrences, including the substitution method, the recursion-tree method, and the master method for solving recurrences of the form $T(n) = aT(n/b) + f(n)$.\n\n# 5 Chapter 5: Probabilistic Analysis and Randomized Algorithms (Overview based on Part I Introduction)\nChapter 5 introduces probabilistic analysis, used to determine an algorithm's running time when it may differ on inputs of the same size due to an inherent probability distribution (either in the inputs or from random choices made by the algorithm). Randomized algorithms use a random-number generator to make choices. This chapter explores concepts such as indicator random variables. Example applications include the hiring problem (analysis shows an expected hiring cost of $\\Theta(c_h \\ln n)$), the birthday paradox, and balls and bins problems. (Detailed content for Chapters 4 and 5 is not included in the provided OCR pages used for these notes).\n"
            },
            {
                "file": "PartI Foundations Algorithms.md",
                "contents": "# Part I: Foundations - Algorithms Pseudocode\n\n# C2 Getting Started Algorithms\n\n## 2.1 INSERTION-SORT(A, n)\n```\nINSERTION-SORT(A, n)\n1  for i = 2 to n\n2      key = A[i]\n3      // Insert A[i] into the sorted subarray A[1:i-1].\n4      j = i - 1\n5      while j > 0 and A[j] > key\n6          A[j+1] = A[j]\n7          j = j - 1\n8      A[j+1] = key\n```\n\n## 2.1 SUM-ARRAY(A, n) (Exercise 2.1-2)\n```\nSUM-ARRAY(A, n)\n1  sum = 0\n2  for i = 1 to n\n3      sum = sum + A[i]\n4  return sum\n```\n\n## 2.3 MERGE(A, p, q, r)\n```\nMERGE(A, p, q, r)\n1  n_L = q - p + 1\n2  n_R = r - q\n3  let L[0:n_L-1] and R[0:n_R-1] be new arrays\n4  for i = 0 to n_L-1  // copy A[p:q] into L[0:n_L-1]\n5      L[i] = A[p+i]\n6  for j = 0 to n_R-1  // copy A[q+1:r] into R[0:n_R-1]\n7      R[j] = A[q+j+1]\n8  i = 0  // i indexes the smallest remaining element in L\n9  j = 0  // j indexes the smallest remaining element in R\n10 k = p  // k indexes the location in A to fill\n11 // As long as each of the arrays L and R contains an unmerged element,\n12 //   copy the smallest unmerged element back into A[p:r].\n13 while i < n_L and j < n_R\n14     if L[i] <= R[j]\n15         A[k] = L[i]\n16         i = i + 1\n17     else A[k] = R[j]\n18         j = j + 1\n19     k = k + 1\n20 // Having gone through one of L and R entirely, copy the\n21 //   remainder of the other to the end of A[p:r].\n22 while i < n_L\n23     A[k] = L[i]\n24     i = i + 1\n25     k = k + 1\n26 while j < n_R\n27     A[k] = R[j]\n28     j = j + 1\n29     k = k + 1\n```\n\n## 2.3 MERGE-SORT(A, p, r)\n```\nMERGE-SORT(A, p, r)\n1  if p >= r          // zero or one element?\n2      return\n3  q = floor((p+r)/2) // midpoint of A[p:r]\n4  MERGE-SORT(A, p, q)   // recursively sort A[p:q]\n5  MERGE-SORT(A, q+1, r) // recursively sort A[q+1:r]\n6  // Merge A[p:q] and A[q+1:r] into A[p:r].\n7  MERGE(A, p, q, r)\n```\n"
            },
            {
                "file": "PartI Foundations/C1 The Role of Algorithms in Computing.md",
                "contents": "# 1 The Role of Algorithms in Computing\n\nThis chapter provides an overview of algorithms and their place in modern computing systems.\n\n## 1.1 Algorithms\nInformally, an *algorithm* is any well-defined computational procedure that takes some value, or set of values, as *input* and produces some value, or set of values, as *output* in a finite amount of time. An algorithm is thus a sequence of computational steps that transform the input into the output.\nAn algorithm can also be viewed as a tool for solving a well-specified *computational problem*. The problem statement specifies the desired input/output relationship. For example, the *sorting problem* is:\n- Input: A sequence of $n$ numbers $(a_1, a_2, ..., a_n)$.\n- Output: A permutation (reordering) $(a'_1, a'_2, ..., a'_n)$ of the input sequence such that $a'_1 \\le a'_2 \\le ... \\le a'_n$.\nAn *instance of a problem* consists of the input (satisfying constraints imposed in the problem statement) needed to compute a solution to the problem.\nAn algorithm is *correct* if, for every problem instance provided as input, it halts and outputs the correct solution. An incorrect algorithm might not halt or might halt with an incorrect answer.\nAlgorithms can be specified in English, as a computer program, or as a hardware design.\n\n### 1.1.1 What kinds of problems are solved by algorithms?\nAlgorithms are used in a vast array of practical applications:\n- The Human Genome Project uses algorithms for gene identification, sequence determination, data storage, and analysis (e.g., dynamic programming for sequence similarity).\n- The internet relies on algorithms for routing data (e.g., shortest path algorithms) and for search engines to find information.\n- Electronic commerce depends on algorithms for privacy and security, such as public-key cryptography and digital signatures, based on numerical algorithms and number theory.\n- Manufacturing and commercial enterprises use algorithms for resource allocation, often modeled as linear programs.\n\nSpecific problems solved by algorithms include:\n- Finding the shortest path in a graph (modeling road maps or networks).\n- Topological sorting (ordering parts in a mechanical design or tasks with dependencies).\n- Clustering algorithms (e.g., determining if a medical image represents a cancerous tumor by comparing similarity to known examples).\n- Data compression (e.g., Huffman coding).\nThese problems often have many candidate solutions, making efficient search for the best solution challenging. They also have significant practical applications.\nSome problems, like the Fast Fourier Transform (FFT), don't have an easily identified set of candidate solutions but are crucial in signal processing and data compression.\n\n### 1.1.2 Data structures\nA *data structure* is a way to store and organize data to facilitate access and modifications. Choosing appropriate data structures is an important part of algorithm design, as no single data structure is optimal for all purposes.\n\n### 1.1.3 Technique\nThis book teaches techniques for algorithm design and analysis, enabling development of new algorithms, proving their correctness, and analyzing their efficiency. Techniques include divide-and-conquer, dynamic programming, and amortized analysis.\n\n### 1.1.4 Hard problems\nSome problems, known as *NP-complete* problems, have no known efficient algorithm. For these:\n- No efficient algorithm has been found, nor has it been proven that one cannot exist.\n- If an efficient algorithm exists for one NP-complete problem, then efficient algorithms exist for all of them.\n- They are often similar to problems with known efficient solutions.\nIf a problem is NP-complete, effort may be better spent developing an *approximation algorithm* that finds a good, but not necessarily optimal, solution. The *traveling-salesperson problem* is a classic example.\n\n### 1.1.5 Alternative computing models\n- *Parallel computing*: Multicore processors require algorithms designed for parallelism (e.g., task-parallel algorithms) to achieve best performance.\n- *Online algorithms*: Handle input that arrives over time, requiring decisions without full knowledge of future input (e.g., scheduling in data centers, routing in networks, triage in emergency rooms).\n\n## 1.2 Algorithms as a technology\nAlgorithm efficiency is crucial. Even with fast computers, an inefficient algorithm can be impractical for large inputs. For example, an algorithm with $\\Theta(n^2)$ running time (like insertion sort) is significantly outperformed by an algorithm with $\\Theta(n \\lg n)$ running time (like merge sort) for large $n$, regardless of constant factors or hardware speed.\nChoosing efficient algorithms is as important as choosing fast hardware for total system performance. Advances in algorithms are as impactful as advances in other computer technologies.\nAlgorithms are fundamental even in applications that might not seem overtly algorithmic at the application level, as they underpin hardware design, GUIs, networking, compilers, and machine learning.\nMachine learning itself is a collection of algorithms. While it automates algorithmic design for some problems (especially where human understanding of the ideal algorithm is limited, like computer vision), for many well-understood problems, specifically designed efficient algorithms are typically more successful. Data science also heavily relies on the design and analysis of algorithms."
            },
            {
                "file": "PartI Foundations/C2 Getting Started.md",
                "contents": "# 2 Getting Started\n\nThis chapter introduces a framework for thinking about algorithm design and analysis, focusing on insertion sort and merge sort.\n\n## 2.1 Insertion sort\nInsertion sort is an efficient algorithm for sorting a small number of elements. It solves the sorting problem:\n- Input: A sequence of $n$ numbers $(a_1, a_2, ..., a_n)$, also known as *keys*.\n- Output: A permutation $(a'_1, a'_2, ..., a'_n)$ such that $a'_1 \\le a'_2 \\le ... \\le a'_n$.\nKeys are often associated with *satellite data*, forming a *record*. Sorting usually moves records with their keys.\nInsertion sort works like sorting a hand of playing cards: take cards one by one from a pile and insert them into the correct position in your left hand. The left hand always holds sorted cards.\nThe pseudocode for insertion sort is [[PartI Foundations Algorithms.md#2.1 INSERTION-SORT]]. It sorts an array $A[1..n]$ in place.\n\n### 2.1.1 Loop invariants and the correctness of insertion sort\nA *loop invariant* is a property that holds true before each iteration of a loop. To show correctness using a loop invariant, we demonstrate three things:\n- *Initialization*: The invariant is true prior to the first loop iteration.\n- *Maintenance*: If the invariant is true before an iteration, it remains true before the next iteration.\n- *Termination*: When the loop terminates, the invariant, combined with the termination condition, helps prove the algorithm's correctness.\n\nFor INSERTION-SORT (lines 1-8 of [[PartI Foundations Algorithms.md#2.1 INSERTION-SORT]]):\n- *Loop Invariant*: At the start of each iteration of the `for` loop (indexed by `i`), the subarray $A[1..i-1]$ consists of the elements originally in $A[1..i-1]$, but in sorted order.\n- *Initialization*: Before the first iteration ($i=2$), $A[1..i-1]$ is $A[1]$. This single-element subarray is the original element $A[1]$ and is trivially sorted. The invariant holds.\n- *Maintenance*: The body of the `for` loop (lines 2-8) works by moving $A[i-1], A[i-2], \text{etc.,}$ one position to the right until the correct position for `key` (original $A[i]$) is found. Then `key` is inserted. This ensures $A[1..i]$ now contains original elements from $A[1..i]$ in sorted order. Incrementing $i$ for the next iteration then maintains the invariant for the new $A[1..i-1]$. (A more formal proof would use a nested loop invariant for the `while` loop of lines 5-7).\n- *Termination*: The `for` loop terminates when $i$ exceeds $n$, so $i=n+1$. Substituting $n+1$ for $i$ in the invariant, the subarray $A[1..n]$ consists of the elements originally in $A[1..n]$, but in sorted order. This means the entire array is sorted.\n\n### 2.1.2 Pseudocode conventions\n- Indentation indicates block structure.\n- Looping constructs (`while`, `for`, `repeat-until`) and conditionals (`if-else`) are similar to C, Java, Python. Loop counters retain their value after exiting a `for` loop (value that exceeded the bound).\n- `//` denotes a comment.\n- Variables are local to the procedure unless specified.\n- Array elements are accessed via `A[i]`. 1-origin indexing is common, but bounds are specified if ambiguous. `A[i..j]` denotes a subarray.\n- Compound data is organized into objects with attributes, accessed via `object.attribute`.\n- Variables for arrays/objects are treated as pointers/references. Assignment copies the reference, not the object. Changes to attributes via one reference are visible via others pointing to the same object.\n- Parameters are passed *by value*. For objects/arrays, the reference is passed by value; the called procedure gets a copy of the reference. Modifying array elements or object attributes *is* visible to the caller.\n- `return` statement transfers control and optionally a value. Multiple values can be returned.\n- Boolean operators `and` and `or` are short-circuiting.\n- `error` keyword indicates an unrecoverable error, terminating the procedure.\n\n## 2.2 Analyzing algorithms\nAnalyzing an algorithm means predicting the resources it requires, typically computational time. This is done using a computational model, usually the *random-access machine (RAM)* model. In the RAM model:\n- Instructions execute one after another (no concurrency).\n- Basic instructions (arithmetic, data movement, control) take a constant amount of time.\n- Data types are integers, floating-point numbers, characters. Word size is assumed to be constant (e.g., $c \\lg n$ bits for input size $n$).\nThis model doesn't account for memory hierarchy (caches, virtual memory) but is often a good predictor of performance.\n\n### 2.2.1 Analysis of insertion sort\nLet $c_k$ be the cost of line $k$. Let $t_i$ be the number of times the `while` loop test in line 5 is executed for a given $i$.\nThe running time $T(n)$ of INSERTION-SORT is:\n$T(n) = c_1 n + c_2(n-1) + c_4(n-1) + c_5 \\sum_{i=2}^{n} t_i + c_6 \\sum_{i=2}^{n} (t_i-1) + c_7 \\sum_{i=2}^{n} (t_i-1) + c_8(n-1)$.\n\n- *Best case*: Array is already sorted. $t_i=1$ for all $i=2,...,n$. The `while` loop condition fails immediately.\n$T(n) = (c_1+c_2+c_4+c_5+c_8)n - (c_2+c_4+c_5+c_8)$. This is a linear function of $n$, so $T(n) = \\Theta(n)$.\n\n- *Worst case*: Array is in reverse sorted order. The `while` loop must compare `key` with every element in $A[1..i-1]$. So $t_i=i$ for $i=2,...,n$.\n$\\sum_{i=2}^{n} i = \\frac{n(n+1)}{2} - 1$.\n$\\sum_{i=2}^{n} (i-1) = \\frac{n(n-1)}{2}$.\n$T(n) = (\\frac{c_5}{2}+\\frac{c_6}{2}+\\frac{c_7}{2})n^2 + (c_1+c_2+c_4+\\frac{c_5}{2}-\\frac{c_6}{2}-\\frac{c_7}{2}+c_8)n - (c_2+c_4+c_5+c_8)$.\nThis is a quadratic function of $n$, so $T(n) = \\Theta(n^2)$.\n\n### 2.2.2 Worst-case and average-case analysis\n- *Worst-case running time*: Longest time for any input of size $n$. Provides an upper bound. Important for guarantees and real-time systems.\n- *Average-case running time*: Expected time over all inputs of size $n$, assuming a probability distribution. Often, the average case is roughly as bad as the worst case (e.g., for insertion sort, it's also $\\Theta(n^2)$ if all permutations are equally likely).\n\n### 2.2.3 Order of growth\nThe rate of growth, or *order of growth*, is the primary concern. We focus on the leading term of the formula (e.g., $an^2$ from $an^2+bn+c$) and ignore the constant coefficient $a$. This abstraction is captured by asymptotic notation (e.g., $\\Theta(n^2)$).\nAn algorithm with a lower order of growth is generally more efficient for large inputs.\n\n## 2.3 Designing algorithms\n\n### 2.3.1 The divide-and-conquer method\nMany algorithms are recursive. Divide-and-conquer is a common approach:\n1.  *Divide*: Break the problem into several subproblems that are similar to the original but smaller.\n2.  *Conquer*: Solve the subproblems recursively. If subproblems are small enough (base case), solve directly.\n3.  *Combine*: Merge the solutions of subproblems to create the solution for the original problem.\n\n*Merge Sort* algorithm follows this paradigm to sort a subarray $A[p..r]$:\n- *Divide*: If $p < r$, calculate $q = \\lfloor (p+r)/2 \\rfloor$. Divide $A[p..r]$ into $A[p..q]$ and $A[q+1..r]$.\n- *Conquer*: Recursively sort $A[p..q]$ and $A[q+1..r]$ using [[PartI Foundations Algorithms.md#2.3 MERGE-SORT]]. The base case is when $p \\ge r$ (subarray has 0 or 1 element, which is already sorted).\n- *Combine*: Merge the two sorted subarrays $A[p..q]$ and $A[q+1..r]$ into a single sorted subarray $A[p..r]$ using the auxiliary procedure [[PartI Foundations Algorithms.md#2.3 MERGE]]. The MERGE procedure takes two sorted piles of cards (subarrays) and merges them into a single sorted output pile. It copies $A[p..q]$ to $L$ and $A[q+1..r]$ to $R$, then iteratively picks the smaller of $L[i]$ and $R[j]$ to place into $A[k]$, advancing appropriate indices. This takes $\\Theta(n)$ time for $n = r-p+1$ elements.\n\n### 2.3.2 Analyzing divide-and-conquer algorithms\nRunning time is often described by a *recurrence equation*. For merge sort, sorting $n$ numbers:\n- *Divide*: Computing $q$ takes constant time, $\\Theta(1)$.\n- *Conquer*: Recursively solving two subproblems of size $n/2$ (approx.) takes $2T(n/2)$ time.\n- *Combine*: Merging $n$ elements takes $\\Theta(n)$ time.\nSo, the recurrence is $T(n) = 2T(n/2) + \\Theta(n)$. The base case is $T(n) = \\Theta(1)$ for small $n$ (e.g., $n=1$).\n\nThis recurrence can be solved using a *recursion tree*. For $T(n) = 2T(n/2) + cn$ (assuming $n$ is a power of 2 for simplicity):\n- Level 0 (root): cost $cn$.\n- Level 1: 2 nodes, each $T(n/2)$, cost $c(n/2)$ each. Total cost $2 \\cdot c(n/2) = cn$.\n- Level $i$: $2^i$ nodes, each problem size $n/2^i$, cost $c(n/2^i)$ each. Total cost $2^i \\cdot c(n/2^i) = cn$.\n- Last level (leaves): $n$ nodes (when problem size is 1), level $\\lg n$. Cost per node $T(1)=\\Theta(1)$ (say $c_1$). Total cost $c_1 n$.\nThere are $\\lg n + 1$ levels. Total cost is sum of costs at each level: $(\\lg n) \\cdot cn + c_1 n = cn \\lg n + c_1 n = \\Theta(n \\lg n)$.\nMerge sort's worst-case running time is $\\Theta(n \\lg n)$, significantly better than insertion sort's $\\Theta(n^2)$.\n"
            },
            {
                "file": "PartI Foundations/C3 Characterizing Running Times.md",
                "contents": "# 3 Characterizing Running Times\n\nThis chapter formally defines asymptotic notations used to characterize the order of growth of algorithm running times, simplifying analysis by focusing on behavior for large inputs.\n\n## 3.1 O-notation, $\\Omega$-notation, and $\\Theta$-notation (Informal Introduction)\nWhen analyzing insertion sort's worst-case running time, we derived a complex expression $an^2+bn+c$. We simplified this by discarding lower-order terms ($bn, c$) and the leading coefficient ($a$), resulting in $\\Theta(n^2)$.\n\n- *O-notation* ($O$): Describes an upper bound. A function $f(n)$ is $O(g(n))$ if $f(n)$ grows no faster than $g(n)$ (e.g., $7n^3+100n^2-20n+6$ is $O(n^3)$, also $O(n^4)$).\n- *$\\Omega$-notation* ($\\Omega$): Describes a lower bound. $f(n)$ is $\\Omega(g(n))$ if $f(n)$ grows at least as fast as $g(n)$ (e.g., $7n^3+100n^2-20n+6$ is $\\Omega(n^3)$, also $\\Omega(n^2)$).\n- *$\\Theta$-notation* ($\\Theta$): Describes a tight bound. $f(n)$ is $\\Theta(g(n))$ if $f(n)$ grows at the same rate as $g(n)$, up to constant factors (e.g., $7n^3+100n^2-20n+6$ is $\\Theta(n^3)$). $f(n)=\\Theta(g(n))$ if $f(n)=O(g(n))$ and $f(n)=\\Omega(g(n))$.\n\nExample: Insertion sort. Worst-case running time is $\\Theta(n^2)$. The outer `for` loop runs $n-1$ times. The inner `while` loop can run up to $i-1$ times for each $i$. So, total operations are $O(n^2)$. For a worst-case input (e.g., reverse sorted, first $n/3$ elements are the largest $n/3$), elements must move significantly, leading to $\\Omega(n^2)$ operations. Thus, worst-case is $\\Theta(n^2)$. Best-case is $\\Theta(n)$.\n\n## 3.2 Asymptotic notation: formal definitions\nFunctions are typically defined on natural numbers $\\mathbb{N}$.\n\n### 3.2.1 $O$-notation (Big-O notation)\n$O(g(n)) = \\{f(n) : \\text{there exist positive constants } c \\text{ and } n_0 \\text{ such that } 0 \\le f(n) \\le cg(n) \\text{ for all } n \\ge n_0\\}$.\n$f(n)$ must be asymptotically nonnegative. $g(n)$ must also be asymptotically nonnegative.\nExample: $4n^2+100n+500 = O(n^2)$ because for $c=5.05$ and $n_0=100$, $4n^2+100n+500 \\le 5.05n^2$.\n\n### 3.2.2 $\\Omega$-notation (Big-Omega notation)\n$\\Omega(g(n)) = \\{f(n) : \\text{there exist positive constants } c \\text{ and } n_0 \\text{ such that } 0 \\le cg(n) \\le f(n) \\text{ for all } n \\ge n_0\\}$.\nExample: $4n^2+100n+500 = \\Omega(n^2)$ because for $c=4$ and $n_0=1$, $4n^2 \\le 4n^2+100n+500$.\n\n### 3.2.3 $\\Theta$-notation (Theta-notation)\n$\\Theta(g(n)) = \\{f(n) : \\text{there exist positive constants } c_1, c_2, \\text{ and } n_0 \\text{ such that } 0 \\le c_1 g(n) \\le f(n) \\le c_2 g(n) \\text{ for all } n \\ge n_0\\}$.\n*Theorem 3.1*: For any two functions $f(n)$ and $g(n)$, $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$.\nWhen characterizing running times, use the most precise bound possible (often $\\Theta$) and specify if it's worst-case, best-case, or applies to all cases (e.g., merge sort's running time is $\\Theta(n \\lg n)$ for all cases).\n\n### 3.2.4 Asymptotic notation in equations and inequalities\n$f(n) = O(g(n))$ means $f(n) \\in O(g(n))$.\nIn a formula like $2n^2 + 3n + 1 = 2n^2 + \\Theta(n)$, $\\Theta(n)$ stands for an anonymous function $h(n) \\in \\Theta(n)$ (here, $h(n)=3n+1$).\nAn equation like $2n^2 + \\Theta(n) = \\Theta(n^2)$ means that for any function $f(n) \\in \\Theta(n)$ on the left, there exists a function $g(n) \\in \\Theta(n^2)$ on the right that makes the equation true.\n\n### 3.2.5 Proper abuses of asymptotic notation\n- The variable tending to $\\infty$ is inferred from context (e.g., $f(n)=O(1)$ means $f(n)$ is bounded by a constant as $n \\to \\infty$).\n- $T(n) = O(1)$ for $n < k$ means $\\exists c > 0$ s.t. $T(n) \\le c$ for $n < k$.\n\n### 3.2.6 $o$-notation (Little-o notation)\nAn upper bound that is not asymptotically tight.\n$o(g(n)) = \\{f(n) : \\text{for any positive constant } c > 0, \\text{ there exists a constant } n_0 > 0 \\text{ such that } 0 \\le f(n) < cg(n) \\text{ for all } n \\ge n_0\\}$.\nIntuitively, $f(n)$ becomes insignificant relative to $g(n)$ as $n$ gets large. $\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = 0$.\nExample: $2n = o(n^2)$, but $2n^2 \\neq o(n^2)$.\n\n### 3.2.7 $\\omega$-notation (Little-omega notation)\nA lower bound that is not asymptotically tight.\n$\\omega(g(n)) = \\{f(n) : \\text{for any positive constant } c > 0, \\text{ there exists a constant } n_0 > 0 \\text{ such that } 0 \\le cg(n) < f(n) \\text{ for all } n \\ge n_0\\}$.\nIntuitively, $f(n)$ becomes arbitrarily large relative to $g(n)$ as $n$ gets large. $\\lim_{n\\to\\infty} \\frac{f(n)}{g(n)} = \\infty$.\nAlso, $f(n) \\in \\omega(g(n)) \\iff g(n) \\in o(f(n))$.\nExample: $n^2/2 = \\omega(n)$, but $n^2/2 \\neq \\omega(n^2)$.\n\n### 3.2.8 Comparing functions (Properties for asymptotically positive $f, g$)\n- *Transitivity*: Holds for $\\Theta, O, \\Omega, o, \\omega$.\n- *Reflexivity*: $f(n) = \\Theta(f(n))$, $f(n) = O(f(n))$, $f(n) = \\Omega(f(n))$.\n- *Symmetry*: $f(n) = \\Theta(g(n)) \\iff g(n) = \\Theta(f(n))$.\n- *Transpose symmetry*: $f(n) = O(g(n)) \\iff g(n) = \\Omega(f(n))$; $f(n) = o(g(n)) \\iff g(n) = \\omega(f(n))$.\n*Trichotomy* ($a<b, a=b, \text{or } a>b$) does not hold for functions; e.g., $n$ and $n^{1+\\sin n}$ are not asymptotically comparable.\n\n## 3.3 Standard notations and common functions\n\n### 3.3.1 Monotonicity\n- *Monotonically increasing*: $m \\le n \\implies f(m) \\le f(n)$.\n- *Monotonically decreasing*: $m \\le n \\implies f(m) \\ge f(n)$.\n- *Strictly increasing*: $m < n \\implies f(m) < f(n)$.\n- *Strictly decreasing*: $m < n \\implies f(m) > f(n)$.\n\n### 3.3.2 Floors and ceilings\n- $\\lfloor x \\rfloor$: greatest integer $\\le x$.\n- $\\lceil x \\rceil$: least integer $\\ge x$.\nProperties: $x-1 < \\lfloor x \\rfloor \\le x \\le \\lceil x \\rceil < x+1$. For integer $n$, $\\lfloor n \\rfloor = \\lceil n \\rceil = n$.\n\n### 3.3.3 Modular arithmetic\n$a \\bmod n = a - n \\lfloor a/n \\rfloor$. So $0 \\le a \\bmod n < n$.\n$a \\equiv b \\pmod n$ if $(a \\bmod n) = (b \\bmod n)$.\n\n### 3.3.4 Polynomials\nA polynomial in $n$ of degree $d$ is $p(n) = \\sum_{i=0}^d a_i n^i$, where $a_d \\neq 0$. For asymptotically positive $p(n)$, $p(n) = \\Theta(n^d)$.\nA function $f(n)$ is *polynomially bounded* if $f(n) = O(n^k)$ for some constant $k$.\n\n### 3.3.5 Exponentials\nIdentities: $a^0=1, a^1=a, a^{-1}=1/a, (a^m)^n=a^{mn}, (a^m)^n=(a^n)^m, a^m a^n = a^{m+n}$.\nFor $a>1, b$, $\\lim_{n\\to\\infty} \\frac{n^b}{a^n} = 0$, so $n^b = o(a^n)$. Any positive exponential function grows faster than any polynomial.\nFor all real $x$, $e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + ... = \\sum_{i=0}^{\\infty} \\frac{x^i}{i!}$. Also, $1+x \\le e^x$.\n\n### 3.3.6 Logarithms\nNotations: $\\lg n = \\log_2 n$ (binary), $\\ln n = \\log_e n$ (natural), $\\lg^k n = (\\lg n)^k$, $\\lg \\lg n = \\lg(\\lg n)$.\nConvention: $\\lg n+1$ means $(\\lg n)+1$.\nProperties: $a=b^{\\log_b a}$, $\\log_c(ab) = \\log_c a + \\log_c b$, $\\log_b a^n = n \\log_b a$, $\\log_b a = \\frac{\\log_c a}{\\log_c b}$.\nChanging log base only changes value by a constant factor.\nFor $a>0, b$, $\\lg^b n = o(n^a)$. Any positive polynomial function grows faster than any polylogarithmic function.\n\n### 3.3.7 Factorials\n$n! = \\begin{cases} 1 & \\text{if } n=0 \\\\ n \\cdot (n-1)! & \\text{if } n > 0 \\end{cases}$. So $n! = 1 \\cdot 2 \\cdot ... \\cdot n$.\nStirling's approximation: $n! = \\sqrt{2\\pi n} (\\frac{n}{e})^n (1 + \\Theta(\\frac{1}{n}))$.\nBounds: $n! = o(n^n)$, $n! = \\omega(2^n)$, $\\lg(n!) = \\Theta(n \\lg n)$.\n\n### 3.3.8 Functional iteration\n$f^{(i)}(n)$ is $f(n)$ applied $i$ times to $n$. $f^{(0)}(n)=n$. $f^{(i)}(n) = f(f^{(i-1)}(n))$ for $i>0$.\n\n### 3.3.9 The iterated logarithm function\n$\\lg^* n = \\min \\{i \\ge 0 : \\lg^{(i)} n \\le 1\\}$. This function grows extremely slowly. For practical purposes, $\\lg^* n \\le 5$.\n\n### 3.3.10 Fibonacci numbers\n$F_0=0, F_1=1, F_i = F_{i-1} + F_{i-2}$ for $i \\ge 2$. Sequence: $0,1,1,2,3,5,8,...$\nRelated to golden ratio $\\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.61803$ and its conjugate $\\hat{\\phi} = \\frac{1-\\sqrt{5}}{2} \\approx -0.61803$.\n$F_i = \\frac{\\phi^i - \\hat{\\phi}^i}{\\sqrt{5}}$. Since $|\\hat{\\phi}| < 1$, $F_i$ is $\\frac{\\phi^i}{\\sqrt{5}}$ rounded to the nearest integer. Fibonacci numbers grow exponentially: $F_i = \\Theta(\\phi^i)$.\n"
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartIII Data Structures Algorithms.md",
                "contents": "# Part III Data Structures Algorithms\n\n# C10 Elementary Data Structures Algorithms\n\n## 10.1.3 STACK-EMPTY (S)\n```\nSTACK-EMPTY (S)\n1 if S.top == 0\n2   return TRUE\n3 else return FALSE\n```\n\n## 10.1.3 PUSH (S, x)\n```\nPUSH(S, x)\n1 if S.top == S.size\n2   error \"overflow\"\n3 else S.top = S.top + 1\n4   S[S.top] = x\n```\n\n## 10.1.3 POP (S)\n```\nPOP(S)\n1 if STACK-EMPTY(S)\n2   error \"underflow\"\n3 else S.top = S.top - 1\n4   return S[S.top + 1]\n```\n\n## 10.1.3 ENQUEUE (Q, x)\n```\nENQUEUE(Q, x)\n1 Q[Q.tail] = x\n2 if Q.tail == Q.size\n3   Q.tail = 1\n4 else Q.tail = Q.tail + 1\n```\n\n## 10.1.3 DEQUEUE (Q)\n```\nDEQUEUE(Q)\n1 x = Q[Q.head]\n2 if Q.head == Q.size\n3   Q.head = 1\n4 else Q.head = Q.head + 1\n5 return x\n```\n\n## 10.2 LIST-SEARCH (L, k)\n```\nLIST-SEARCH(L, k)\n1 x = L.head\n2 while x != NIL and x.key != k\n3   x = x.next\n4 return x\n```\n\n## 10.2 LIST-PREPEND (L, x)\n```\nLIST-PREPEND(L, x)\n1 x.next = L.head\n2 x.prev = NIL\n3 if L.head != NIL\n4   L.head.prev = x\n5 L.head = x\n```\n\n## 10.2 LIST-INSERT (x, y)\n```\nLIST-INSERT(x, y)\n1 x.next = y.next\n2 x.prev = y\n3 if y.next != NIL\n4   y.next.prev = x\n5 y.next = x\n```\n\n## 10.2 LIST-DELETE (L, x)\n```\nLIST-DELETE(L, x)\n1 if x.prev != NIL\n2   x.prev.next = x.next\n3 else L.head = x.next\n4 if x.next != NIL\n5   x.next.prev = x.prev\n```\n\n## 10.2 LIST-DELETE' (x) (with sentinel)\n```\nLIST-DELETE'(x)\n1 x.prev.next = x.next\n2 x.next.prev = x.prev\n```\n\n## 10.2 LIST-INSERT' (x, y) (with sentinel)\n```\nLIST-INSERT'(x, y)\n1 x.next = y.next\n2 x.prev = y\n3 y.next.prev = x\n4 y.next = x\n```\n\n## 10.2 LIST-SEARCH' (L, k) (with sentinel)\n```\nLIST-SEARCH'(L, k)\n1 L.nil.key = k // store the key in the sentinel to guarantee it is in list\n2 x = L.nil.next // start at the head of the list\n3 while x.key != k\n4   x = x.next\n5 if x == L.nil // found k in the sentinel\n6   return NIL // k was not really in the list\n7 else return x // found k in element x\n```\n\n# C11 Hash Tables Algorithms\n\n## 11.1 DIRECT-ADDRESS-SEARCH (T, k)\n```\nDIRECT-ADDRESS-SEARCH(T, k)\n1 return T[k]\n```\n\n## 11.1 DIRECT-ADDRESS-INSERT (T, x)\n```\nDIRECT-ADDRESS-INSERT(T, x)\n1 T[x.key] = x\n```\n\n## 11.1 DIRECT-ADDRESS-DELETE (T, x)\n```\nDIRECT-ADDRESS-DELETE(T, x)\n1 T[x.key] = NIL\n```\n\n## 11.2 CHAINED-HASH-INSERT (T, x)\n```\nCHAINED-HASH-INSERT(T, x)\n1 LIST-PREPEND(T[h(x.key)], x)\n```\n\n## 11.2 CHAINED-HASH-SEARCH (T, k)\n```\nCHAINED-HASH-SEARCH(T, k)\n1 return LIST-SEARCH(T[h(k)], k)\n```\n\n## 11.2 CHAINED-HASH-DELETE (T, x)\n```\nCHAINED-HASH-DELETE(T, x)\n1 LIST-DELETE(T[h(x.key)], x)\n```\n\n## 11.4 HASH-INSERT (T, k)\n```\nHASH-INSERT(T, k)\n1 i = 0\n2 repeat\n3   q = h(k, i)\n4   if T[q] == NIL\n5     T[q] = k\n6     return q\n7   else i = i + 1\n8 until i == m\n9 error \"hash table overflow\"\n```\n\n## 11.4 HASH-SEARCH (T, k)\n```\nHASH-SEARCH(T, k)\n1 i = 0\n2 repeat\n3   q = h(k, i)\n4   if T[q] == k\n5     return q\n6   i = i + 1\n7 until T[q] == NIL or i == m\n8 return NIL\n```\n\n## 11.5.1 LINEAR-PROBING-HASH-DELETE (T, q)\n```\nLINEAR-PROBING-HASH-DELETE(T, q)\n1 while TRUE\n2   T[q] = NIL // make slot q empty\n3   q' = q // starting point for search\n4   repeat\n5     q' = (q' + 1) mod m // next slot number with linear probing\n6     k' = T[q'] // next key to try to move\n7     if k' == NIL\n8       return // return when an empty slot is found\n9   until g(k', q) < g(k', q') // was empty slot q probed before q'?\n10  T[q] = k' // move k' into slot q\n11  q = q' // free up slot q'\n```\n\n## 11.5.2 WEE (k, a, b, t, r, m)\n```\nWEE(k, a, b, t, r, m)\n1 u = ceil(t/w)\n2 <k1, k2, ..., ku> = chop(k)\n3 q = b\n4 for i = 1 to u\n5   q = f^(r)_(a+2t)(ki + q)\n6 return q mod m\n```\n\n# C12 Binary Search Trees Algorithms\n\n## 12.1 INORDER-TREE-WALK (x)\n```\nINORDER-TREE-WALK(x)\n1 if x != NIL\n2   INORDER-TREE-WALK(x.left)\n3   print x.key\n4   INORDER-TREE-WALK(x.right)\n```\n\n## 12.2 TREE-SEARCH (x, k)\n```\nTREE-SEARCH(x, k)\n1 if x == NIL or k == x.key\n2   return x\n3 if k < x.key\n4   return TREE-SEARCH(x.left, k)\n5 else return TREE-SEARCH(x.right, k)\n```\n\n## 12.2 ITERATIVE-TREE-SEARCH (x, k)\n```\nITERATIVE-TREE-SEARCH(x, k)\n1 while x != NIL and k != x.key\n2   if k < x.key\n3     x = x.left\n4   else x = x.right\n5 return x\n```\n\n## 12.2 TREE-MINIMUM (x)\n```\nTREE-MINIMUM(x)\n1 while x.left != NIL\n2   x = x.left\n3 return x\n```\n\n## 12.2 TREE-MAXIMUM (x)\n```\nTREE-MAXIMUM(x)\n1 while x.right != NIL\n2   x = x.right\n3 return x\n```\n\n## 12.2 TREE-SUCCESSOR (x)\n```\nTREE-SUCCESSOR(x)\n1 if x.right != NIL\n2   return TREE-MINIMUM(x.right) // leftmost node in right subtree\n3 else // find the lowest ancestor of x whose left child is an ancestor of x\n4   y = x.p\n5   while y != NIL and x == y.right\n6     x = y\n7     y = y.p\n8   return y\n```\n\n## 12.3 TREE-INSERT (T, z)\n```\nTREE-INSERT(T, z)\n1 x = T.root // node being compared with z\n2 y = NIL // y will be parent of z\n3 while x != NIL // descend until reaching a leaf\n4   y = x\n5   if z.key < x.key\n6     x = x.left\n7   else x = x.right\n8 z.p = y // found the location\u2014insert z with parent y\n9 if y == NIL\n10  T.root = z // tree T was empty\n11 elseif z.key < y.key\n12  y.left = z\n13 else y.right = z\n```\n\n## 12.3 TRANSPLANT (T, u, v)\n```\nTRANSPLANT(T, u, v)\n1 if u.p == NIL\n2   T.root = v\n3 elseif u == u.p.left\n4   u.p.left = v\n5 else u.p.right = v\n6 if v != NIL\n7   v.p = u.p\n```\n\n## 12.3 TREE-DELETE (T, z)\n```\nTREE-DELETE(T, z)\n1 if z.left == NIL\n2   TRANSPLANT(T, z, z.right) // replace z by its right child\n3 elseif z.right == NIL\n4   TRANSPLANT(T, z, z.left) // replace z by its left child\n5 else y = TREE-MINIMUM(z.right) // y is z\u2019s successor\n6   if y != z.right // is y farther down the tree?\n7     TRANSPLANT(T, y, y.right) // replace y by its right child\n8     y.right = z.right // z\u2019s right child becomes\n9     y.right.p = y // y\u2019s right child\n10  TRANSPLANT(T, z, y) // replace z by its successor y\n11  y.left = z.left // and give z\u2019s left child to y,\n12  y.left.p = y // which had no left child\n```\n\n# C13 Red-Black Trees Algorithms\n\n## 13.2 LEFT-ROTATE (T, x)\n```\nLEFT-ROTATE(T, x)\n1 y = x.right\n2 x.right = y.left // turn y\u2019s left subtree into x\u2019s right subtree\n3 if y.left != T.nil // if y\u2019s left subtree is not empty ...\n4   y.left.p = x // ... then x becomes the parent of the subtree\u2019s root\n5 y.p = x.p // x\u2019s parent becomes y\u2019s parent\n6 if x.p == T.nil // if x was the root ...\n7   T.root = y // ... then y becomes the root\n8 elseif x == x.p.left // otherwise, if x was a left child ...\n9   x.p.left = y // ... then y becomes a left child\n10 else x.p.right = y // otherwise, x was a right child, and now y is\n11 y.left = x // make x become y\u2019s left child\n12 x.p = y\n```\n\n## 13.2 RIGHT-ROTATE (T, y)\n```\nRIGHT-ROTATE(T, y)\n1 x = y.left\n2 y.left = x.right // turn x\u2019s right subtree into y\u2019s left subtree\n3 if x.right != T.nil // if x\u2019s right subtree is not empty ...\n4   x.right.p = y // ... then y becomes the parent of the subtree\u2019s root\n5 x.p = y.p // y\u2019s parent becomes x\u2019s parent\n6 if y.p == T.nil // if y was the root ...\n7   T.root = x // ... then x becomes the root\n8 elseif y == y.p.right // otherwise, if y was a right child ...\n9   y.p.right = x // ... then x becomes a right child\n10 else y.p.left = x // otherwise, y was a left child, and now x is\n11 x.right = y // make y become x\u2019s right child\n12 y.p = x\n```\n\n## 13.3 RB-INSERT (T, z)\n```\nRB-INSERT(T, z)\n1 x = T.root // node being compared with z\n2 y = T.nil // y will be parent of z\n3 while x != T.nil // descend until reaching the sentinel\n4   y = x\n5   if z.key < x.key\n6     x = x.left\n7   else x = x.right\n8 z.p = y // found the location\u2014insert z with parent y\n9 if y == T.nil\n10  T.root = z // tree T was empty\n11 elseif z.key < y.key\n12  y.left = z\n13 else y.right = z\n14 z.left = T.nil // both of z\u2019s children are the sentinel\n15 z.right = T.nil\n16 z.color = RED // the new node starts out red\n17 RB-INSERT-FIXUP(T, z) // correct any violations of red-black properties\n```\n\n## 13.3 RB-INSERT-FIXUP (T, z)\n```\nRB-INSERT-FIXUP(T, z)\n1 while z.p.color == RED\n2   if z.p == z.p.p.left // is z\u2019s parent a left child?\n3     y = z.p.p.right // y is z\u2019s uncle\n4     if y.color == RED // are z\u2019s parent and uncle both red? (Case 1)\n5       z.p.color = BLACK\n6       y.color = BLACK\n7       z.p.p.color = RED\n8       z = z.p.p\n9     else \n10      if z == z.p.right // (Case 2)\n11        z = z.p\n12        LEFT-ROTATE(T, z)\n13      z.p.color = BLACK // (Case 3)\n14      z.p.p.color = RED\n15      RIGHT-ROTATE(T, z.p.p)\n16  else // same as lines 3-15, but with \"right\" and \"left\" exchanged\n17    y = z.p.p.left\n18    if y.color == RED\n19      z.p.color = BLACK\n20      y.color = BLACK\n21      z.p.p.color = RED\n22      z = z.p.p\n23    else\n24      if z == z.p.left\n25        z = z.p\n26        RIGHT-ROTATE(T, z)\n27      z.p.color = BLACK\n28      z.p.p.color = RED\n29      LEFT-ROTATE(T, z.p.p)\n30 T.root.color = BLACK\n```\n\n## 13.4 RB-TRANSPLANT (T, u, v)\n```\nRB-TRANSPLANT(T, u, v)\n1 if u.p == T.nil\n2   T.root = v\n3 elseif u == u.p.left\n4   u.p.left = v\n5 else u.p.right = v\n6 v.p = u.p\n```\n\n## 13.4 RB-DELETE (T, z)\n```\nRB-DELETE(T, z)\n1  y = z\n2  y-original-color = y.color\n3  if z.left == T.nil\n4    x = z.right\n5    RB-TRANSPLANT(T, z, z.right)\n6  elseif z.right == T.nil\n7    x = z.left\n8    RB-TRANSPLANT(T, z, z.left)\n9  else y = TREE-MINIMUM(z.right)\n10   y-original-color = y.color\n11   x = y.right\n12   if y != z.right \n13     RB-TRANSPLANT(T, y, y.right)\n14     y.right = z.right\n15     y.right.p = y\n16   else x.p = y \n17   RB-TRANSPLANT(T, z, y)\n18   y.left = z.left\n19   y.left.p = y\n20   y.color = z.color\n21 if y-original-color == BLACK\n22  RB-DELETE-FIXUP(T, x)\n```\n\n## 13.4 RB-DELETE-FIXUP (T, x)\n```\nRB-DELETE-FIXUP(T, x)\n1  while x != T.root and x.color == BLACK\n2    if x == x.p.left\n3      w = x.p.right\n4      if w.color == RED // Case 1\n5        w.color = BLACK\n6        x.p.color = RED\n7        LEFT-ROTATE(T, x.p)\n8        w = x.p.right\n9      if w.left.color == BLACK and w.right.color == BLACK // Case 2\n10       w.color = RED\n11       x = x.p\n12     else \n13       if w.right.color == BLACK // Case 3\n14         w.left.color = BLACK\n15         w.color = RED\n16         RIGHT-ROTATE(T, w)\n17         w = x.p.right\n18       w.color = x.p.color // Case 4\n19       x.p.color = BLACK\n20       w.right.color = BLACK\n21       LEFT-ROTATE(T, x.p)\n22       x = T.root\n23   else // same as lines 3-22, but with \"right\" and \"left\" exchanged\n24     w = x.p.left\n25     if w.color == RED\n26       w.color = BLACK\n27       x.p.color = RED\n28       RIGHT-ROTATE(T, x.p)\n29       w = x.p.left\n30     if w.right.color == BLACK and w.left.color == BLACK\n31       w.color = RED\n32       x = x.p\n33     else \n34       if w.left.color == BLACK\n35         w.right.color = BLACK\n36         w.color = RED\n37         LEFT-ROTATE(T, w)\n38         w = x.p.left\n39       w.color = x.p.color\n40       x.p.color = BLACK\n41       w.left.color = BLACK\n42       RIGHT-ROTATE(T, x.p)\n43       x = T.root\n44 x.color = BLACK\n```\n"
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": "# Part III Data Structures Introduction\n\nSets are fundamental to computer science. Mathematical sets are unchanging, but algorithms manipulate **dynamic sets** that can grow, shrink, or change over time. This part presents basic techniques for representing finite dynamic sets and manipulating them on a computer.\n\nOperations on sets can include inserting elements, deleting elements, and testing membership. A dynamic set supporting these is a **dictionary**. More complicated operations might be needed, like in min-priority queues (Chapter 6) which support inserting an element and extracting the smallest element. The best way to implement a dynamic set depends on the required operations.\n\n## Elements of a dynamic set\nEach element is typically an object with attributes. Some dynamic sets assume one attribute is an identifying **key**. If keys are unique, the set can be viewed as a set of key values. Objects may contain **satellite data**, carried with the object but unused by set implementation, and other attributes manipulated by set operations (data or pointers to other objects in the set).\nSome dynamic sets assume keys are from a totally ordered set (e.g., real numbers, words under alphabetic order), allowing definition of minimum/maximum elements or next/previous element.\n\n## Operations on dynamic sets\nOperations are grouped into **queries** (return information) and **modifying operations** (change the set).\nTypical operations:\n- `SEARCH(S, k)`: Query, returns a pointer to an element x in S with x.key = k, or NIL.\n- `INSERT(S, x)`: Modifying, adds element x to S. Assumes attributes needed by implementation are initialized.\n- `DELETE(S, x)`: Modifying, removes element x from S (takes pointer, not key value).\n- `MINIMUM(S)` and `MAXIMUM(S)`: Queries on totally ordered set S, return pointer to element with smallest/largest key.\n- `SUCCESSOR(S, x)`: Query on totally ordered set S, returns pointer to next larger element than x, or NIL if x is maximum.\n- `PREDECESSOR(S, x)`: Query on totally ordered set S, returns pointer to next smaller element than x, or NIL if x is minimum.\n`SUCCESSOR` and `PREDECESSOR` can be extended to sets with nondistinct keys. `MINIMUM` followed by $n-1$ calls to `SUCCESSOR` enumerates elements in sorted order.\nTime complexity is usually measured in terms of set size $n$.\n\n## Overview of Part III Chapters\n- **Chapter 10 (Elementary Data Structures)**: Essentials of arrays, matrices, stacks, queues, linked lists, and rooted trees. Much may be familiar from introductory programming.\n- **Chapter 11 (Hash Tables)**: Widely used for dictionary operations. Worst-case search $\\Theta(n)$, but expected time is $O(1)$ under reasonable assumptions.\n- **Chapter 12 (Binary Search Trees)**: Support all dynamic-set operations. Worst-case $\\Theta(n)$ time per operation. Basis for many other data structures.\n- **Chapter 13 (Red-Black Trees)**: Variant of binary search trees. Guaranteed $O(\\lg n)$ worst-case time for operations. A balanced search tree.\n\nImplementing a dynamic set with an array:\n- Advantage: Simple algorithms.\n- Downside: Many operations $\\Theta(n)$ worst-case time.\n  - Unsorted array: `INSERT` and `DELETE` (if deleting last element or order doesn't matter for deletion position) can be $\\Theta(1)$, others $\\Theta(n)$.\n  - Sorted array: `MINIMUM`, `MAXIMUM`, `SUCCESSOR`, `PREDECESSOR` take $\\Theta(1)$; `SEARCH` takes $O(\\lg n)$ (binary search); `INSERT` and `DELETE` take $\\Theta(n)$.\nData structures in this part improve upon array implementations for many dynamic-set operations.\n\n# 10 Elementary Data Structures\nThis chapter examines representation of dynamic sets by simple data structures using pointers: arrays, matrices, stacks, queues, linked lists, and rooted trees.\n\n## 10.1 Simple array-based data structures: arrays, matrices, stacks, queues\n\n### 10.1.1 Arrays\nAn array is stored as a contiguous sequence of bytes in memory. If the first element has index $s$, starts at memory address $a$, and each element occupies $b$ bytes, the $i$-th element occupies bytes $a+b(i-s)$ through $a+b(i-s+1)-1$. Accessing any array element takes constant time (RAM model assumption).\nMost languages require elements of an array to be the same size. If elements vary in size, array elements are usually pointers to objects, and pointer sizes are constant.\n\n### 10.1.2 Matrices\nA matrix (2D array) is typically represented by one or more 1D arrays. For an $m \\times n$ matrix:\n- **Row-major order**: Matrix stored row by row.\n- **Column-major order**: Matrix stored column by column.\nFor an $m \\times n$ matrix $M$ with 0-origin indexing, $M[i, j]$ is at index $ni+j$ (row-major) or $i+mj$ (column-major) in a single 1D array.\nMultiple-array strategies: An array of pointers, where each pointer points to an array representing a row (for row-major) or a column (for column-major). These can be more flexible (e.g., ragged arrays) but single-array representations are often more efficient.\n**Block representation**: Matrix divided into blocks, each block stored contiguously.\n\n### 10.1.3 Stacks and queues\nDynamic sets where the element removed by `DELETE` is prespecified.\n- **Stack**: Implements a last-in, first-out (LIFO) policy. `INSERT` is `PUSH`, `DELETE` (no argument) is `POP`.\n- **Queue**: Implements a first-in, first-out (FIFO) policy. `INSERT` is `ENQUEUE`, `DELETE` (no argument) is `DEQUEUE`.\n\n**Stacks**\nImplemented with an array $S[1..n]$. Attributes: $S.top$ (index of most recently inserted element) and $S.size$ (size $n$ of array). Elements $S[1..S.top]$.\n- $S.top = 0$: stack is empty.\n- Test for empty: [[PartIII Data Structures Algorithms.md#10.1.3 STACK-EMPTY]].\n- Pop from empty stack: **underflow**.\n- Push to full stack ($S.top = S.size$): **overflow**.\nOperations [[PartIII Data Structures Algorithms.md#10.1.3 PUSH (S, x)]] and [[PartIII Data Structures Algorithms.md#10.1.3 POP (S)]] take $O(1)$ time.\n\n**Queues**\nImplemented with an array $Q[1..n]$. Attributes: $Q.size$ (size $n$ of array), $Q.head$ (indexes head), $Q.tail$ (indexes next location for new element).\nElements are in $Q.head, Q.head+1, \\dots, Q.tail-1$, with wrap-around (location 1 follows $n$).\n- $Q.head = Q.tail$: queue is empty. Initially $Q.head = Q.tail = 1$.\n- Dequeue from empty queue: **underflow**.\n- Enqueue to full queue ($Q.head = Q.tail+1$ or ($Q.head=1$ and $Q.tail=Q.size$)): **overflow**. A queue can hold at most $n-1$ elements with this setup.\nOperations [[PartIII Data Structures Algorithms.md#10.1.3 ENQUEUE (Q, x)]] and [[PartIII Data Structures Algorithms.md#10.1.3 DEQUEUE (Q)]] take $O(1)$ time. Error checking for underflow/overflow is omitted in book pseudocode but Exercise 10.1-5 asks to add it.\n\n## 10.2 Linked lists\nA data structure where objects are in linear order determined by a pointer in each object.\nEach element of a **doubly linked list** $L$ is an object with `key`, `next` pointer, and `prev` pointer. May also contain satellite data.\n- `x.prev = NIL`: $x$ is the **head** (first element).\n- `x.next = NIL`: $x$ is the **tail** (last element).\n- `L.head`: points to the first element. If `L.head = NIL`, list is empty.\nForms of lists:\n- **Singly linked**: elements have `next` pointer but not `prev`.\n- **Sorted**: linear order of list corresponds to linear order of keys.\n- **Unsorted**: elements can appear in any order.\n- **Circular**: `prev` pointer of head points to tail, `next` pointer of tail points to head.\nThis section assumes unsorted, doubly linked lists.\n\n**Searching a linked list**\nProcedure [[PartIII Data Structures Algorithms.md#10.2 LIST-SEARCH (L, k)]] finds first element with key $k$ by linear search. Takes $\\Theta(n)$ time in worst case for an $n$-object list.\n\n**Inserting into a linked list**\nProcedure [[PartIII Data Structures Algorithms.md#10.2 LIST-PREPEND (L, x)]] adds $x$ to front of list in $O(1)$ time.\nProcedure [[PartIII Data Structures Algorithms.md#10.2 LIST-INSERT (x, y)]] inserts element $x$ immediately following element $y$ in $O(1)$ time. (Assumes pointer $y$ is given).\n\n**Deleting from a linked list**\nProcedure [[PartIII Data Structures Algorithms.md#10.2 LIST-DELETE (L, x)]] removes element $x$ (given by pointer) in $O(1)$ time. To delete an element with a given key, first call `LIST-SEARCH`, making worst-case $\\Theta(n)$.\nInsertion/deletion are faster on doubly linked lists ($O(1)$ if pointer given) than arrays ($\\Theta(n)$ for first element if order maintained). Finding $k$-th element is $O(1)$ in array, $\\Theta(k)$ in list.\n\n**Sentinels**\nA **sentinel** is a dummy object to simplify boundary conditions. For list $L$, $L.nil$ represents NIL but has all attributes of other objects. Turns a doubly linked list into a **circular, doubly linked list with a sentinel**.\n- $L.nil$ lies between head and tail.\n- $L.nil.next$ points to head. $L.nil.prev$ points to tail.\n- `next` of tail and `prev` of head point to $L.nil$.\n- $L.head$ attribute can be eliminated, use $L.nil.next$.\n- Empty list: $L.nil.next$ and $L.nil.prev$ point to $L.nil$.\nSimplified procedures (denoted with prime):\n- [[PartIII Data Structures Algorithms.md#10.2 LIST-DELETE' (x) (with sentinel)]]: $O(1)$.\n- [[PartIII Data Structures Algorithms.md#10.2 LIST-INSERT' (x, y) (with sentinel)]]: $O(1)$. To insert at head, let $y = L.nil$. To insert at tail, let $y = L.nil.prev$.\n- [[PartIII Data Structures Algorithms.md#10.2 LIST-SEARCH' (L, k) (with sentinel)]]: Stores key $k$ in sentinel before search. Loop compares only $x.key \ne k$. If $x$ becomes $L.nil$, key wasn't in original list. Can decrease constant factor in search time. Sentinels use extra storage, use judiciously.\n\n## 10.3 Representing rooted trees\nLinked data structures for rooted trees. Each node is an object with `key` and pointers.\n\n**Binary trees**\nAttributes $p, left, right$ for parent, left child, right child. If $x.p = NIL$, $x$ is root. If no left/right child, corresponding attribute is NIL. Tree attribute $T.root$ points to root. If $T.root = NIL$, tree is empty.\n\n**Rooted trees with unbounded branching**\nIf max children $k$ is small constant, can use $child_1, \text{child}_2, \text{...}, child_k$. Not suitable for unbounded or large $k$.\n**Left-child, right-sibling representation**: Uses $O(n)$ space for $n$-node tree.\nEach node $x$ has parent pointer $p$, and T.root points to root. Node $x$ has two pointers:\n1. $x.left\text{-}child$: points to leftmost child of $x$.\n2. $x.right\text{-}sibling$: points to sibling of $x$ immediately to its right.\nIf $x$ has no children, $x.left\text{-}child = NIL$. If $x$ is rightmost child, $x.right\text{-}sibling = NIL$.\n\n**Other tree representations**\nExample: Heap (Chapter 6) represented by single array. Trees in Chapter 19 (for disjoint sets) traversed only toward root, so only parent pointers. Choice depends on application.\n"
            },
            {
                "file": "PartIII Data Structures/C11 Hash Tables.md",
                "contents": "# 11 Hash Tables\nMany applications require a dynamic set supporting only dictionary operations: `INSERT`, `SEARCH`, `DELETE`. A **hash table** is an effective data structure for dictionaries. Average time for search is $O(1)$ under reasonable assumptions, though worst-case can be $\\Theta(n)$.\nA hash table generalizes an array. Direct addressing (Section 11.1) uses a key as a direct index into an array. Hash tables compute an array index from the key using a **hash function**, typically using an array size proportional to number of keys stored, not total possible keys.\n\n## 11.1 Direct-address tables\nWorks well when universe $U$ of keys is small. Dynamic set elements have distinct keys from $U = \\{0, 1, \\dots, m-1\\}$.\nUse a **direct-address table** (array) $T[0..m-1]$. Each position (slot) corresponds to a key in $U$.\n- Slot $k$ points to element with key $k$. If no such element, $T[k] = NIL$.\nDictionary operations:\n- [[PartIII Data Structures Algorithms.md#11.1 DIRECT-ADDRESS-SEARCH (T, k)]]\n- [[PartIII Data Structures Algorithms.md#11.1 DIRECT-ADDRESS-INSERT (T, x)]]\n- [[PartIII Data Structures Algorithms.md#11.1 DIRECT-ADDRESS-DELETE (T, x)]]\nEach takes $O(1)$ time. Direct-address table can store elements themselves or pointers. Special key can indicate empty slot, or index implies key. A bit vector can represent set of distinct elements with no satellite data.\n\n## 11.2 Hash tables\nWhen universe $U$ is large, direct addressing is impractical. If set $K$ of stored keys is much smaller than $U$, hash table storage is $\\Theta(|K|)$. Search is $O(1)$ on average.\nA **hash function** $h$ maps keys from $U$ to slots in hash table $T[0..m-1]$: $h: U \\rightarrow \\{0, 1, \\dots, m-1\\}$.\nElement with key $k$ hashes to slot $h(k)$. $h(k)$ is the **hash value** of $k$.\n**Collision**: Two keys hash to the same slot. Resolved using techniques like chaining or open addressing.\nIdeally, hash function distributes keys randomly. However, if $|U| > m$, collisions are unavoidable (pigeonhole principle).\n\n**Independent uniform hashing**\nAn ideal hash function $h$ would have $h(k)$ be a randomly and independently chosen value from $\\{0, 1, \\dots, m-1\\}$ for each $k \n\n## 13.2 Rotations\nSearch-tree operations like `TREE-INSERT` and `TREE-DELETE` take $O(\\lg n)$ time on a red-black tree but may violate red-black properties. **Rotation** is a local operation that changes pointer structure while preserving the binary-search-tree property. It's used to restore red-black properties.\nThere are two kinds of rotations: left and right, as shown in Figure 13.2 of the book.\n\n**Left Rotation**\n`LEFT-ROTATE(T, x)` transforms the structure assuming $x.right \\neq T.nil$ and root's parent is $T.nil$. If $y = x.right$, $y$ becomes the new root of the subtree. $x$ becomes $y$'s left child. $y$'s original left child $\\beta$ becomes $x$'s right child.\nPseudocode: [[PartIII Data Structures Algorithms.md#13.2 LEFT-ROTATE (T, x)]]\n\n**Right Rotation**\n`RIGHT-ROTATE(T, y)` is symmetric to `LEFT-ROTATE(T, x)`. If $x = y.left$, $x$ becomes the new root. $y$ becomes $x$'s right child. $x$'s original right child $\\beta$ becomes $y$'s left child.\nPseudocode: [[PartIII Data Structures Algorithms.md#13.2 RIGHT-ROTATE (T, y)]]\nBoth `LEFT-ROTATE` and `RIGHT-ROTATE` run in $O(1)$ time. Only pointers are changed.\n\n## 13.3 Insertion\nTo insert a node $z$ into an $n$-node red-black tree $T$ in $O(\\lg n)$ time while maintaining red-black properties:\n1. Use a modified `TREE-INSERT` to insert $z$ into $T$ as if it were an ordinary binary search tree.\n2. Color $z$ red.\n3. Call an auxiliary procedure `RB-INSERT-FIXUP` to restore red-black properties.\n\nThe procedure [[PartIII Data Structures Algorithms.md#13.3 RB-INSERT (T, z)]] handles this. Differences from `TREE-INSERT`:\n- All `NIL` instances replaced by `T.nil`.\n- $z.left$ and $z.right$ are set to `T.nil` (lines 14-15).\n- $z$ is colored red (line 16).\n- `RB-INSERT-FIXUP(T, z)` is called (line 17).\n\n**RB-INSERT-FIXUP**\nPseudocode: [[PartIII Data Structures Algorithms.md#13.3 RB-INSERT-FIXUP (T, z)]]\nViolations that can occur when a red node $z$ is inserted:\n- Property 2 (root is black): If $z$ is the root.\n- Property 4 (red node has black children): If $z.p$ is red.\n(Properties 1, 3, 5 remain satisfied initially.)\n\nThe `while` loop (lines 1-29) aims to fix violations. Loop invariant at the start of each iteration:\na. Node $z$ is red.\nb. If $z.p$ is the root, then $z.p$ is black.\nc. If the tree violates any red-black properties, it's at most one: either property 2 ( $z$ is root and red) or property 4 ($z$ and $z.p$ are red).\n\n**Initialization**: The invariant holds before the first iteration.\n- (a) $z$ is the newly added red node.\n- (b) If $z.p$ is root, it was black (original root or $T.nil$).\n- (c) As discussed above, only property 2 or 4 can be violated.\n\n**Maintenance**: The loop considers three cases (and their symmetric counterparts). We focus on when $z.p$ is a left child ($z.p = z.p.p.left$). $y$ is $z$'s uncle ($z.p.p.right$). $z.p.p$ exists and is black (since $z.p$ is red, violating property 4 with $z$, so $z.p$ cannot be root; if $z.p$ is red, its parent $z.p.p$ must be black).\n\n- **Case 1: $z$'s uncle $y$ is red** (lines 4-8)\n  - $z.p$ and $y$ are colored black.\n  - $z.p.p$ is colored red (to maintain black-heights - property 5).\n  - $z$ moves up to $z.p.p$.\n  - The invariant is maintained for the new $z$. Pointer $z$ moves two levels up. The only possible violation of property 4 is now between new $z$ (which is red) and its parent, if its parent is also red.\n\n- **Case 2: $z$'s uncle $y$ is black, and $z$ is a right child** (lines 10-12)\n  - $z$ is updated to $z.p$.\n  - A `LEFT-ROTATE` on the new $z$ transforms this into Case 3.\n  - Both $z$ and new $z.p$ (original $z$) are red, violating property 4. Black-heights and property 5 are preserved.\n\n- **Case 3: $z$'s uncle $y$ is black, and $z$ is a left child** (lines 13-15)\n  - $z.p$ is colored black.\n  - $z.p.p$ is colored red.\n  - A `RIGHT-ROTATE` on $z.p.p$.\n  - This resolves the property 4 violation between $z$ and $z.p$. No new violations are introduced. The new $z.p$ is black, so the `while` loop terminates.\n\n**Termination**: The loop terminates because either $z.p$ becomes black (Case 3 handles this directly, or Case 2 transforms to Case 3) or $z$ moves up the tree (Case 1). If $z$ reaches the root, $z.p$ is $T.nil$, which is black, so loop terminates. Line 30 ensures property 2 (root is black) holds.\n\n**Analysis**: `RB-INSERT` takes $O(\\lg n)$ time. `RB-INSERT-FIXUP` loop runs $O(\\lg n)$ times (Case 1 moves $z$ up). Rotations are $O(1)$, at most 2 rotations occur. Total time $O(\\lg n)$.\n\n## 13.4 Deletion\nDeletion also takes $O(\\lg n)$ time. It's based on `TREE-DELETE`.\nCustomized [[PartIII Data Structures Algorithms.md#13.4 RB-TRANSPLANT (T, u, v)]] is used, similar to `TRANSPLANT` but references `T.nil` and $v.p$ is always set.\n\nThe procedure [[PartIII Data Structures Algorithms.md#13.4 RB-DELETE (T, z)]] is structured like `TREE-DELETE`.\n- $y$ is the node either removed from the tree or moved within the tree. If $z$ has $\\le 1$ child, $y=z$. If $z$ has 2 children, $y=TREE-MINIMUM(z.right)$ (z's successor).\n- $x$ is the node that moves into $y$'s original position.\n- `y-original-color` stores $y$'s color before it's changed or $y$ is moved.\n- If $y$ was black, its removal/movement could violate red-black properties. `RB-DELETE-FIXUP(T, x)` is called to restore them.\n  - If $y$ was red, properties still hold: no black-heights change, no red nodes made adjacent, root remains black (if $y$ was red, it wasn't root).\n\n**RB-DELETE-FIXUP**\nPseudocode: [[PartIII Data Structures Algorithms.md#13.4 RB-DELETE-FIXUP (T, x)]]\nCalled when $y$, which was black, is removed or moved. Node $x$ now has an \"extra black\".\nGoal of `while` loop (lines 1-43): Move the extra black up the tree until:\n1. $x$ points to a red-and-black node (becomes singly black by coloring $x$ black - line 44).\n2. $x$ points to the root (extra black can be removed).\n3. Suitable rotations and recolorings resolve the issue.\n$x$ always points to a nonroot doubly black node in the loop. $w$ is $x$'s sibling.\n\nFour cases (and symmetric counterparts) when $x$ is a left child. $w = x.p.right$.\n- **Case 1: $x$'s sibling $w$ is red** (lines 4-8)\n  - $w$ must have black children.\n  - Color $w$ black, $x.p$ red. `LEFT-ROTATE` on $x.p$.\n  - New sibling of $x$ (one of $w$'s original children) is black.\n  - Transforms into Case 2, 3, or 4.\n\n- **Case 2: $x$'s sibling $w$ is black, and both of $w$'s children are black** (lines 9-11)\n  - Remove one black from $x$ (now singly black) and $w$ (becomes red).\n  - Add an extra black to $x.p$ to compensate.\n  - The pointer $x$ moves up to $x.p$. Loop continues.\n  - If entered from Case 1, $x.p$ (new $x$) was red. New $x$ is now red-and-black. Loop terminates, line 44 makes $x$ black.\n\n- **Case 3: $x$'s sibling $w$ is black, $w.left$ is red, and $w.right$ is black** (lines 13-17)\n  - Color $w.left$ black, $w$ red. `RIGHT-ROTATE` on $w$.\n  - New sibling $w$ of $x$ is black with a red right child.\n  - Transforms into Case 4.\n\n- **Case 4: $x$'s sibling $w$ is black, and $w.right$ is red** (lines 18-22)\n  - Recolor $w$ with $x.p$'s color, $x.p$ black, $w.right$ black.\n  - `LEFT-ROTATE` on $x.p$.\n  - Extra black on $x$ is removed. $x$ set to $T.root$. Loop terminates.\n\n**Analysis**: `RB-DELETE` takes $O(\\lg n)$ time without `RB-DELETE-FIXUP`. In `RB-DELETE-FIXUP`:\n- Case 1, 3, 4 terminate after constant color changes and $\\le 3$ rotations.\n- Case 2 is the only one where loop repeats. Pointer $x$ moves up tree.\n- Loop executes $O(\\lg n)$ times. At most 3 rotations total.\nTotal time for `RB-DELETE` is $O(\\lg n)$.\n"
            }
        ]
    },
    {
        "items": [
            {
                "file": "foundations overview.md",
                "contents": "# Foundations Overview\n\nThis overview covers foundational concepts in algorithms, drawing primarily from Chapters 4 (Divide-and-Conquer) and 5 (Probabilistic Analysis and Randomized Algorithms) of CLRS, and summarizing key concepts typically introduced in earlier chapters regarding algorithm analysis and design.\n\n## 1 The Role of Algorithms in Computing (Summary)\n\nA *problem statement* defines a relationship between input and output, possibly with additional constraints. An *instance of a problem* is any input that satisfies these constraints.\nAn *algorithm* is a well-defined computational procedure that takes some value, or set of values, as *input* and produces some value, or set of values, as *output*. An algorithm is thus a sequence of computational steps that transform the input into the output. An algorithm is a tool for solving a well-specified *computational problem*.\nA *data structure* is a way to store and organize data in order to facilitate access and modifications.\n\n## 2 Getting Started (Summary)\n\n### 2.1 Analyzing Algorithms\nAnalyzing an algorithm means predicting the resources that the algorithm requires. Resources such as memory, communication bandwidth, or computational hardware are of primary concern, but most often it is *computational time* that we want to measure.\n*Input size* depends on the problem being studied. For many problems, such as sorting or Fourier transforms, the most natural measure is the number of items in the input\u2014for example, the number $n$ of items being sorted.\n*Running time* of an algorithm on a particular input is the number of primitive operations or \u201csteps\u201d executed. It is convenient to define the notion of step so that it is as machine-independent as possible.\n\n### 2.2 Designing Algorithms\nOne common algorithmic technique is *incremental approach*: having sorted the subarray $A[1 .. j-1]$, we insert the single element $A[j]$ into its proper place, yielding the sorted subarray $A[1 .. j]$.\n\n### 2.3 Loop Invariants and Correctness\nA *loop invariant* is a property used to help show that an algorithm is correct. For a loop invariant, we must show three things:\n- **Initialization**: It is true prior to the first iteration of the loop.\n- **Maintenance**: If it is true before an iteration of the loop, it remains true before the next iteration.\n- **Termination**: When the loop terminates, the invariant\u2014usually along with the reason that the loop terminated\u2014gives us a useful property that helps show that the algorithm is correct.\n\n## 3 Characterizing Running Times (Summary)\n\nWe usually focus on the *worst-case running time*: the longest running time for any input of size $n$. Reasons include: it's an upper bound, the worst case occurs fairly often for some algorithms, and the average case is often as bad as the worst case.\nThe *order of growth* or *rate of growth* of the running time is an abstraction that considers only the leading term of a formula for running time, ignoring lower-order terms and the leading term's constant coefficient. This simplifies comparisons of algorithm efficiency, especially for large inputs.\n\n### 3.1 Asymptotic Notation\nAsymptotic notation provides a way to describe the behavior of functions in the limit, typically used for characterizing algorithm running times.\n- **$\\\\Theta(g(n))$: Asymptotically tight bound**\n  $\\\\Theta(g(n)) = \\\\{f(n) : \\\\exists c_1 > 0, c_2 > 0, n_0 > 0 \\\\text{ s.t. } 0 \\\\le c_1 g(n) \\\\le f(n) \\\\le c_2 g(n) \\\\forall n \\\\ge n_0 \\\\}$.\n- **$O(g(n))$: Asymptotic upper bound**\n  $O(g(n)) = \\\\{f(n) : \\\\exists c > 0, n_0 > 0 \\\\text{ s.t. } 0 \\\\le f(n) \\\\le c g(n) \\\\forall n \\\\ge n_0 \\\\}$.\n- **$\\\\Omega(g(n))$: Asymptotic lower bound**\n  $\\\\Omega(g(n)) = \\\\{f(n) : \\\\exists c > 0, n_0 > 0 \\\\text{ s.t. } 0 \\\\le c g(n) \\\\le f(n) \\\\forall n \\\\ge n_0 \\\\}$.\n- **$o(g(n))$: Asymptotic upper bound, not tight**\n  $o(g(n)) = \\\\{f(n) : \\\\forall c > 0, \\\\exists n_0 > 0 \\\\text{ s.t. } 0 \\\\le f(n) < c g(n) \\\\forall n \\\\ge n_0 \\\\}$. This implies $\\\\lim_{n\\\\to\\\\infty} f(n)/g(n) = 0$.\n- **$\\\\omega(g(n))$: Asymptotic lower bound, not tight**\n  $\\\\omega(g(n)) = \\\\{f(n) : \\\\forall c > 0, \\\\exists n_0 > 0 \\\\text{ s.t. } 0 \\\\le c g(n) < f(n) \\\\forall n \\\\ge n_0 \\\\}$. This implies $\\\\lim_{n\\\\to\\\\infty} f(n)/g(n) = \\\\infty$.\n\n## 4 Divide-and-Conquer Overview\nThe divide-and-conquer method is a powerful strategy for designing asymptotically efficient algorithms. It involves three characteristic steps:\n1.  **Divide** the problem into one or more subproblems that are smaller instances of the same problem.\n2.  **Conquer** the subproblems by solving them recursively. If a subproblem size is small enough (base case), solve it directly.\n3.  **Combine** the subproblem solutions to form a solution to the original problem.\n\nRunning times of divide-and-conquer algorithms are often described by *recurrence relations* (or recurrences), which are equations or inequalities that describe a function in terms of its value on smaller inputs. For example, the recurrence for merge sort is $T(n) = 2T(n/2) + \\\\Theta(n)$, with solution $T(n) = \\\\Theta(n \\\\lg n)$.\n\nSeveral methods are used to solve recurrences:\n- **Substitution method**: Guess the form of the solution and use mathematical induction to find constants and show that the solution works.\n- **Recursion-tree method**: Convert the recurrence into a tree whose nodes represent costs at various levels of recursion. Sum costs to obtain a guess for the solution, then verify with substitution.\n- **Master method**: Provides a 'cookbook' for solving recurrences of the form $T(n) = aT(n/b) + f(n)$, where $a \\\\ge 1, b > 1$. It compares $f(n)$ with $n^{\\\\log_b a}$ and has three cases.\n- **Akra-Bazzi method**: Generalizes the master method for recurrences of the form $T(n) = f(n) + \\\\sum_{i=1}^k a_i T(n/b_i)$. It requires $f(n)$ to satisfy a polynomial-growth condition and involves solving for a unique $p$ such that $\\\\sum a_i / b_i^p = 1$.\n\nExamples of divide-and-conquer include matrix multiplication. A simple recursive approach leads to $T(n) = 8T(n/2) + \\\\Theta(1)$, yielding $T(n) = \\\\Theta(n^3)$. Strassen's algorithm improves this to $T(n) = 7T(n/2) + \\\\Theta(n^2)$, solving to $T(n) = \\\\Theta(n^{\\\\lg 7}) \\\\approx \\\\Theta(n^{2.81})$.\n\n## 5 Probabilistic Analysis and Randomized Algorithms Overview\n\n*Probabilistic analysis* is the use of probability in analyzing problems or algorithms. It typically involves computing an average-case running time by taking the expectation over a distribution of possible inputs. This requires knowledge or assumptions about the input distribution.\n\n*Randomized algorithms* make random choices during their execution. Their behavior is determined not only by input but also by values from a random-number generator. The running time of a randomized algorithm is an *expected running time*, where the expectation is over the random choices made by the algorithm, for any specific input.\n\n*Indicator random variables* are a useful tool. For an event $A$, the indicator random variable $I\\\\{A\\\\}$ is 1 if $A$ occurs and 0 otherwise. A key property is $E[I\\\\{A\\\\}] = Pr\\\\{A\\\\}$. Linearity of expectation states that the expectation of a sum of random variables is the sum of their expectations, even if they are dependent. This simplifies many analyses.\n\n**The Hiring Problem**: An example where a candidate is hired if better than all previous ones. If candidates arrive in random order (probabilistic analysis), the expected number of hires is $H_n = \\\\sum_{i=1}^n (1/i) = \\\\ln n + O(1)$. A randomized version permutes candidates first, achieving the same expected number of hires for any input.\n\n**Randomly Permuting Arrays**: Generating a uniform random permutation (each of $n!$ permutations equally likely) is crucial for many randomized algorithms. The `RANDOMLY-PERMUTE` algorithm achieves this in $O(n)$ time by iteratively swapping $A[i]$ with a random element from $A[i..n]$.\n\nFurther examples illustrating these concepts include:\n- **Birthday Paradox**: With 23 people, probability of a shared birthday is >0.5. Expected number of shared-birthday pairs is 1 with about $\\\\sqrt{2n}$ people (for $n$ days in year).\n- **Balls and Bins**: Tossing $n$ balls into $b$ bins. The coupon collector's problem (expected tosses to hit all $b$ bins) is $bH_b \\\\approx b \\\\ln b$.\n- **Streaks**: Longest expected streak of heads in $n$ coin flips is $\\\\Theta(\\\\lg n)$.\n- **Online Hiring Problem**: Hire exactly once by interviewing/rejecting first $k$ candidates, then hiring next best. Optimal $k \\\\approx n/e$ yields success probability $\\\\approx 1/e$."
            },
            {
                "file": "PartI Foundations Algorithms.md",
                "contents": "# Part I Foundations Algorithms\n\n## C4 Divide-and-Conquer Algorithms\n\n### 4.1 MATRIX-MULTIPLY\n```\nMATRIX-MULTIPLY(A, B, C, n)\n  // Assumes A, B, C are n x n matrices. Adds A*B to C.\n  // To compute C = A*B, initialize C to all zeros first.\n  for i = 1 to n\n    for j = 1 to n\n      for k = 1 to n\n        C[i,j] = C[i,j] + A[i,k] * B[k,j]\n```\n\n### 4.1 MATRIX-MULTIPLY-RECURSIVE\n```\nMATRIX-MULTIPLY-RECURSIVE(A, B, C, n)\n  // Assumes A, B, C are n x n matrices. Adds A*B to C.\n  // n is an exact power of 2.\n  if n == 1\n    // Base case\n    C[1,1] = C[1,1] + A[1,1] * B[1,1]\n    return\n  \n  // Divide A, B, C into n/2 x n/2 submatrices\n  // A11, A12, A21, A22; B11, B12, B21, B22; C11, C12, C21, C22\n  // This partitioning is done by index calculation, taking Theta(1) time.\n  \n  // Conquer (Recursive calls)\n  MATRIX-MULTIPLY-RECURSIVE(A11, B11, C11, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A11, B12, C12, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A21, B11, C21, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A21, B12, C22, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A12, B21, C11, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A12, B22, C12, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A22, B21, C21, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A22, B22, C22, n/2)\n```\n\n### 4.2 Strassen's algorithm structure\nStrassen's algorithm computes $C = A \ncdot B$ (or $C = C + A \ncdot B$) for $n \ntimes n$ matrices.\n1.  **Divide**: Partition $A, B, C$ into $n/2 \ntimes n/2$ submatrices (if $n > 1$). Takes $\\\\Theta(1)$ time with index calculation.\n2.  **Create intermediate matrices**: Create 10 matrices $S_1, S_2, \nldots, S_{10}$, each $n/2 \ntimes n/2$. Each $S_i$ is the sum or difference of two submatrices from step 1. This takes $\\\\Theta(n^2)$ time.\n    $S_1 = B_{12} - B_{22}$\n    $S_2 = A_{11} + A_{12}$\n    $S_3 = A_{21} + A_{22}$\n    $S_4 = B_{21} - B_{11}$\n    $S_5 = A_{11} + A_{22}$\n    $S_6 = B_{11} + B_{22}$\n    $S_7 = A_{12} - A_{22}$\n    $S_8 = B_{21} + B_{22}$\n    $S_9 = A_{11} - A_{21}$\n    $S_{10} = B_{11} + B_{12}$\n3.  **Conquer**: Recursively compute 7 matrix products $P_1, P_2, \nldots, P_7$, each $n/2 \ntimes n/2$. This step takes $7T(n/2)$ time.\n    $P_1 = A_{11} \ncdot S_1$\n    $P_2 = S_2 \ncdot B_{22}$\n    $P_3 = S_3 \ncdot B_{11}$\n    $P_4 = A_{22} \ncdot S_4$\n    $P_5 = S_5 \ncdot S_6$\n    $P_6 = S_7 \ncdot S_8$\n    $P_7 = S_9 \ncdot S_{10}$\n4.  **Combine**: Compute the submatrices of $C$ by adding and subtracting various $P_i$ matrices. This takes $\\\\Theta(n^2)$ time.\n    $C_{11} = P_5 + P_4 - P_2 + P_6$\n    $C_{12} = P_1 + P_2$\n    $C_{21} = P_3 + P_4$\n    $C_{22} = P_5 + P_1 - P_3 - P_7$\n(If computing $C=C+A \ncdot B$, then these values are added to the existing $C_{ij}$ values.)\n\n## C5 Probabilistic Analysis and Randomized Algorithms Algorithms\n\n### 5.1 HIRE-ASSISTANT\n```\nHIRE-ASSISTANT(n)\n  best = 0 // candidate 0 is a least-qualified dummy candidate\n  for i = 1 to n\n    interview candidate i\n    if candidate i is better than candidate best\n      best = i\n      hire candidate i\n```\n\n### 5.3 RANDOMIZED-HIRE-ASSISTANT\n```\nRANDOMIZED-HIRE-ASSISTANT(n)\n  randomly permute the list of candidates\n  HIRE-ASSISTANT(n)\n```\n\n### 5.3 RANDOMLY-PERMUTE\n```\nRANDOMLY-PERMUTE(A, n)\n  for i = 1 to n\n    swap A[i] with A[RANDOM(i, n)]\n```\n\n### 5.3 PERMUTE-WITHOUT-IDENTITY (Exercise 5.3-2)\n```\nPERMUTE-WITHOUT-IDENTITY(A, n)\n  for i = 1 to n - 1\n    swap A[i] with A[RANDOM(i + 1, n)]\n```\n\n### 5.3 PERMUTE-WITH-ALL (Exercise 5.3-3)\n```\nPERMUTE-WITH-ALL(A, n)\n  for i = 1 to n\n    swap A[i] with A[RANDOM(1, n)]\n```\n\n### 5.3 PERMUTE-BY-CYCLE (Exercise 5.3-4)\n```\nPERMUTE-BY-CYCLE(A, n)\n  let B[1:n] be a new array\n  offset = RANDOM(1, n)\n  for i = 1 to n\n    dest = i + offset\n    if dest > n\n      dest = dest - n\n    B[dest] = A[i]\n  return B\n```\n\n### 5.3 RANDOM-SAMPLE (Exercise 5.3-5)\n```\nRANDOM-SAMPLE(m, n)\n  S = \u00d8\n  for k = n - m + 1 to n // iterates m times\n    i = RANDOM(1, k)\n    if i \u2208 S\n      S = S \u222a {k}\n    else S = S \u222a {i}\n  return S\n```\n\n### 5.4 ONLINE-MAXIMUM\n```\nONLINE-MAXIMUM(k, n)\n  best-score = -\u221e\n  for i = 1 to k\n    if score(i) > best-score\n      best-score = score(i)\n  for i = k + 1 to n\n    if score(i) > best-score\n      return i\n  return n\n```"
            },
            {
                "file": "PartI Foundations/C4 Divide-and-Conquer.md",
                "contents": "# 4 Divide-and-Conquer\n\nThe divide-and-conquer method is a powerful strategy for designing asymptotically efficient algorithms. It involves breaking a problem into smaller subproblems, solving these recursively, and then combining their solutions. We explore applications of this method and acquire mathematical tools for analyzing the recurrences that arise.\n\nRecall that for divide-and-conquer, you solve a given problem (instance) recursively. If the problem is small enough\u2014the *base case*\u2014you just solve it directly without recursing. Otherwise\u2014the *recursive case*\u2014you perform three characteristic steps:\n- **Divide** the problem into one or more subproblems that are smaller instances of the same problem.\n- **Conquer** the subproblems by solving them recursively.\n- **Combine** the subproblem solutions to form a solution to the original problem.\nA divide-and-conquer algorithm breaks down a large problem into smaller subproblems, which themselves may be broken down. The recursion bottoms out when it reaches a base case.\n\n## 4.1 Recurrences\nTo analyze recursive divide-and-conquer algorithms, we use recurrences.\nA *recurrence* is an equation or inequality that describes a function in terms of its value on other, typically smaller, arguments. Recurrences characterize the running times of recursive algorithms mathematically.\n\nAn *algorithmic recurrence* $T(n)$ describes the running time of a divide-and-conquer algorithm. For every sufficiently large threshold constant $n_0 > 0$, it typically satisfies two properties:\n1.  For all $n < n_0$, $T(n) = \\\\Theta(1)$.\n2.  For all $n \\\\ge n_0$, every path of recursion terminates in a defined base case within a finite number of recursive invocations.\n\n**Conventions for recurrences**: Whenever a recurrence is stated without an explicit base case, we assume that the recurrence is algorithmic, meaning $T(n) = \\\\Theta(1)$ for sufficiently small $n$. Asymptotic solutions often do not change when floors or ceilings are dropped, simplifying analysis.\n\nIf a recurrence is an inequality, such as $T(n) \\\\le 2T(n/2) + \\\\Theta(n)$, its solution is expressed using $O$-notation. If $T(n) \\\\ge 2T(n/2) + \\\\Theta(n)$, its solution uses $\\\\Omega$-notation.\n\n**Examples of recurrences**: \n- A simple matrix multiplication algorithm breaking an $n \\times n$ problem into four $n/2 \\times n/2$ subproblems might have a recurrence $T(n) = 4T(n/2) + \\\\Theta(n^2)$ if combining costs $\\\\Theta(n^2)$, but the text uses $T(n) = 8T(n/2) + \\\\Theta(1)$ for the basic recursive version if partitioning and combining basic scalar operations are $\\\\Theta(1)$ relative to subproblem calls. This has solution $T(n) = \\\\Theta(n^3)$.\n- Strassen's algorithm: $T(n) = 7T(n/2) + \\\\Theta(n^2)$, solution $T(n) = \\\\Theta(n^{\\\\lg 7})$.\n- Varying subproblem sizes: $T(n) = T(n/3) + T(2n/3) + \\\\Theta(n)$, solution $T(n) = \\\\Theta(n \\\\lg n)$.\n- Linear search (recursive): $T(n) = T(n-1) + \\\\Theta(1)$, solution $T(n) = \\\\Theta(n)$.\nMost efficient divide-and-conquer algorithms solve subproblems that are a constant fraction of the original problem size.\n\nThis chapter introduces four methods for solving recurrences: substitution, recursion-tree, master method, and Akra-Bazzi method.\n\n## 4.1 Multiplying square matrices\nLet $A = (a_{ik})$ and $B = (b_{jk})$ be square $n \\times n$ matrices. Their product $C = A \ncdot B$ is also $n \\times n$, where $c_{ij} = \\\\sum_{k=1}^{n} a_{ik} \ncdot b_{kj}$.\n\n**Straightforward algorithm**: [[PartI Foundations Algorithms.md#4.1 MATRIX-MULTIPLY]] uses three nested loops and runs in $\\\\Theta(n^3)$ time. It computes $C = C + A \ncdot B$. To compute $C = A \ncdot B$, $C$ must first be initialized to all zeros, taking $\\\\Theta(n^2)$ time, which is dominated by the $\\\\Theta(n^3)$ multiplication.\n\n**A simple divide-and-conquer algorithm**:\nPartition $A, B, C$ into four $n/2 \ntimes n/2$ submatrices each (assuming $n$ is a power of 2):\n$A = \\\\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\\\end{pmatrix}$, $B = \\\\begin{pmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\\\end{pmatrix}$, $C = \\\\begin{pmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\\\end{pmatrix}$.\nThen compute:\n$C_{11} = A_{11}B_{11} + A_{12}B_{21}$\n$C_{12} = A_{11}B_{12} + A_{12}B_{22}$\n$C_{21} = A_{21}B_{11} + A_{22}B_{21}$\n$C_{22} = A_{21}B_{12} + A_{22}B_{22}$\nThis involves 8 recursive multiplications of $n/2 \ntimes n/2$ matrices and 4 matrix additions (which take $\\\\Theta(n^2)$ time). If partitioning is done by copying, it takes $\\\\Theta(n^2)$ time. If done by index calculation, it takes $\\\\Theta(1)$ time per subproblem creation.\nThe procedure [[PartI Foundations Algorithms.md#4.1 MATRIX-MULTIPLY-RECURSIVE]] implements this strategy computing $C = C+A \ncdot B$. It makes 8 recursive calls. If partitioning (line 6) takes $\\\\Theta(1)$ time (using index calculation), the recurrence for its running time is $T(n) = 8T(n/2) + \\\\Theta(1)$. This recurrence has the solution $T(n) = \\\\Theta(n^3)$, same as the straightforward method. If partitioning involves copying elements taking $\\\\Theta(n^2)$ time, the recurrence would be $T(n) = 8T(n/2) + \\\\Theta(n^2)$, which also solves to $T(n) = \\\\Theta(n^3)$.\nThe recursion tree for $T(n) = 8T(n/2) + \\\\Theta(1)$ is much bushier than for merge sort's $T(n) = 2T(n/2) + \\\\Theta(n)$, leading to a faster-growing solution.\n\n## 4.2 Strassen's algorithm for matrix multiplication\nStrassen's algorithm (1969) was the first to beat the $\\\\Theta(n^3)$ time bound for matrix multiplication. It runs in $O(n^{\\\\lg 7}) \\\\approx O(n^{2.81})$ time.\nIt uses a divide-and-conquer approach similar to [[PartI Foundations Algorithms.md#4.1 MATRIX-MULTIPLY-RECURSIVE]] but reduces the number of recursive multiplications of $n/2 \ntimes n/2$ matrices from 8 to 7, at the cost of more matrix additions and subtractions (still a constant number of $\\\\Theta(n^2)$ operations).\n\nThe steps are outlined in [[PartI Foundations Algorithms.md#4.2 Strassen's algorithm structure]].\n1.  Base case ($n=1$) or partition input matrices ($A, B$) and output matrix ($C$) into $n/2 \ntimes n/2$ submatrices. This step takes $\\\\Theta(1)$ time using index calculation.\n2.  Create 10 matrices $S_1, \nldots, S_{10}$, each of size $n/2 \ntimes n/2$. Each $S_i$ is a sum or difference of two submatrices from step 1. This involves 10 matrix additions/subtractions, taking $\\\\Theta(n^2)$ time.\n3.  Recursively compute 7 matrix products $P_1, \nldots, P_7$. Each $P_k$ is a product of an $A$-submatrix (or an $S_i$ derived from $A$-submatrices) and a $B$-submatrix (or an $S_j$ derived from $B$-submatrices). This step makes $7$ recursive calls, taking $7T(n/2)$ time.\n4.  Compute the desired submatrices $C_{11}, C_{12}, C_{21}, C_{22}$ by adding or subtracting various $P_k$ matrices. This involves 8 matrix additions/subtractions (e.g., $C_{11} = P_5 + P_4 - P_2 + P_6$), taking $\\\\Theta(n^2)$ time.\n\nThe running time is described by the recurrence $T(n) = 7T(n/2) + \\\\Theta(n^2)$. Using the master method (Case 1, $n^{\\\\log_2 7}$ vs $n^2$, since $\\\\lg 7 \\\\approx 2.807 > 2$), the solution is $T(n) = \\\\Theta(n^{\\\\lg 7})$.\n\n## 4.3 The substitution method for solving recurrences\nThe substitution method for solving recurrences comprises two steps:\n1.  Guess the form of the solution (e.g., $T(n) = O(g(n))$).\n2.  Use mathematical induction to find the constants and show that the solution works.\nThis method is powerful but requires a good guess. It's often used to establish an upper ($O$) or lower ($\\\\Omega$) bound.\n\n**Example**: Solve $T(n) = 2T(\\\\lfloor n/2 \\\\rfloor) + \\\\Theta(n)$. Guess $T(n) = O(n \\\\lg n)$.\nInductive hypothesis: Assume $T(k) \\\\le ck \\\\lg k$ for $k < n$, specifically for $k = \\\\lfloor n/2 \\\\rfloor$. The $\\\\Theta(n)$ term can be replaced by $dn$ for some constant $d>0$ for an upper bound proof.\n$T(n) \\\\le 2(c \\\\lfloor n/2 \\\\rfloor \\\\lg(\\\\lfloor n/2 \\\\rfloor)) + dn$\n$\\\\le 2(c (n/2) \\\\lg(n/2)) + dn$\n$= cn \\\\lg(n/2) + dn$\n$= cn (\\\\lg n - \\\\lg 2) + dn$\n$= cn \\\\lg n - cn + dn$\nThis is $\\\\le cn \\\\lg n$ if $-cn + dn \\\\le 0$, i.e., $d \\\\le c$. We can pick $c$ large enough for this to hold.\nBase cases: We must also show the hypothesis holds for base cases. For $T(n) = \\\\Theta(1)$ for small $n$, we can choose $c$ large enough such that $cn \\\\lg n$ is greater than $T(n)$ for these base cases (e.g., for $n=2,3$ if $n_0=2$).\n\n**Making a good guess**: Experience, solving similar recurrences, or using recursion trees helps. Loose bounds can be determined and then tightened.\n\n**A trick: Subtracting a lower-order term**: If an inductive proof fails because the inductive hypothesis is not strong enough, try subtracting a lower-order term. For $T(n) = 2T(n/2) + \\\\Theta(1)$, guess $T(n) = O(n)$. Assume $T(k) \\\\le ck$. Then $T(n) \\\\le 2(c(n/2)) + d_0 = cn + d_0$, which is not $\\\\le cn$. Instead, guess $T(n) \\\\le cn - d$ for some $d > 0$.\n$T(n) \\\\le 2(c(n/2) - d) + d_0$\n$= cn - 2d + d_0$\nThis is $\\\\le cn - d$ if $-2d + d_0 \\\\le -d$, which means $d \\\\ge d_0$. This can be satisfied by choosing $d$ large enough.\n\n**Avoiding pitfalls**: Do not use asymptotic notation in the inductive hypothesis (e.g., $T(n) \\\\le O(n)$). Prove the exact form of the guessed solution (e.g., $T(n) \\\\le cn$). Constants matter in inductive proofs.\n\n## 4.4 The recursion-tree method for solving recurrences\nThe recursion-tree method converts a recurrence into a tree where each node represents the cost of a single subproblem. Summing costs per level and then across levels gives a guess for the solution. It's good for generating guesses or can be a direct proof if done carefully.\n\n**Example**: $T(n) = 3T(n/4) + cn^2$.\n- Root: cost $cn^2$. Children: 3 nodes, each for $T(n/4)$, cost $c(n/4)^2$.\n- Level 0: $cn^2$\n- Level 1: $3 \ncdot c(n/4)^2 = (3/16)cn^2$\n- Level 2: $3^2 \ncdot c((n/4)^2)^2 = (3/16)^2 cn^2$\n- Level $i$: $3^i \ncdot c(n/4^i)^2 = (3/16)^i cn^2$\n- Tree height: Subproblem size $n/4^i = 1 \\\\implies i = \\\\log_4 n$. Number of levels is $\\\\log_4 n + 1$.\n- Leaves: At depth $\\\\log_4 n$. Number of leaves $3^{\\\\log_4 n} = n^{\\\\log_4 3}$. Each leaf cost is $T(1) = \\\\Theta(1)$. Total leaf cost $\\\\Theta(n^{\\\\log_4 3})$.\n- Total cost: $T(n) = cn^2 \\\\sum_{i=0}^{\\\\log_4 n -1} (3/16)^i + \\\\Theta(n^{\\\\log_4 3})$.\nThe sum is a geometric series $\\\\sum_{i=0}^{k} x^i \\\\le 1/(1-x)$ for $x < 1$. Here $x=3/16$.\n$T(n) \\\\le cn^2 (1/(1-3/16)) + \\\\Theta(n^{\\\\log_4 3}) = cn^2 (16/13) + \\\\Theta(n^{\\\\log_4 3})$.\nSince $\\\\log_4 3 < 2$, the $cn^2$ term dominates. So $T(n) = O(n^2)$. Since $f(n)=cn^2$ is part of $T(n)$, $T(n) = \\\\Omega(n^2)$. Thus $T(n) = \\\\Theta(n^2)$.\nThis can be verified by substitution method: $T(n) \\\\le dn^2$. $3d(n/4)^2 + cn^2 = (3/16)dn^2 + cn^2 \\\\le dn^2$ if $cn^2 \\\\le (1 - 3/16)dn^2 = (13/16)dn^2$, so $c \\\\le (13/16)d$, or $d \\\\ge (16/13)c$.\n\n**Irregular example**: $T(n) = T(n/3) + T(2n/3) + cn$.\n- Root: $cn$. Children: $T(n/3)$ and $T(2n/3)$ with costs $c(n/3)$ and $c(2n/3)$.\n- Cost at each level $i$: Sum of costs of nodes is $cn$.\n- Longest path: $n \\\\to (2/3)n \\\\to (2/3)^2 n \\\\to \nldots \\\\to 1$. Height is $\\\\log_{3/2} n$.\n- Total cost: $cn \ncdot (\\\\log_{3/2} n + 1) = O(n \\\\lg n)$.\n- Number of leaves $L(n)$: Recurrence $L(n) = L(n/3) + L(2n/3)$ with $L(1)=1$. Solution $L(n)=\\\\Theta(n)$. Total leaf cost $\\\\Theta(n)$.\nThus $T(n) = \\\\Theta(n \\\\lg n)$.\n\n## 4.5 The master method for solving recurrences\nThe master method provides a way to solve recurrences of the form $T(n) = aT(n/b) + f(n)$, where $a \\\\ge 1, b > 1$ are constants, and $f(n)$ is an asymptotically positive function.\nCompare $f(n)$ with the *watershed function* $n^{\\\\log_b a}$.\n\n**Theorem 4.1 (Master Theorem)**\n1.  If $f(n) = O(n^{\\\\log_b a - \\\\epsilon})$ for some constant $\\\\epsilon > 0$, then $T(n) = \\\\Theta(n^{\\\\log_b a})$. (The cost of leaves dominates.)\n2.  If $f(n) = \\\\Theta(n^{\\\\log_b a} \\\\lg^k n)$ for some constant $k \\\\ge 0$, then $T(n) = \\\\Theta(n^{\\\\log_b a} \\\\lg^{k+1} n)$. (Cost is spread evenly or nearly evenly. If $k=0$, $T(n) = \\\\Theta(n^{\\\\log_b a} \\\\lg n)$.)\n3.  If $f(n) = \\\\Omega(n^{\\\\log_b a + \\\\epsilon})$ for some constant $\\\\epsilon > 0$, and if $af(n/b) \\\\le cf(n)$ for some constant $c < 1$ and all sufficiently large $n$ (the *regularity condition*), then $T(n) = \\\\Theta(f(n))$. (The cost at the root dominates.)\n\n**Examples**:\n- $T(n) = 9T(n/3) + n$. Here $a=9, b=3$. $n^{\\\\log_b a} = n^{\\\\log_3 9} = n^2$. $f(n)=n$. Case 1 applies with $\\\\epsilon=1$ since $n = O(n^{2-1})$. Solution $T(n) = \\\\Theta(n^2)$.\n- $T(n) = T(2n/3) + 1$. Here $a=1, b=3/2$. $n^{\\\\log_b a} = n^{\\\\log_{3/2} 1} = n^0 = 1$. $f(n)=1$. Case 2 applies with $k=0$ since $1 = \\\\Theta(1 \ncdot \\\\lg^0 n)$. Solution $T(n) = \\\\Theta(\\\\lg n)$.\n- $T(n) = 3T(n/4) + n \\\\lg n$. Here $a=3, b=4$. $n^{\\\\log_b a} = n^{\\\\log_4 3} \\\\approx n^{0.793}$. $f(n) = n \\\\lg n$. Case 3 applies since $n \\\\lg n = \\\\Omega(n^{\\\\log_4 3 + \\\\epsilon})$ (e.g., $\\\\epsilon \\\\approx 0.2$). Regularity: $3(n/4)\\\\lg(n/4) \\\\le (3/4)n \\\\lg n$. Choose $c=3/4 < 1$. Solution $T(n) = \\\\Theta(n \\\\lg n)$.\n- $T(n) = 2T(n/2) + n \\\\lg n$. Here $a=2, b=2$. $n^{\\\\log_b a} = n^{\\\\log_2 2} = n$. $f(n) = n \\\\lg n$. Case 2 applies with $k=1$. Solution $T(n) = \\\\Theta(n \\\\lg^2 n)$.\n\nThe master method does not cover all cases. There are gaps between the cases, e.g., if $f(n)$ is asymptotically larger than $n^{\\\\log_b a}$ but not polynomially larger. For $T(n) = 2T(n/2) + n/\\\\lg n$, the master method does not apply.\n\n## 4.6 Proof of the continuous master theorem (*)\nThis section proves a version of the master theorem for continuous variables, avoiding floors and ceilings, to illustrate the main ideas. It uses two lemmas.\n\n**Lemma 4.2**: For $T(n) = aT(n/b) + f(n)$ (with $T(n) = \\\\Theta(1)$ for $0 \\\\le n < 1$), the solution is $T(n) = \\\\Theta(n^{\\\\log_b a}) + \\\\sum_{j=0}^{\\\\lfloor \\\\log_b n \\\\rfloor} a^j f(n/b^j)$. The first term is leaf costs, the sum is internal node costs from recursion tree.\n\n**Lemma 4.3**: This lemma provides bounds for the summation $g(n) = \\\\sum_{j=0}^{\\\\lfloor \\\\log_b n \\\\rfloor} a^j f(n/b^j)$, corresponding to the three cases of the Master Theorem:\n1.  If $f(n) = O(n^{\\\\log_b a - \\\\epsilon})$, then $g(n) = O(n^{\\\\log_b a})$.\n2.  If $f(n) = \\\\Theta(n^{\\\\log_b a} \\\\lg^k n)$, then $g(n) = \\\\Theta(n^{\\\\log_b a} \\\\lg^{k+1} n)$.\n3.  If $af(n/b) \\\\le cf(n)$ for $c<1$ (and $f(n) = \\\\Omega(n^{\\\\log_b a + \\\\epsilon})$ implied by Theorem 4.4 context, though Lemma 4.3 here only needs the regularity for its Case 3 part: $g(n) = \\\\Theta(f(n))$), then $g(n) = \\\\Theta(f(n))$.\n\n**Theorem 4.4 (Continuous Master Theorem)**: Formally states the three cases for $T(n) = aT(n/b) + f(n)$ defined on positive reals with a base case threshold $n_0$. The proof uses auxiliary functions $T'(n) = T(n_0 n)$ and $f'(n) = f(n_0 n)$ to shift the base case to $n < 1$, then applies Lemma 4.2 and Lemma 4.3.\n\n## 4.7 Akra-Bazzi recurrences (*)\nThis method handles recurrences of the form $T(n) = f(n) + \\\\sum_{i=1}^k a_i T(n/b_i)$, where $a_i > 0, b_i > 1$ are constants, and $f(n)$ is a non-negative driving function.\n\n**Polynomial-growth condition**: A function $f(n)$ satisfies this if for any $\\\\phi \\\\ge 1$, there exists $d > 1$ such that for all $1 \\\\le \\\\psi \\\\le \\\\phi$ and $n \\\\ge \\\\bar{n}$, $f(n)/d \\\\le f(\\\\psi n) \\\\le df(n)$. This roughly means $f(O(n)) = \\\\Theta(f(n))$ and implies $f(n)$ is asymptotically positive. Most polynomial and polylogarithmic functions satisfy this.\n\n**Theorem 4.5**: If $f(n)$ satisfies the polynomial-growth condition, then floors and ceilings in the arguments $n/b_i$ (e.g., $T(\\\\lfloor n/b_i \\\\rfloor)$ or $T(\\\\lceil n/b_i \\\\rceil)$) do not affect the asymptotic solution of the Akra-Bazzi recurrence.\n\n**The Akra-Bazzi method**:\n1.  Find the unique real number $p$ such that $\\\\sum_{i=1}^k a_i / b_i^p = 1$.\n2.  The solution to the recurrence is $T(n) = \\\\Theta \\\\left( n^p \\\\left( 1 + \\\\int_{x_0}^n \\\\frac{f(x)}{x^{p+1}} dx \\\\right) \\\\right)$ for some sufficiently small $x_0$.\n\n**Example**: $T(n) = T(n/5) + T(7n/10) + n$. Here $a_1=1, b_1=5, a_2=1, b_2=10/7, f(n)=n$. Find $p$ such that $(1/5)^p + (1/(10/7))^p = (1/5)^p + (7/10)^p = 1$. We find $0 < p < 1$. For example, if $p=0.83978...$, this holds. The integral is $\\\\int_1^n x/(x^{p+1}) dx = \\\\int_1^n x^{-p} dx = [x^{-p+1}/(-p+1)]_1^n = \\\\Theta(n^{1-p})$ since $1-p > 0$.\nThen $T(n) = \\\\Theta(n^p (1 + n^{1-p})) = \\\\Theta(n^p + n) = \\\\Theta(n)$ (since $p<1$)."
            },
            {
                "file": "PartI Foundations/C5 Probabilistic Analysis and Randomized Algorithms.md",
                "contents": "# 5 Probabilistic Analysis and Randomized Algorithms\n\nThis chapter introduces probabilistic analysis and randomized algorithms. Probabilistic analysis uses probability to analyze algorithms, often assuming a distribution for inputs. Randomized algorithms use random numbers to make choices during execution.\n\n## 5.1 The hiring problem\nScenario: Hire a new office assistant. An employment agency sends one candidate per day. Interview, then decide to hire or not. Hiring is costly (fire current, pay agency fee). Goal: always have the best person seen so far.\nStrategy: Interview candidate $i$. If $i$ is better than current assistant, fire current and hire $i$.\nCost model: Interview cost $c_i$, hiring cost $c_h$. If $m$ people are hired out of $n$ interviewed, total cost is $O(c_i n + c_h m)$. We focus on the hiring cost $c_h m$.\nProcedure: [[PartI Foundations Algorithms.md#5.1 HIRE-ASSISTANT]].\n\n**Worst-case analysis**: If candidates arrive in increasing order of quality, we hire all $n$ candidates. Total hiring cost is $O(c_h n)$.\n\n**Probabilistic analysis**: Assume candidates arrive in a random order. This means each of the $n!$ permutations of candidate ranks (1 to $n$) is equally likely. We analyze the average-case hiring cost.\n\n**Randomized algorithms**: If we cannot assume random input order, we can enforce it by randomizing the algorithm. For the hiring problem, we can randomly permute the list of candidates before applying the deterministic hiring strategy. The expected cost is then taken over the algorithm's random choices, not the input distribution.\n\n## 5.2 Indicator random variables\nIndicator random variables provide a convenient way to convert between probabilities and expectations.\nFor a sample space $S$ and an event $A$, the *indicator random variable* $I\\\\{A\\\\}$ is defined as:\n$I\\\\{A\\\\} = \\\\begin{cases} 1 & \\\\text{if } A \\\\text{ occurs} \\\\ 0 & \\\\text{if } A \\\\text{ does not occur} \\\\end{cases}$\n\n**Lemma 5.1**: For an event $A$, $E[I\\\\{A\\\\}] = Pr\\\\{A\\\\}$.\nProof: $E[I\\\\{A\\\\}] = 1 \ncdot Pr\\\\{A\\\\} + 0 \ncdot Pr\\\\{\\\\bar{A}\\\\} = Pr\\\\{A\\\\}$.\n\nIndicator RVs are useful for analyzing repeated trials. *Linearity of expectation* states that $E[\\\\sum X_i] = \\\\sum E[X_i]$, even if $X_i$ are dependent. This is a powerful tool.\n\n**Analysis of the hiring problem using indicator random variables**:\nAssume candidates arrive in a random order.\nLet $X$ be the random variable for the number of times a new assistant is hired.\nLet $X_i = I\\\\{\\\\text{candidate } i \\\\text{ is hired}\\\\}$.\nThen $X = X_1 + X_2 + \nldots + X_n$.\n$E[X] = E[\\\\sum_{i=1}^n X_i] = \\\\sum_{i=1}^n E[X_i]$.\nCandidate $i$ is hired if candidate $i$ is better than candidates $1, \nldots, i-1$. Since the first $i$ candidates arrive in a random order, any of these $i$ candidates is equally likely to be the best so far.\nThus, $Pr\\\\{\\\\text{candidate } i \\\\text{ is hired}\\\\} = 1/i$.\nSo, $E[X_i] = 1/i$.\n$E[X] = \\\\sum_{i=1}^n (1/i) = H_n$, the $n$-th harmonic number.\n$H_n = \\\\ln n + O(1)$.\n\n**Lemma 5.2**: Assuming candidates are presented in a random order, algorithm `HIRE-ASSISTANT` has an average-case total hiring cost of $O(c_h \\\\ln n)$.\nThis is a significant improvement over the worst-case $O(c_h n)$.\n\n## 5.3 Randomized algorithms\nInstead of assuming a random input distribution (probabilistic analysis), a randomized algorithm imposes a distribution by making random choices.\nFor the hiring problem, [[PartI Foundations Algorithms.md#5.3 RANDOMIZED-HIRE-ASSISTANT]] first randomly permutes the list of candidates, then applies the deterministic hiring strategy. Its analysis is identical to the probabilistic analysis of the deterministic algorithm on random inputs.\n\n**Lemma 5.3**: The expected hiring cost of `RANDOMIZED-HIRE-ASSISTANT` is $O(c_h \\\\ln n)$.\nThis holds for *any* input, as the randomization is part of the algorithm.\n\n**Randomly permuting arrays**:\nA common step in randomized algorithms is to produce a *uniform random permutation* of an input array, where each of the $n!$ permutations is equally likely.\nProcedure [[PartI Foundations Algorithms.md#5.3 RANDOMLY-PERMUTE]] permutes $A[1..n]$ in place in $\\\\Theta(n)$ time. In iteration $i$, it swaps $A[i]$ with a random element from $A[i..n]$.\n\n**Lemma 5.4**: `RANDOMLY-PERMUTE` computes a uniform random permutation.\nProof uses a loop invariant:\n*Loop Invariant*: Prior to the $i$-th iteration, for each possible $(i-1)$-permutation, the subarray $A[1..i-1]$ contains this $(i-1)$-permutation with probability $(n-i+1)!/n!$.\n- **Initialization** ($i=1$): $A[1..0]$ is empty. There's one 0-permutation (empty). Probability $(n!/n!) = 1$. Holds.\n- **Maintenance**: Assume invariant holds for $i$. For iteration $i$, let $E_1$ be event that $A[1..i-1]$ is a specific $(i-1)$-permutation $(x_1, \nldots, x_{i-1})$. $Pr\\\\{E_1\\\\} = (n-i+1)!/n!$. Let $E_2$ be event that $A[i]$ gets value $x_i$ (chosen from $n-i+1$ remaining elements). $Pr\\\\{E_2 | E_1\\\\} = 1/(n-i+1)$. Probability that $A[1..i]$ is $(x_1, \nldots, x_i)$ is $Pr\\\\{E_2 \ncap E_1\\\\} = Pr\\\\{E_2 | E_1\\\\}Pr\\\\{E_1\\\\} = (1/(n-i+1)) \ncdot ((n-i+1)!/n!) = (n-i)!/n!$. This is the invariant for iteration $i+1$.\n- **Termination** ($i=n+1$): $A[1..n]$ contains a specific $n$-permutation with probability $(n-(n+1)+1)!/n! = 0!/n! = 1/n!$.\n\n## 5.4 Probabilistic analysis and further uses of indicator random variables (*)\nThis section presents more examples of probabilistic analysis.\n\n### 5.4.1 The birthday paradox\nHow many people $k$ must be in a room for a $>50\\\\%$ chance that at least two share a birthday? Assume $n=365$ days, birthdays are independent and uniformly distributed.\n- **Analysis via complementary event**: Let $B_k$ be event that $k$ people have distinct birthdays. $Pr\\\\{B_k\\\\} = Pr\\\\{B_{k-1}\\\\}Pr\\\\{A_k|B_{k-1}\\\\}$, where $A_k$ is event that person $k$'s birthday is different from first $k-1$. $Pr\\\\{A_k|B_{k-1}\\\\} = (n-k+1)/n$. So $Pr\\\\{B_k\\\\} = 1 \ncdot \\\\frac{n-1}{n} \ncdot \\\\frac{n-2}{n} \ncdots \\\\frac{n-k+1}{n}$. Using $1+x \\\\le e^x$, $Pr\\\\{B_k\\\\} \\\\le e^{-k(k-1)/(2n)}$. This is $\\\\le 1/2$ when $k(k-1) \\\\ge 2n \\\\ln 2$. For $n=365$, $k \\\\ge 23$.\n- **Analysis using indicator RVs**: Let $X_{ij} = I\\\\{\\\\text{person } i \\\\text{ and } j \\\\text{ share a birthday}\\\\}$ for $1 \\\\le i < j \\\\le k$. $Pr\\\\{\\\\text{persons } i,j \\\\text{ share birthday}\\\\} = 1/n$. $E[X_{ij}] = 1/n$. Let $X$ be total pairs with same birthday. $X = \\\\sum_{i=1}^{k-1} \\\\sum_{j=i+1}^k X_{ij}$. $E[X] = \\\\binom{k}{2} (1/n) = k(k-1)/(2n)$. We expect $E[X]=1$ when $k(k-1) = 2n$, so $k \\\\approx \\\\sqrt{2n}$. For $n=365, k \\\\approx 28$. Both analyses yield $k = \\\\Theta(\\\\sqrt{n})$.\n\n### 5.4.2 Balls and bins\nRandomly toss $n_{balls}$ identical balls into $b$ bins. Tosses are independent, each bin equally likely ($1/b$).\n- **Expected balls in a given bin**: If $n_{balls}$ are tossed, this is $n_{balls}/b$.\n- **Expected tosses until a given bin contains a ball**: This follows a geometric distribution with success probability $1/b$. Expected tosses = $b$.\n- **Expected tosses until every bin contains at least one ball (Coupon Collector's Problem)**: Let $n_i$ be tosses for $i$-th stage (after $(i-1)$-th hit until $i$-th hit). Probability of hit in stage $i$ is $(b-i+1)/b$. $E[n_i] = b/(b-i+1)$. Total expected tosses $N = \\\\sum_{i=1}^b E[n_i] = \\\\sum_{i=1}^b b/(b-i+1) = b \\\\sum_{j=1}^b (1/j) = b H_b \\\\approx b \\\\ln b$.\n\n### 5.4.3 Streaks\nLongest streak of consecutive heads in $n$ fair coin flips.\n- **Expected length is $O(\\\\lg n)$**: Let $A_{ik}$ be event that streak of $\\\\ge k$ heads starts at $i$-th flip. $Pr\\\\{A_{ik}\\\\} = 1/2^k$. For $k = 2\\\\lceil \\\\lg n \\\\rceil$, $Pr\\\\{A_{i,2\\\\lceil \\\\lg n \\\\rceil}\\\\} \\\\le 1/n^2$. Probability any such streak starts $\\\\le n \ncdot (1/n^2) = 1/n$. Let $L$ be length of longest streak. $E[L] = \\\\sum j Pr\\\\{L_j\\\\}$. By splitting sum at $2\\\\lceil \\\\lg n \\\\rceil$, $E[L] < 2\\\\lceil \\\\lg n \\\\rceil \ncdot 1 + n \ncdot (1/n) = O(\\\\lg n)$.\n- **Expected length is $\\\\Omega(\\\\lg n)$**: Partition $n$ flips into $\\\\approx n/s$ groups of $s$ flips. Choose $s = \\\\lfloor (\\\\lg n)/2 \\\\rfloor$. Probability a specific group is all heads is $1/2^s \\\\ge 1/\\\\sqrt{n}$. Probability no group is all heads $\\\\le (1-1/\\\\sqrt{n})^{n/s} \\\\approx e^{-(n/s)/\\\\sqrt{n}} = O(1/n)$. So, with high probability, at least one streak of length $s = \\\\Omega(\\\\lg n)$ occurs. This implies $E[L] = \\\\Omega(\\\\lg n)$.\nThus, longest expected streak is $\\\\Theta(\\\\lg n)$.\n\n### 5.4.4 The online hiring problem\nGoal: Hire exactly once. Strategy: Interview and reject first $k$ candidates. Then hire the first subsequent candidate who is better than all $k$ initial candidates. If no such candidate appears, hire the $n$-th candidate.\nProcedure: [[PartI Foundations Algorithms.md#5.4 ONLINE-MAXIMUM]].\nLet $S$ be event of successfully hiring the best overall candidate. Let $S_i$ be event of success when actual best is candidate $i$.\n$Pr\\\\{S_i\\\\} = 0$ for $i \\\\le k$.\nFor $i > k$, success requires candidate $i$ to be best (event $B_i$, $Pr\\\\{B_i\\\\}=1/n$) AND candidates $k+1, \nldots, i-1$ are not chosen (event $O_i$). Event $O_i$ occurs if the best among $1, \nldots, i-1$ is one of the first $k$. $Pr\\\\{O_i\\\\} = k/(i-1)$.\nEvents $B_i$ and $O_i$ are independent. So $Pr\\\\{S_i\\\\} = Pr\\\\{B_i\\\\}Pr\\\\{O_i\\\\} = (1/n) \ncdot (k/(i-1))$.\nTotal success probability $Pr\\\\{S\\\\} = \\\\sum_{i=k+1}^n Pr\\\\{S_i\\\\} = \\\\sum_{i=k+1}^n \\\\frac{k}{n(i-1)} = \\\\frac{k}{n} \\\\sum_{j=k}^{n-1} \\\\frac{1}{j}$.\nUsing integral approximation for harmonic series: $\\\\sum_{j=k}^{n-1} (1/j) \\\\approx \\\\int_k^{n-1} (1/x)dx = \\\\ln(n-1) - \\\\ln k$.\nSo $Pr\\\\{S\\\\} \\\\approx (k/n)(\\\\ln n - \\\\ln k)$.\nTo maximize this, differentiate wrt $k$ and set to 0: $(\\\\ln n - \\\\ln k - 1)/n = 0 \\\\implies \\\\ln k = \\\\ln n - 1 = \\\\ln(n/e)$. So $k \\\\approx n/e$.\nWith optimal $k=n/e$, $Pr\\\\{S\\\\} \\\\approx ( (n/e)/n ) (\nolimits \\\\ln n - \\\\ln(n/e) ) = (1/e)(\\\\ln n - (\\\\ln n - \\\\ln e)) = (1/e)(\\\\ln e) = 1/e$.\nThus, one can hire the best candidate with probability about $1/e \\\\approx 0.368$."
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartII Sorting and Order Statistics Algorithms.md",
                "contents": "# C6 Heapsort Algorithms\n\n## 6.1 PARENT(i)\n```\nPARENT(i)\n  1 return floor(i/2)\n```\n\n## 6.1 LEFT(i)\n```\nLEFT(i)\n  1 return 2i\n```\n\n## 6.1 RIGHT(i)\n```\nRIGHT(i)\n  1 return 2i + 1\n```\n\n## 6.2 MAX-HEAPIFY(A, i)\n```\nMAX-HEAPIFY(A, i)\n  1  l = LEFT(i)\n  2  r = RIGHT(i)\n  3  if l <= A.heap-size and A[l] > A[i]\n  4      largest = l\n  5  else largest = i\n  6  if r <= A.heap-size and A[r] > A[largest]\n  7      largest = r\n  8  if largest != i\n  9      exchange A[i] with A[largest]\n  10     MAX-HEAPIFY(A, largest)\n```\n\n## 6.3 BUILD-MAX-HEAP(A, n)\n```\nBUILD-MAX-HEAP(A, n)\n  1  A.heap-size = n\n  2  for i = floor(n/2) downto 1\n  3      MAX-HEAPIFY(A, i)\n```\n\n## 6.4 HEAPSORT(A, n)\n```\nHEAPSORT(A, n)\n  1  BUILD-MAX-HEAP(A, n)\n  2  for i = n downto 2\n  3      exchange A[1] with A[i]\n  4      A.heap-size = A.heap-size - 1\n  5      MAX-HEAPIFY(A, 1)\n```\n\n## 6.5 MAX-HEAP-MAXIMUM(A)\n```\nMAX-HEAP-MAXIMUM(A)\n  1  if A.heap-size < 1\n  2      error \"heap underflow\"\n  3  return A[1]\n```\n\n## 6.5 MAX-HEAP-EXTRACT-MAX(A)\n```\nMAX-HEAP-EXTRACT-MAX(A)\n  1  max = MAX-HEAP-MAXIMUM(A)\n  2  A[1] = A[A.heap-size]\n  3  A.heap-size = A.heap-size - 1\n  4  MAX-HEAPIFY(A, 1)\n  5  return max\n```\n\n## 6.5 MAX-HEAP-INCREASE-KEY(A, x, k)\n```\nMAX-HEAP-INCREASE-KEY(A, x, k)\n  1  if k < x.key\n  2      error \"new key is smaller than current key\"\n  3  x.key = k\n  4  find the index i in array A where object x occurs\n  5  while i > 1 and A[PARENT(i)].key < A[i].key\n  6      exchange A[i] with A[PARENT(i)], updating the information that maps\n         priority queue objects to array indices\n  7      i = PARENT(i)\n```\n\n## 6.5 MAX-HEAP-INSERT(A, x, n)\n```\nMAX-HEAP-INSERT(A, x, n)\n  1  if A.heap-size == n\n  2      error \"heap overflow\"\n  3  A.heap-size = A.heap-size + 1\n  4  k = x.key\n  5  x.key = -infinity\n  6  A[A.heap-size] = x\n  7  map x to index heap-size in the array\n  8  MAX-HEAP-INCREASE-KEY(A, x, k)\n```\n\n# C7 Quicksort Algorithms\n\n## 7.1 QUICKSORT(A, p, r)\n```\nQUICKSORT(A, p, r)\n  1  if p < r\n  2      // Partition the subarray around the pivot, which ends up in A[q].\n  3      q = PARTITION(A, p, r)\n  4      QUICKSORT(A, p, q - 1)  // recursively sort the low side\n  5      QUICKSORT(A, q + 1, r)  // recursively sort the high side\n```\n\n## 7.1 PARTITION(A, p, r)\n```\nPARTITION(A, p, r)\n  1  x = A[r]                // the pivot\n  2  i = p - 1               // highest index into the low side\n  3  for j = p to r - 1      // process each element other than the pivot\n  4      if A[j] <= x        // does this element belong on the low side?\n  5          i = i + 1       // index of a new slot in the low side\n  6          exchange A[i] with A[j] // put this element there\n  7  exchange A[i + 1] with A[r] // pivot goes just to the right of the low side\n  8  return i + 1             // new index of the pivot\n```\n\n## 7.3 RANDOMIZED-PARTITION(A, p, r)\n```\nRANDOMIZED-PARTITION(A, p, r)\n  1  i = RANDOM(p, r)\n  2  exchange A[r] with A[i]\n  3  return PARTITION(A, p, r)\n```\n\n## 7.3 RANDOMIZED-QUICKSORT(A, p, r)\n```\nRANDOMIZED-QUICKSORT(A, p, r)\n  1  if p < r\n  2      q = RANDOMIZED-PARTITION(A, p, r)\n  3      RANDOMIZED-QUICKSORT(A, p, q - 1)\n  4      RANDOMIZED-QUICKSORT(A, q + 1, r)\n```\n\n## C7.P7-1 HOARE-PARTITION(A, p, r)\n```\nHOARE-PARTITION(A, p, r)\n  1  x = A[p]\n  2  i = p - 1\n  3  j = r + 1\n  4  while TRUE\n  5      repeat\n  6          j = j - 1\n  7      until A[j] <= x\n  8      repeat\n  9          i = i + 1\n  10     until A[i] >= x\n  11     if i < j\n  12         exchange A[i] with A[j]\n  13     else return j\n```\n\n## C7.P7-4 STOOGE-SORT(A, p, r)\n```\nSTOOGE-SORT(A, p, r)\n  1  if A[p] > A[r]\n  2      exchange A[p] with A[r]\n  3  if p + 1 < r\n  4      k = floor((r - p + 1)/3)  // round down\n  5      STOOGE-SORT(A, p, r - k)    // first two-thirds\n  6      STOOGE-SORT(A, p + k, r)    // last two-thirds\n  7      STOOGE-SORT(A, p, r - k)    // first two-thirds again\n```\n\n## C7.P7-5 TRE-QUICKSORT(A, p, r)\n```\nTRE-QUICKSORT(A, p, r)\n  1  while p < r\n       // Partition and then sort the low side.\n  2      q = PARTITION(A, p, r)\n  3      TRE-QUICKSORT(A, p, q - 1)\n  4      p = q + 1\n```\n\n# C8 Sorting in Linear Time Algorithms\n\n## 8.2 COUNTING-SORT(A, n, k)\n```\nCOUNTING-SORT(A, n, k)\n  1  let B[1:n] and C[0:k] be new arrays\n  2  for i = 0 to k\n  3      C[i] = 0\n  4  for j = 1 to n\n  5      C[A[j]] = C[A[j]] + 1\n  6  // C[i] now contains the number of elements equal to i.\n  7  for i = 1 to k\n  8      C[i] = C[i] + C[i - 1]\n  9  // C[i] now contains the number of elements less than or equal to i.\n  10 // Copy A to B, starting from the end of A.\n  11 for j = n downto 1\n  12     B[C[A[j]]] = A[j]\n  13     C[A[j]] = C[A[j]] - 1  // to handle duplicate values\n  14 return B\n```\n\n## 8.3 RADIX-SORT(A, n, d)\n```\nRADIX-SORT(A, n, d)\n  1  for i = 1 to d\n  2      use a stable sort to sort array A[1:n] on digit i\n```\n\n## 8.4 BUCKET-SORT(A, n)\n```\nBUCKET-SORT(A, n)\n  1  let B[0:n-1] be a new array\n  2  for i = 0 to n - 1\n  3      make B[i] an empty list\n  4  for i = 1 to n\n  5      insert A[i] into list B[floor(n * A[i])]\n  6  for i = 0 to n - 1\n  7      sort list B[i] with insertion sort\n  8  concatenate the lists B[0], B[1], ..., B[n-1] together in order\n  9  return the concatenated lists\n```\n\n## C8.P8-7 COMPARE-EXCHANGE(A, i, j)\n```\nCOMPARE-EXCHANGE(A, i, j)\n  1  if A[i] > A[j]\n  2      exchange A[i] with A[j]\n```\n\n## C8.P8-7 COMPARE-EXCHANGE-INSERTION-SORT(A, n)\n```\nCOMPARE-EXCHANGE-INSERTION-SORT(A, n)\n  1  for i = 2 to n\n  2      for j = i - 1 downto 1\n  3          COMPARE-EXCHANGE(A, j, j + 1)\n```\n\n# C9 Medians and Order Statistics Algorithms\n\n## 9.1 MINIMUM(A, n)\n```\nMINIMUM(A, n)\n  1  min = A[1]\n  2  for i = 2 to n\n  3      if min > A[i]\n  4          min = A[i]\n  5  return min\n```\n\n## 9.2 RANDOMIZED-SELECT(A, p, r, i)\n```\nRANDOMIZED-SELECT(A, p, r, i)\n  1  if p == r\n  2      return A[p]  // 1 <= i <= r - p + 1 when p == r means that i = 1\n  3  q = RANDOMIZED-PARTITION(A, p, r)\n  4  k = q - p + 1\n  5  if i == k\n  6      return A[q]  // the pivot value is the answer\n  7  elseif i < k\n  8      return RANDOMIZED-SELECT(A, p, q - 1, i)\n  9  else return RANDOMIZED-SELECT(A, q + 1, r, i - k)\n```\n\n## 9.3 SELECT(A, p, r, i)\n```\nSELECT(A, p, r, i)\n  1  while (r - p + 1) mod 5 != 0\n  2      for j = p + 1 to r\n  3          if A[p] > A[j]\n  4              exchange A[p] with A[j] // put the minimum into A[p]\n       // If we want the minimum of A[p:r], we're done.\n  5      if i == 1\n  6          return A[p]\n       // Otherwise, we want the (i-1)st element of A[p+1:r].\n  7      p = p + 1\n  8      i = i - 1\n  9  g = (r - p + 1)/5  // number of 5-element groups\n  10 for j = p to p + g - 1 // sort each group\n  11     sort (A[j], A[j+g], A[j+2g], A[j+3g], A[j+4g]) in place\n     // All group medians now lie in the middle fifth of A[p:r].\n     // Find the pivot x recursively as the median of the group medians.\n  12 x = SELECT(A, p + 2g, p + 3g - 1, floor(g/2))\n  13 q = PARTITION-AROUND(A, p, r, x) // partition around the pivot\n     // The rest is just like lines 3-9 of RANDOMIZED-SELECT.\n  14 k = q - p + 1\n  15 if i == k\n  16     return A[q] // the pivot value is the answer\n  17 elseif i < k\n  18     return SELECT(A, p, q - 1, i)\n  19 else return SELECT(A, q + 1, r, i - k)\n```\n\n## C9.P9-2 SIMPLER-RANDOMIZED-SELECT(A, p, r, i)\n```\nSIMPLER-RANDOMIZED-SELECT(A, p, r, i)\n  1  if p == r\n  2      return A[p] // 1 <= i <= r - p + 1 means that i = 1\n  3  q = RANDOMIZED-PARTITION(A, p, r)\n  4  k = q - p + 1\n  5  if i <= k\n  6      return SIMPLER-RANDOMIZED-SELECT(A, p, q, i)\n  7  else return SIMPLER-RANDOMIZED-SELECT(A, q + 1, r, i - k)\n```\n\n## C9.Notes SELECT3(A, p, r, i)\n```\nSELECT3(A, p, r, i)\n  1  while (r - p + 1) mod 9 != 0\n  2      for j = p + 1 to r\n  3          if A[p] > A[j]\n  4              exchange A[p] with A[j] // put the minimum into A[p]\n  5      // If we want the minimum of A[p:r], we're done.\n  6      if i == 1\n  7          return A[p]\n  8      // Otherwise, we want the (i-1)st element of A[p+1:r].\n  9      p = p + 1\n  10     i = i - 1\n  11 g = (r - p + 1)/3                 // number of 3-element groups\n  12 for j = p to p + g - 1            // run through the groups\n  13     sort (A[j], A[j+g], A[j+2g]) in place\n     // All group medians now lie in the middle third of A[p:r].\n  14 g' = g/3                          // number of 3-element subgroups\n  15 for j = p + g to p + g + g' - 1   // sort the subgroups\n  16     sort (A[j], A[j+g'], A[j+2g']) in place\n     // All subgroup medians now lie in the middle ninth of A[p:r].\n     // Find the pivot x recursively as the median of the subgroup medians.\n  17 x = SELECT3(A, p + 4g', p + 5g' - 1, floor(g'/2))\n  18 q = PARTITION-AROUND(A, p, r, x)  // partition around the pivot\n     // The rest is just like lines 19-24 of SELECT.\n  19 k = q - p + 1\n  20 if i == k\n  21     return A[q]                    // the pivot value is the answer\n  22 elseif i < k\n  23     return SELECT3(A, p, q - 1, i)\n  24 else return SELECT3(A, q + 1, r, i - k)\n```\n"
            },
            {
                "file": "PartII Sorting and Order Statistics/C6 Heapsort.md",
                "contents": "# Part II Sorting and Order Statistics\n\n## Introduction to Part II Sorting and Order Statistics\nThis part presents several algorithms that solve the sorting problem and related problems concerning order statistics.\n\n### The Sorting Problem\n**Input**: A sequence of $n$ numbers $\\langle a_1, a_2, \\ldots, a_n \\rangle$.\n**Output**: A permutation (reordering) $\\langle a'_1, a'_2, \\ldots, a'_n \\rangle$ of the input sequence such that $a'_1 \\le a'_2 \\le \\ldots \\le a'_n$.\nThe input sequence is usually an $n$-element array, although it may be represented in some other fashion, such as a linked list.\n\n### The structure of the data\nIn practice, numbers to be sorted are rarely isolated values. Each is usually part of a collection of data called a **record**. Each record contains a **key**, which is the value to be sorted. The remainder of the record consists of **satellite data**. Sorting algorithms typically permute keys and must permute satellite data as well. If satellite data is large, permuting an array of pointers to the records is often more efficient than permuting the records themselves to minimize data movement.\nA sorting algorithm describes the *method* to determine the sorted order, regardless of whether sorting individual numbers or large records. We typically assume input consists only of numbers for simplicity.\n\n### Why sorting?\nMany computer scientists consider sorting to be the most fundamental problem in the study of algorithms for several reasons:\n- Applications inherently need to sort information (e.g., banks sorting checks by number for customer statements).\n- Algorithms often use sorting as a key subroutine (e.g., rendering graphical objects layered on top of each other).\n- A wide variety of sorting algorithms exist, employing a rich set of techniques. Many important techniques used throughout algorithm design appear in sorting algorithms developed over the years.\n- We can prove nontrivial lower bounds for sorting. Since the best upper bounds match the lower bound asymptotically, certain sorting algorithms are asymptotically optimal. Moreover, the lower bound for sorting can be used to prove lower bounds for various other problems.\n- Many engineering issues come to the fore when implementing sorting algorithms, depending on factors like prior knowledge about keys and satellite data, memory hierarchy, and software environment.\n\n### Overview of Algorithms in Part II\nThis part introduces several sorting algorithms and algorithms for order statistics.\n\n#### Sorting Algorithms\n- **Comparison sorts** determine the sorted order of an input array by comparing elements.\n    - **Heapsort** (Chapter 6): Sorts $n$ numbers in place in $O(n \\lg n)$ time. It uses an important data structure called a heap.\n    - **Quicksort** (Chapter 7): Also sorts $n$ numbers in place. Its worst-case running time is $\\Theta(n^2)$, but its expected running time is $O(n \\lg n)$ and it generally outperforms heapsort in practice.\n- Chapter 8 introduces the decision-tree model to study performance limitations of comparison sorts, proving an $\\Omega(n \\lg n)$ lower bound on their worst-case running time.\n- **Linear-time sorts** (Chapter 8): These algorithms are not comparison sorts and can achieve better performance under certain assumptions.\n    - **Counting sort**: Assumes input numbers belong to the set $\\{0, 1, \\ldots, k\\}$. It sorts $n$ numbers in $\\Theta(k+n)$ time.\n    - **Radix sort**: Extends counting sort. If there are $n$ integers to sort, each integer has $d$ digits, and each digit can take on up to $k$ possible values, radix sort sorts the numbers in $\\Theta(d(n+k))$ time.\n    - **Bucket sort**: Requires knowledge of the probabilistic distribution of numbers in the input array. It can sort $n$ real numbers uniformly distributed in the half-open interval $[0, 1)$ in average-case $O(n)$ time.\nA table on page 160 (OCR page 5) summarizes the running times of these algorithms.\n\n#### Order Statistics (Chapter 9)\nThe $i$-th order statistic of a set of $n$ numbers is the $i$-th smallest number in the set.\nChapter 9 shows how to find the $i$-th smallest element in $O(n)$ time, even when elements are arbitrary real numbers. It presents a randomized algorithm with $O(n)$ expected time and a more complicated algorithm that runs in $O(n)$ worst-case time.\n\n### Background\nAnalyses of quicksort, bucket sort, and the order-statistic algorithm use probability, which is reviewed in Appendix C, and material on probabilistic analysis and randomized algorithms from Chapter 5.\n\n---\n# 6 Heapsort\n## Introduction (Chapter 6 Introduction)\nThis chapter introduces heapsort. Like merge sort, but unlike insertion sort, heapsort's running time is $O(n \\lg n)$. Like insertion sort, but unlike merge sort, heapsort sorts in place: only a constant number of array elements are stored outside the input array at any time. Thus, heapsort combines better attributes of the two sorting algorithms already discussed.\nHeapsort also introduces using a data structure (a \"heap\") to manage information. The heap data structure is useful for heapsort and for making an efficient priority queue. The term \"heap\" in this context refers to the data structure, not to garbage-collected storage common in languages like Java and Python.\n\n## 6.1 Heaps\nThe (binary) **heap** data structure is an array object that we can view as a nearly complete binary tree (see Section B.5.3), as shown in Figure 6.1 of the book. Each node of the tree corresponds to an element of the array. The tree is completely filled on all levels except possibly the lowest, which is filled from the left up to a point.\nAn array $A[1 \\ldots n]$ that represents a heap is an object with an attribute `A.heap-size`, which represents how many elements in the heap are stored within array $A$. That is, although $A[1 \\ldots n]$ may contain numbers, only the elements in $A[1 \\ldots \\text{A.heap-size}]$, where $0 \\le \\text{A.heap-size} \\le n$, are valid elements of the heap. If `A.heap-size` = 0, the heap is empty. The root of the tree is $A[1]$.\nGiven the index $i$ of a node:\n- `PARENT(i)` returns $\\lfloor i/2 \\rfloor$. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.1 PARENT(i)]].\n- `LEFT(i)` returns $2i$. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.1 LEFT(i)]].\n- `RIGHT(i)` returns $2i+1$. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.1 RIGHT(i)]].\nThese operations can be computed efficiently, often using bit shifts.\n\nThere are two kinds of binary heaps: max-heaps and min-heaps. In both kinds, the values in the nodes satisfy a **heap property**.\n- In a **max-heap**, the **max-heap property** is that for every node $i$ other than the root, $A[\\text{PARENT}(i)] \\ge A[i]$. Thus, the largest element in a max-heap is stored at the root, and the subtree rooted at a node contains values no larger than that contained at the node itself.\n- In a **min-heap**, the **min-heap property** is that for every node $i$ other than the root, $A[\\text{PARENT}(i)] \\le A[i]$. The smallest element in a min-heap is at the root.\nThe heapsort algorithm uses max-heaps. Min-heaps commonly implement priority queues.\n\nThe **height** of a node in a heap is the number of edges on the longest simple downward path from the node to a leaf. The **height of the heap** is the height of its root. Since a heap of $n$ elements is based on a complete binary tree, its height is $\\Theta(\\lg n)$ (see Exercise 6.1-2). Basic operations on heaps run in time at most proportional to the height of the tree, and thus take $O(\\lg n)$ time.\nThis chapter presents:\n- The `MAX-HEAPIFY` procedure, which runs in $O(\\lg n)$ time, key to maintaining the max-heap property.\n- The `BUILD-MAX-HEAP` procedure, which runs in linear time, produces a max-heap from an unordered input array.\n- The `HEAPSORT` procedure, which runs in $O(n \\lg n)$ time, sorts an array in place.\n- Procedures for implementing a priority queue using a heap.\n\n## 6.2 Maintaining the heap property\nThe procedure `MAX-HEAPIFY` maintains the max-heap property. Its inputs are an array $A$ and an index $i$ into the array. When it is called, `MAX-HEAPIFY` assumes that the binary trees rooted at `LEFT(i)` and `RIGHT(i)` are max-heaps, but that $A[i]$ might be smaller than its children, thus violating the max-heap property. `MAX-HEAPIFY` lets the value at $A[i]$ \"float down\" in the max-heap so that the subtree rooted at index $i$ obeys the max-heap property. Figure 6.2 of the book illustrates its action. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.2 MAX-HEAPIFY(A, i)]].\nTo analyze `MAX-HEAPIFY`, let $T(n)$ be the worst-case running time on a subtree of size at most $n$. The running time is $\\Theta(1)$ to fix relationships among $A[i]$, $A[\\text{LEFT}(i)]$, and $A[\\text{RIGHT}(i)]$, plus time to run `MAX-HEAPIFY` on a child's subtree. Children's subtrees each have size at most $2n/3$. The recurrence is $T(n) \\le T(2n/3) + \\Theta(1)$. By case 2 of the master theorem, the solution is $T(n) = O(\\lg n)$. Alternatively, running time on a node of height $h$ is $O(h)$.\n\n## 6.3 Building a heap\nThe procedure `BUILD-MAX-HEAP` converts an array $A[1 \\ldots n]$ into a max-heap by calling `MAX-HEAPIFY` in a bottom-up manner. Elements in $A[\\lfloor n/2 \\rfloor + 1 \\ldots n]$ are leaves, so each is a 1-element heap. `BUILD-MAX-HEAP` iterates through remaining nodes and runs `MAX-HEAPIFY` on each. Figure 6.3 of the book shows an example. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.3 BUILD-MAX-HEAP(A, n)]].\nCorrectness uses a loop invariant:\n- **Loop Invariant**: At the start of each iteration of the `for` loop (lines 2-3), each node $i+1, i+2, \\ldots, n$ is the root of a max-heap.\n- **Initialization**: Prior to first iteration, $i = \\lfloor n/2 \\rfloor$. Nodes $\\lfloor n/2 \\rfloor + 1, \\ldots, n$ are leaves, thus roots of trivial max-heaps.\n- **Maintenance**: Children of node $i$ are numbered higher than $i$. By loop invariant, they are roots of max-heaps. This is the condition for `MAX-HEAPIFY(A, i)` to make node $i$ a max-heap root. `MAX-HEAPIFY` preserves property for nodes $i+1, \\ldots, n$. Decrementing $i$ reestablishes invariant for next iteration.\n- **Termination**: Loop terminates when $i=0$. Each node $1, \\ldots, n$ is root of a max-heap. Node 1 is root of the overall max-heap.\nA simple upper bound on running time is $O(n \\lg n)$ ($O(n)$ calls to `MAX-HEAPIFY`, each $O(\\lg n)$). A tighter bound observes that time for `MAX-HEAPIFY` varies with node height. An $n$-element heap has height $\\lfloor \\lg n \\rfloor$ and at most $\\lceil n/2^{h+1} \\rceil$ nodes of any height $h$. Time for `MAX-HEAPIFY` on a node of height $h$ is $O(h)$. Total cost is bounded by $\\sum_{h=0}^{\\lfloor \\lg n \\rfloor} \\lceil n/2^{h+1} \\rceil O(h) = O(n \\sum_{h=0}^{\\lfloor \\lg n \\rfloor} h/2^h) = O(n)$. Thus, `BUILD-MAX-HEAP` takes $O(n)$ time.\nTo build a min-heap, use `BUILD-MIN-HEAP` with `MIN-HEAPIFY`.\n\n## 6.4 The heapsort algorithm\nThe `HEAPSORT` algorithm starts by calling `BUILD-MAX-HEAP` to build a max-heap on the input array $A[1 \\ldots n]$. Since $A[1]$ is the maximum element, it is placed in its correct final position by exchanging it with $A[n]$. Node $n$ is then discarded from the heap (by decrementing `A.heap-size`). The children of the root remain max-heaps, but the new root might violate the max-heap property. `MAX-HEAPIFY(A,1)` restores it. This process repeats for max-heaps of size $n-1$ down to 2. Figure 6.4 of the book shows an example. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.4 HEAPSORT(A, n)]].\nThe `HEAPSORT` procedure takes $O(n \\lg n)$ time, since `BUILD-MAX-HEAP` takes $O(n)$ time and each of the $n-1$ calls to `MAX-HEAPIFY` takes $O(\\lg n)$ time.\n\n## 6.5 Priority queues\nA **priority queue** is a data structure for maintaining a set $S$ of elements, each with an associated value called a key. A **max-priority queue** supports operations:\n- `INSERT(S, x, k)`: inserts element $x$ with key $k$ into $S$.\n- `MAXIMUM(S)`: returns element of $S$ with largest key.\n- `EXTRACT-MAX(S)`: removes and returns element of $S$ with largest key.\n- `INCREASE-KEY(S, x, k)`: increases value of element $x$'s key to new value $k$.\nApplications include job scheduling. A **min-priority queue** supports `INSERT`, `MINIMUM`, `EXTRACT-MIN`, `DECREASE-KEY`. Applications include event-driven simulators.\nHeaps can implement priority queues. `MAX-HEAP-MAXIMUM` takes $\\Theta(1)$ time. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.5 MAX-HEAP-MAXIMUM(A)]].\n`MAX-HEAP-EXTRACT-MAX` is similar to the loop body of `HEAPSORT`. Running time is $O(\\lg n)$. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.5 MAX-HEAP-EXTRACT-MAX(A)]].\n`MAX-HEAP-INCREASE-KEY` updates an element's key. If the new key violates the max-heap property, it traverses up the tree, exchanging with parent if element's key is larger, until max-heap property holds. Running time on an $n$-element heap is $O(\\lg n)$. Figure 6.5 of the book shows an example. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.5 MAX-HEAP-INCREASE-KEY(A, x, k)]].\n`MAX-HEAP-INSERT` expands the max-heap, adds new leaf with key $-\\infty$, then calls `MAX-HEAP-INCREASE-KEY`. Running time on an $n$-element heap is $O(\\lg n)$. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C6.5 MAX-HEAP-INSERT(A, x, n)]].\nIn summary, a heap supports priority-queue operations on a set of size $n$ in $O(\\lg n)$ time, plus mapping overhead if applicable.\n\n## Problems\nChapter 6 problems include topics such as: Building a heap using insertion, Analysis of d-ary heaps, Young tableaus.\n\n## Chapter notes\nThe heapsort algorithm was invented by Williams (1964). `BUILD-MAX-HEAP` was suggested by Floyd (1964). Schaffer and Sedgewick (1993) analyzed element movements in heapsort. More complex data structures like Fibonacci heaps offer better amortized times for some priority queue operations. Other advanced heaps exist if keys are integers or drawn from specific sets.\n"
            },
            {
                "file": "PartII Sorting and Order Statistics/C7 Quicksort.md",
                "contents": "# 7 Quicksort\n## Introduction\nQuicksort has a worst-case running time of $\\Theta(n^2)$ on an input array of $n$ numbers. Despite this, quicksort is often the best practical choice for sorting because it is remarkably efficient on average: its expected running time is $O(n \\lg n)$ when all numbers are distinct, and the constant factors hidden in the $O(n \\lg n)$ notation are small. Unlike merge sort, it also has the advantage of sorting in place, and it works well even in virtual-memory environments.\n\n## 7.1 Description of quicksort\nQuicksort, like merge sort, applies the divide-and-conquer method.\nIn the divide-and-conquer method for sorting subarray $A[p \\ldots r]$:\n- **Divide**: Partition (rearrange) $A[p \\ldots r]$ into two (possibly empty) subarrays $A[p \\ldots q-1]$ (low side) and $A[q+1 \\ldots r]$ (high side) such that each element in the low side is $\\le$ the pivot $A[q]$, which is, in turn, $\\le$ each element in the high side. Compute index $q$ as part of partitioning.\n- **Conquer**: Call quicksort recursively to sort $A[p \\ldots q-1]$ and $A[q+1 \\ldots r]$.\n- **Combine**: Do nothing. The subarrays are sorted in place, and after conquering, the entire $A[p \\ldots r]$ is sorted.\nRefer to [[PartII Sorting and Order Statistics Algorithms.md#C7.1 QUICKSORT(A, p, r)]] for the `QUICKSORT` procedure. To sort an entire $n$-element array $A[1 \\ldots n]$, the initial call is `QUICKSORT(A, 1, n)`.\n\n### Partitioning the array\nThe key is the `PARTITION` procedure, which rearranges $A[p \\ldots r]$ in place. `PARTITION` always selects $x = A[r]$ as the pivot. As it runs, elements fall into one of four regions. Figure 7.2 of the book shows these regions. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C7.1 PARTITION(A, p, r)]].\nCorrectness of `PARTITION` uses a loop invariant for the `for` loop (lines 3-6):\nAt the beginning of each iteration, for any array index $k$:\n1. If $p \\le k \\le i$, then $A[k] \\le x$.\n2. If $i+1 \\le k \\le j-1$, then $A[k] > x$.\n3. If $k=r$, then $A[k]=x$.\n- **Initialization**: Before the first loop iteration, $i=p-1$ and $j=p$. Regions for conditions 1 and 2 are empty. Condition 3 holds by $A[r]=x$.\n- **Maintenance**: If $A[j] > x$, only $j$ increments, maintaining condition 2 for $A[j-1]$. If $A[j] \\le x$, $i$ increments, $A[i]$ and $A[j]$ swap, then $j$ increments. This maintains condition 1 for new $A[i]$ and condition 2 for $A[j-1]$.\n- **Termination**: Loop terminates when $j=r$. All elements are partitioned into three sets: those $\\le x$, those $> x$, and the pivot $x$. The final swap (line 7) places pivot $x$ into its correct sorted position. The procedure returns the pivot's new index.\nThe running time of `PARTITION` on a subarray of $n=r-p+1$ elements is $\\Theta(n)$.\n\n## 7.2 Performance of quicksort\nRunning time depends on how balanced partitioning is, which depends on pivot choice. If balanced, it's as fast as merge sort. If unbalanced, as slow as insertion sort. Quicksort's memory use (stack depth for recursion) is not constant, could be $O(n)$ worst-case.\n\n### Worst-case partitioning\nOccurs if partitioning produces one subproblem with $n-1$ elements and one with 0 elements. Recurrence: $T(n) = T(n-1) + T(0) + \\Theta(n) = T(n-1) + \\Theta(n)$. Solution is $T(n) = \\Theta(n^2)$. This happens if input array is already sorted and $A[r]$ is always chosen as pivot.\n\n### Best-case partitioning\nIf `PARTITION` produces two subproblems, each of size no more than $n/2$. Recurrence: $T(n) = 2T(n/2) + \\Theta(n)$. Solution is $T(n) = \\Theta(n \\lg n)$.\n\n### Balanced partitioning\nEven a constant-proportional split (e.g., 9-to-1) yields $O(n \\lg n)$ running time. If $T(n) = T(9n/10) + T(n/10) + \\Theta(n)$, the recursion tree depth is $\\Theta(\\lg n)$ and cost per level is $O(n)$, so total $O(n \\lg n)$. Figure 7.4 of the book illustrates this.\n\n### Intuition for the average case\nAssuming all permutations of input numbers are equally likely and elements are distinct. Partitioning is unlikely to be same way at every level. A mix of \"good\" (balanced) and \"bad\" (unbalanced) splits occurs. Even if good and bad splits alternate, the cost of a bad split can be absorbed into the cost of a subsequent good split, yielding $O(n \\lg n)$ overall. Figure 7.5 of the book shows this.\n\n## 7.3 A randomized version of quicksort\nTo obtain good expected performance over all inputs, we randomize the algorithm. Instead of always using $A[r]$ as pivot, `RANDOMIZED-PARTITION` randomly chooses a pivot from subarray $A[p \\ldots r]$ and exchanges it with $A[r]$ before partitioning. This ensures the pivot is equally likely to be any element, making the split well-balanced on average. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C7.3 RANDOMIZED-PARTITION(A, p, r)]] and [[PartII Sorting and Order Statistics Algorithms.md#C7.3 RANDOMIZED-QUICKSORT(A, p, r)]].\n\n## 7.4 Analysis of quicksort\n### 7.4.1 Worst-case analysis\nApplies to both `QUICKSORT` and `RANDOMIZED-QUICKSORT`. The recurrence for worst-case time is $T(n) = \\max_{0 \\le q \\le n-1} \\{T(q) + T(n-1-q)\\} + \\Theta(n)$. The expression $q^2 + (n-1-q)^2$ is maximized when $q=0$ or $q=n-1$. The solution is $T(n) = \\Theta(n^2)$.\n\n### 7.4.2 Expected running time\nAnalysis for `RANDOMIZED-QUICKSORT`, assuming distinct elements. Let elements be $z_1 < z_2 < \\ldots < z_n$ in sorted order.\n**Running time and comparisons**: Lemma 7.1 states running time is $O(n+X)$, where $X$ is total element comparisons.\n**When elements are compared**: Lemma 7.2 states $z_i$ is compared with $z_j$ (for $i<j$) if and only if $z_i$ or $z_j$ is the first element chosen as a pivot from the set $Z_{ij} = \\{z_i, z_{i+1}, \\ldots, z_j\\}$. No two elements are compared more than once.\n**Probability of comparison**: Lemma 7.3 states $Pr\\{z_i \\text{ is compared with } z_j\\} = 2/(j-i+1)$. This is because $z_i$ and $z_j$ are compared iff one of them is chosen as the first pivot from $Z_{ij}$. Any element in $Z_{ij}$ is equally likely to be chosen first, and there are $j-i+1$ elements in $Z_{ij}$.\n**Expected number of comparisons**: Theorem 7.4 states expected running time is $O(n \\lg n)$.\nLet $X_{ij} = I\\{z_i \\text{ is compared with } z_j\\}$. Total comparisons $X = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n X_{ij}$.\n$E[X] = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n E[X_{ij}] = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n Pr\\{z_i \\text{ is compared with } z_j\\} = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n \\frac{2}{j-i+1}$.\nChanging variables $k=j-i$, $E[X] = \\sum_{i=1}^{n-1} \\sum_{k=1}^{n-i} \\frac{2}{k+1} < \\sum_{i=1}^{n-1} \\sum_{k=1}^{n} \\frac{2}{k} = \\sum_{i=1}^{n-1} O(\\lg n) = O(n \\lg n)$.\nThus, expected running time is $O(n \\lg n)$.\n\n## Problems\nChapter 7 problems include topics such as: Hoare partition correctness (see [[PartII Sorting and Order Statistics Algorithms.md#C7.P7-1 HOARE-PARTITION(A, p, r)]]), Quicksort with equal element values, Alternative quicksort analysis, Stooge sort (see [[PartII Sorting and Order Statistics Algorithms.md#C7.P7-4 STOOGE-SORT(A, p, r)]]), Stack depth for quicksort (see [[PartII Sorting and Order Statistics Algorithms.md#C7.P7-5 TRE-QUICKSORT(A, p, r)]]), Median-of-3 partition, Fuzzy sorting of intervals.\n\n## Chapter notes\nQuicksort was invented by Hoare (1961). The `PARTITION` procedure in Section 7.1 is attributed to N. Lomuto. The expected time analysis is based on Motwani and Raghavan. McIlroy showed how to create inputs that cause most quicksort implementations to take $\\Theta(n^2)$ time.\n"
            },
            {
                "file": "PartII Sorting and Order Statistics/C8 Sorting in Linear Time.md",
                "contents": "# 8 Sorting in Linear Time\n## Introduction\nMerge sort and heapsort achieve $O(n \\lg n)$ worst-case time. Quicksort achieves $O(n \\lg n)$ expected time. These algorithms are **comparison sorts**: they determine sorted order based only on comparisons between input elements. This chapter proves that any comparison sort must make $\\Omega(n \\lg n)$ comparisons in the worst case. Thus, merge sort and heapsort are asymptotically optimal.\nThis chapter also examines three algorithms\u2014counting sort, radix sort, and bucket sort\u2014that run in linear time on certain inputs. These algorithms use operations other than comparisons, so the $\\Omega(n \\lg n)$ lower bound does not apply.\n\n## 8.1 Lower bounds for sorting\nA **comparison sort** uses only comparisons ($<, \\le, =, \\ge, >$) between elements to gain order information. Assume all input elements are distinct, simplifying comparisons to, e.g., $a_i \\le a_j$.\n\n### The decision-tree model\nComparison sorts can be viewed abstractly as **decision trees**. A decision tree is a full binary tree representing comparisons for a particular algorithm on an input of fixed size $n$. Internal nodes are annotated $i:j$, representing comparison $a_i \\le a_j$. Left child path is taken if $a_i \\le a_j$, right if $a_i > a_j$. Leaves are annotated with a permutation $\\langle \\pi(1), \\ldots, \\pi(n) \\rangle$ indicating the sorted order $a_{\\pi(1)} \\le \\ldots \\le a_{\\pi(n)}$. Figure 8.1 of the book shows an example for insertion sort on 3 elements.\nFor a correct sorting algorithm, each of the $n!$ permutations must appear as at least one reachable leaf.\n\n### A lower bound for the worst case\nThe length of the longest path from root to any reachable leaf is the worst-case number of comparisons. This is the height $h$ of the decision tree.\n**Theorem 8.1**: Any comparison sort algorithm requires $\\Omega(n \\lg n)$ comparisons in the worst case.\n*Proof*: A decision tree of height $h$ has at most $2^h$ leaves. Since there are $n!$ possible permutations and each must be a reachable leaf, $n! \\le l \\le 2^h$, where $l$ is number of leaves. Thus $h \\ge \\lg(n!)$. Using Stirling's approximation or $\\lg(n!) = \\sum_{k=1}^n \\lg k = \\Omega(n \\lg n)$ (Equation 3.28), $h = \\Omega(n \\lg n)$.\n**Corollary 8.2**: Heapsort and merge sort are asymptotically optimal comparison sorts.\n*Proof*: Their $O(n \\lg n)$ upper bounds match the $\\Omega(n \\lg n)$ lower bound.\n\n## 8.2 Counting sort\nCounting sort assumes each of the $n$ input elements is an integer in the range $0$ to $k$. It runs in $\\Theta(n+k)$ time. If $k=O(n)$, it runs in $\\Theta(n)$ time.\nCounting sort determines for each input element $x$, the number of elements $\\le x$. It uses this to place $x$ directly into its output position. It modifies this to handle duplicate values correctly. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C8.2 COUNTING-SORT(A, n, k)]]. Figure 8.2 of the book illustrates its operation.\n1. Initialize count array $C[0 \\ldots k]$ to zeros (lines 2-3, $\\Theta(k)$ time).\n2. Count occurrences of each element $A[j]$ into $C[A[j]]$ (lines 4-5, $\\Theta(n)$ time).\n3. Compute cumulative counts: $C[i]$ stores number of elements $\\le i$ (lines 7-8, $\\Theta(k)$ time).\n4. Place elements from $A$ into sorted positions in output array $B$, iterating $A$ in reverse. $C[A[j]]$ is correct position for $A[j]$. Decrement $C[A[j]]$ after placing to handle duplicates (lines 11-13, $\\Theta(n)$ time).\nOverall time: $\\Theta(n+k)$.\nCounting sort is **stable**: elements with the same value appear in the output array in the same order as in the input array. Stability is important, e.g., when used as subroutine in radix sort.\n\n## 8.3 Radix sort\nRadix sort is used by card-sorting machines. It sorts on the least significant digit first. It requires the digit sort to be stable.\nExample: Sorting 3-digit numbers. Sort by 1s digit, then 10s digit, then 100s digit, using a stable sort each time. Figure 8.3 of the book shows an example.\nRefer to [[PartII Sorting and Order Statistics Algorithms.md#C8.3 RADIX-SORT(A, n, d)]] for procedure `RADIX-SORT(A, n, d)` which sorts $n$ $d$-digit numbers.\n**Lemma 8.3**: Given $n$ $d$-digit numbers where each digit takes $k$ possible values, `RADIX-SORT` correctly sorts in $\\Theta(d(n+k))$ time if the stable sort takes $\\Theta(n+k)$ time (e.g., counting sort).\n*Proof*: Correctness by induction on digit sorted. Running time is $d$ passes of counting sort.\nFor $n$ $b$-bit numbers, we can view each key as $d = \\lceil b/r \\rceil$ digits of $r$ bits each. Each $r$-bit digit is an integer in $0 \\ldots 2^r-1$. So $k=2^r-1$. Counting sort takes $\\Theta(n+2^r)$ per pass.\n**Lemma 8.4**: `RADIX-SORT` on $n$ $b$-bit numbers using $r$-bit digits takes $\\Theta((b/r)(n+2^r))$ time.\n- If $b < \\lceil \\lg n \\rceil$, choosing $r=b$ gives $\\Theta(n)$ time.\n- If $b \\ge \\lceil \\lg n \\rceil$, choosing $r = \\lceil \\lg n \\rceil$ gives $\\Theta( (b/\\lg n)(n + 2^{\\lg n}) ) = \\Theta(bn/\\lg n)$ time.\nRadix sort is not always better than comparison sorts like quicksort due to constant factors and cache performance. It's generally not in-place if counting sort is used.\n\n## 8.4 Bucket sort\nBucket sort assumes input is drawn from a uniform distribution over $[0,1)$ and has an average-case running time of $O(n)$.\nIt divides $[0,1)$ into $n$ equal-sized subintervals (buckets). It distributes $n$ input numbers into buckets. Since inputs are uniform, few numbers are expected per bucket. It then sorts numbers in each bucket (e.g., with insertion sort) and concatenates sorted buckets. Figure 8.4 of the book shows an example. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C8.4 BUCKET-SORT(A, n)]].\nCorrectness: If $A[i] \\le A[j]$, then $\\lfloor n A[i] \\rfloor \\le \\lfloor n A[j] \\rfloor$. If they are in same bucket, insertion sort orders them. If different buckets, concatenation orders them.\nAnalysis:\n- Lines 1-5 and 8-9 take $O(n)$ time.\n- Line 7 (sorting buckets): Let $n_i$ be random variable for number of elements in bucket $B[i]$. Insertion sort is $O(n_i^2)$. Total time $T(n) = \\Theta(n) + \\sum_{i=0}^{n-1} O(n_i^2)$.\n- Average case: $E[T(n)] = \\Theta(n) + \\sum_{i=0}^{n-1} O(E[n_i^2])$.\n- For $n_i \\sim B(n, p=1/n)$, $E[n_i] = np = 1$, $Var[n_i] = np(1-p) = 1-1/n$. $E[n_i^2] = Var[n_i] + (E[n_i])^2 = (1-1/n) + 1^2 = 2-1/n$.\n- So $E[T(n)] = \\Theta(n) + n \\cdot O(2-1/n) = \\Theta(n)$.\nWorst-case time is $\\Theta(n^2)$ if all elements fall into one bucket. Can be improved to $O(n \\lg n)$ worst-case by using merge sort for buckets.\nBucket sort runs in linear time if sum of squares of bucket sizes is linear in total elements.\n\n## Problems\nChapter 8 problems include topics such as: Probabilistic lower bounds on comparison sorting, Sorting in place in linear time, Sorting variable-length items, Water jugs, Average sorting, Lower bound on merging sorted lists, The 0-1 sorting lemma and columnsort (includes [[PartII Sorting and Order Statistics Algorithms.md#C8.P8-7 COMPARE-EXCHANGE(A, i, j)]] and [[PartII Sorting and Order Statistics Algorithms.md#C8.P8-7 COMPARE-EXCHANGE-INSERTION-SORT(A, n)]]).\n\n## Chapter notes\nDecision-tree model by Ford and Johnson (1959). Counting sort credited to H. H. Seward (1954). Radix sort (LSD first) is a folk algorithm, first published by L. J. Comrie (1929). Bucket sort proposed by Isaac and Singleton (1956). Several advanced sorting algorithms exist for integers that run faster than $O(n \\lg n)$ under specific models.\n"
            },
            {
                "file": "PartII Sorting and Order Statistics/C9 Medians and Order Statistics.md",
                "contents": "# 9 Medians and Order Statistics\n## Introduction\nThe **$i$-th order statistic** of a set of $n$ elements is the $i$-th smallest element. The minimum is the 1st order statistic ($i=1$), maximum is the $n$-th order statistic ($i=n$). A **median** is informally the \"halfway point\". If $n$ is odd, unique median at $i=(n+1)/2$. If $n$ is even, two medians: lower median at $i=n/2$, upper median at $i=n/2+1$. We typically refer to the lower median as \"the median\", occurring at $i = \\lfloor (n+1)/2 \\rfloor$. This chapter addresses selecting the $i$-th order statistic from $n$ distinct numbers.\n**Selection Problem**:\n- **Input**: A set $A$ of $n$ distinct numbers and an integer $i$, $1 \\le i \\le n$.\n- **Output**: The element $x \\in A$ that is larger than exactly $i-1$ other elements of $A$.\nCan be solved in $O(n \\lg n)$ time by sorting. This chapter presents faster algorithms.\n\n## 9.1 Minimum and maximum\nTo find the minimum of $n$ elements, $n-1$ comparisons are necessary and sufficient. The `MINIMUM` procedure iterates through elements, keeping track of smallest seen. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C9.1 MINIMUM(A, n)]]. This is optimal: in a tournament model, every element except the winner must lose at least one match.\nTo find both minimum and maximum:\n- Naive approach: $n-1$ for min, $n-1$ for max, total $2n-2$ comparisons.\n- Better approach: Process elements in pairs. Compare pair elements ($A[k], A[k+1]$). Then compare smaller with current min, larger with current max. This is 3 comparisons for 2 elements. Total comparisons $\\approx 3n/2$. Specifically, at most $3 \\lfloor n/2 \\rfloor$ comparisons.\n    - If $n$ is odd: Initialize min/max to $A[1]$. Process remaining $n-1$ elements in $(n-1)/2$ pairs. Total $3(n-1)/2 = 3\\lfloor n/2 \\rfloor$ comparisons.\n    - If $n$ is even: Compare $A[1], A[2]$ to initialize min/max (1 comp). Process remaining $n-2$ elements in $(n-2)/2$ pairs. Total $1 + 3(n-2)/2 = 3n/2 - 2$ comparisons.\n\n## 9.2 Selection in expected linear time\nThe `RANDOMIZED-SELECT` algorithm finds the $i$-th smallest element. It's modeled after quicksort but recursively processes only one side of the partition. Expected running time is $O(n)$. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C9.2 RANDOMIZED-SELECT(A, p, r, i)]].\nAlgorithm steps:\n1. Base case: If $p=r$, return $A[p]$.\n2. Partition: Call `RANDOMIZED-PARTITION(A, p, r)` to partition $A[p \\ldots r]$ around pivot $A[q]$.\n3. Let $k = q-p+1$ be rank of pivot $A[q]$ within $A[p \\ldots r]$.\n4. Check pivot: If $i=k$, $A[q]$ is the answer.\n5. Recurse: If $i < k$, recursively call `RANDOMIZED-SELECT` on $A[p \\ldots q-1]$ to find $i$-th smallest. If $i > k$, recursively call `RANDOMIZED-SELECT` on $A[q+1 \\ldots r]$ to find $(i-k)$-th smallest. Figure 9.1 of the book illustrates its action.\nWorst-case running time is $\\Theta(n^2)$ (e.g., if always unlucky with pivot choice). Expected running time $O(n)$ because pivot choice is random, leading to balanced partitions on average. If pivot lands in \"middle half\" (between 1st and 3rd quartiles), at least 1/4 of elements are eliminated. This happens with probability $\\approx 1/2$.\n**Lemma 9.1**: A partitioning is helpful (reduces problem size by at least factor $3/4$) with probability at least $1/2$.\n**Theorem 9.2**: The procedure `RANDOMIZED-SELECT` on an input array of $n$ distinct elements has an expected running time of $\\Theta(n)$.\n*Proof sketch*: Define $A^{(j)}$ as elements in play after $j$ partitionings. A partitioning $h_k$ is helpful if $|A^{(h_k)}| \\le (3/4)|A^{(h_{k-1})}|$. Let $X_k$ be number of partitionings in generation $k$ (between $h_k$ and $h_{k+1}-1$). $E[X_k] \\le 2$. Total comparisons are bounded by $\\sum X_k |A^{(h_k)}|$. Expected total comparisons $\\sum E[X_k] |A^{(h_k)}| \\le \\sum 2 \\cdot (3/4)^k n_0 = O(n_0)$. Since initial scan by `PARTITION` is $\\Omega(n)$, total expected time is $\\Theta(n)$.\n\n## 9.3 Selection in worst-case linear time\nThe `SELECT` algorithm finds the $i$-th smallest element in $O(n)$ worst-case time. It guarantees a good pivot choice. Refer to [[PartII Sorting and Order Statistics Algorithms.md#C9.3 SELECT(A, p, r, i)]].\nAlgorithm steps for `SELECT(A, p, r, i)`:\n1. Handle small $n$: If $n=(r-p+1)$ is not divisible by 5, the `while` loop (lines 1-8 in pseudocode, which is slightly different from book text for SELECT itself) effectively reduces $n$ by finding and removing the minimum element if $i>1$, or returning it if $i=1$, until $n \bmod 5 = 0$. This takes $O(n)$ total for these initial steps.\nCore steps for $n$ elements (assume $n \bmod 5 = 0$ for this summary from book page 237, OCR page 82 lines 11-24):\n   a. Divide $n$ elements into $g=n/5$ groups of 5 elements each.\n   b. Find median of each 5-element group (e.g., by sorting each group, takes $O(1)$ per group, total $O(n)$ for all groups).\n   c. Recursively call `SELECT` to find median $x$ of the $g$ group medians. This is the pivot.\n   d. Partition array $A[p \\ldots r]$ around $x$ using a modified `PARTITION` (call it `PARTITION-AROUND`). Let $q$ be index of $x$.\n   e. Let $k=q-p+1$ be rank of pivot $x$. If $i=k$, $x$ is the answer.\n   f. If $i < k$, recursively call `SELECT` on $A[p \\ldots q-1]$ for $i$-th smallest.\n   g. If $i > k$, recursively call `SELECT` on $A[q+1 \\ldots r]$ for $(i-k)$-th smallest.\nFigure 9.3 of the book illustrates element relationships after partitioning around $x$.\nAnalysis:\n- Steps a, b, d take $O(n)$ time.\n- Step c (recursive call for median of medians): $T(n/5)$ time since there are $n/5$ medians.\n- Step e, f, or g (recursive call for selection): Pivot $x$ is median of $g=n/5$ medians. At least half the group medians are $\\le x$. For each such median, 2 more elements in its group are $\\le x$. So at least $3 \\cdot ( (n/5)/2 ) = 3n/10$ elements are $\\le x$. Similarly, at least $3n/10$ elements are $\\ge x$. This means that in the worst case, the recursive call in step f or g is on at most $n - 3n/10 = 7n/10$ elements.\n- Recurrence relation: $T(n) \\le T(n/5) + T(7n/10) + O(n)$.\n**Theorem 9.3**: The running time of `SELECT` on an input of $n$ elements is $\\Theta(n)$.\n*Proof*: Using substitution method for $T(n) \\le cn$: $cn/5 + c(7n/10) + O(n) = c(9n/10) + O(n) = cn - cn/10 + O(n)$. This is $\\le cn$ if $c$ is large enough to dominate $O(n)$ term. Base cases ($n \\le 4$ for $n \bmod 5$ loop, or small $n$ for main recursion) can be handled by choosing $c$ appropriately. Since some steps (like partitioning) take $\\Omega(n)$, overall time is $\\Theta(n)$.\n\n## Problems\nChapter 9 problems include topics such as: Largest $i$ numbers in sorted order, Variant of randomized selection (see [[PartII Sorting and Order Statistics Algorithms.md#C9.P9-2 SIMPLER-RANDOMIZED-SELECT(A, p, r, i)]]), Weighted median, Small order statistics, Alternative analysis of randomized selection, Select with groups of 3 (uses [[PartII Sorting and Order Statistics Algorithms.md#C9.Notes SELECT3(A, p, r, i)]]).\n\n## Chapter notes\nWorst-case linear-time median-finding algorithm by Blum, Floyd, Pratt, Rivest, and Tarjan (1973). Randomized version by Hoare (1961). Floyd and Rivest (1975) developed an improved randomized version. Exact comparisons for median is an open research area with bounds like $2n$ (lower) and $2.95n$ (upper).\n"
            }
        ]
    }
]