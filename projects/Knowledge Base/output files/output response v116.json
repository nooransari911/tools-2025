[
    {
        "items": [
            {
                "file": "foundations overview.md",
                "contents": "# Foundations Overview\n\nThis overview covers foundational concepts for designing and analyzing algorithms, primarily drawing from Chapters 1, 2, and 3.\n\n## 1. Core Concepts\n\n### 1.1 Problems and Algorithms\nA *problem* statement defines a relationship between input and output, possibly with additional constraints. An *instance* of a problem is any input that satisfies the constraints on input as defined in the problem.\n\nAn *algorithm* is any well-defined computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output in a finite amount of time. It is a sequence of computational steps that transform input into output. An algorithm is a tool for solving a well-specified computational problem. An algorithm is *correct* if, for every problem instance provided as input, it halts and outputs the correct solution.\n\n### 1.2 Data Structures\nA *data structure* is a way to store and organize data to facilitate access and modifications. Choosing appropriate data structures is an important part of algorithm design, as no single data structure works well for all purposes.\n\n## 2. Algorithmic Techniques\n\n### 2.1 Incremental Method\nThis method processes input elements one by one, maintaining a solution for the elements processed so far. Insertion sort is an example, where for each element A[i], it is inserted into its proper place in the already sorted subarray A[1:i-1].\n\n### 2.2 Divide-and-Conquer\nThis method involves three characteristic steps for a problem of a certain size:\n1.  **Divide**: If the problem is not a base case (small enough to be solved directly), divide it into one or more subproblems that are smaller instances of the same problem.\n2.  **Conquer**: Solve the subproblems by recursively applying the divide-and-conquer method.\n3.  **Combine**: Combine the solutions to the subproblems to form a solution to the original problem.\nMerge sort is an example of a divide-and-conquer algorithm.\n\n## 3. Analyzing Algorithms\n\n### 3.1 Model of Computation\nAnalysis typically uses a generic one-processor, *random-access machine (RAM)* model. In this model, instructions execute sequentially, and each instruction (arithmetic, data movement, control) and data access takes a constant amount of time. This model simplifies analysis by abstracting away from specific hardware details.\n\n### 3.2 Input Size and Running Time\nThe *input size* depends on the problem. For sorting, it's often the number of items $n$. For multiplying integers, it might be the total number of bits.\nThe *running time* of an algorithm on a particular input is the number of primitive operations or steps executed. It is typically expressed as a function of the input size.\n\n### 3.3 Loop Invariants\nA *loop invariant* is a property used to show algorithm correctness, particularly for iterative algorithms. It requires demonstrating three properties:\n1.  **Initialization**: The invariant is true prior to the first iteration of the loop.\n2.  **Maintenance**: If the invariant is true before an iteration of the loop, it remains true before the next iteration.\n3.  **Termination**: When the loop terminates, the invariant, along with the termination condition, gives a useful property that helps show the algorithm is correct.\n\n### 3.4 Worst-Case and Average-Case Analysis\n*Worst-case running time* is the longest running time for any input of a given size $n$. It provides an upper bound and is often focused on because:\n- It guarantees performance.\n- For some algorithms, the worst case occurs frequently.\n- The average case is often as bad as the worst case.\n*Average-case running time* is the expected time over all inputs of size $n$, assuming a certain probability distribution for inputs. *Probabilistic analysis* can be used for this, or for analyzing *randomized algorithms* where randomness is introduced by the algorithm itself.\n\n### 3.5 Order of Growth\nThis refers to the rate at which the running time increases with input size. For large inputs, the highest-order term in the running time formula dominates. Constant factors and lower-order terms are usually ignored in asymptotic analysis, as the order of growth is the primary factor determining efficiency for large inputs.\n\n## 4. Asymptotic Notation\nAsymptotic notation describes the limiting behavior of functions, typically running times, as the input size $n$ grows large.\n\n-   **$\\\\Theta$-notation (Theta)**: Asymptotic tight bound.\n    $\\\\Theta(g(n)) = \\\\{f(n): \\\\text{there exist positive constants } c_1, c_2, \\\\text{ and } n_0 \\\\text{ such that } 0 \\\\le c_1g(n) \\\\le f(n) \\\\le c_2g(n) \\\\text{ for all } n \\\\ge n_0\\\\}$.\n\n-   **O-notation (Big-O)**: Asymptotic upper bound.\n    $O(g(n)) = \\\\{f(n): \\\\text{there exist positive constants } c \\\\text{ and } n_0 \\\\text{ such that } 0 \\\\le f(n) \\\\le cg(n) \\\\text{ for all } n \\\\ge n_0\\\\}$.\n\n-   **$\\\\Omega$-notation (Big-Omega)**: Asymptotic lower bound.\n    $\\\\Omega(g(n)) = \\\\{f(n): \\\\text{there exist positive constants } c \\\\text{ and } n_0 \\\\text{ such that } 0 \\\\le cg(n) \\\\le f(n) \\\\text{ for all } n \\\\ge n_0\\\\}$.\n\n-   **o-notation (Little-o)**: Upper bound that is not asymptotically tight.\n    $o(g(n)) = \\\\{f(n): \\\\text{for any positive constant } c > 0, \\\\text{ there exists a constant } n_0 > 0 \\\\text{ such that } 0 \\\\le f(n) < cg(n) \\\\text{ for all } n \\\\ge n_0\\\\}$.\n    This implies $\\\\lim_{n\\\\to\\\\infty} f(n)/g(n) = 0$.\n\n-   **$\\\\omega$-notation (Little-omega)**: Lower bound that is not asymptotically tight.\n    $\\\\omega(g(n)) = \\\\{f(n): \\\\text{for any positive constant } c > 0, \\\\text{ there exists a constant } n_0 > 0 \\\\text{ such that } 0 \\\\le cg(n) < f(n) \\\\text{ for all } n \\\\ge n_0\\\\}$.\n    This implies $\\\\lim_{n\\\\to\\\\infty} f(n)/g(n) = \\\\infty$.\n\nTheorem: For any two functions $f(n)$ and $g(n)$, $f(n) = \\\\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\\\Omega(g(n))$.\n\n## 5. Recurrence Relations\nRecurrence relations describe the running time of recursive algorithms. For a divide-and-conquer algorithm, if $T(n)$ is the running time on a problem of size $n$:\n- $D(n)$ is the time to divide the problem.\n- $a$ is the number of subproblems.\n- $n/b$ is the size of each subproblem.\n- $aT(n/b)$ is the time to solve all subproblems.\n- $C(n)$ is the time to combine solutions.\nThe general form is $T(n) = D(n) + aT(n/b) + C(n)$.\nFor Merge Sort, $D(n) = \\\\Theta(1)$, $a=2$, $b=2$, $C(n) = \\\\Theta(n)$, leading to the recurrence:\n$T(n) = 2T(n/2) + \\\\Theta(n)$, which solves to $T(n) = \\\\Theta(n \\\\lg n)$.\nBase cases for recurrences are usually $T(n) = \\\\Theta(1)$ for small $n$ (e.g., $n < n_0$)."
            },
            {
                "file": "PartI Foundations Algorithms.md",
                "contents": "# Part I Foundations Algorithms\n\n## C2 INSERTION-SORT\n```\nINSERTION-SORT(A, n)\n  1 for i = 2 to n\n  2     key = A[i]\n  3     // Insert A[i] into the sorted subarray A[1:i-1].\n  4     j = i - 1\n  5     while j > 0 and A[j] > key\n  6         A[j+1] = A[j]\n  7         j = j - 1\n  8     A[j+1] = key\n```\n\n## C2 MERGE\n```\nMERGE(A, p, q, r)\n  1  n_L = q - p + 1\n  2  n_R = r - q\n  3  let L[0 : n_L-1] and R[0 : n_R-1] be new arrays\n  4  for i = 0 to n_L - 1\n  5      L[i] = A[p+i]\n  6  for j = 0 to n_R - 1\n  7      R[j] = A[q+j+1]\n  8  i = 0\n  9  j = 0\n 10  k = p\n 11  // As long as each of the arrays L and R contains an unmerged element,\n 12  // copy the smallest unmerged element back into A[p:r].\n 13  while i < n_L and j < n_R\n 14      if L[i] <= R[j]\n 15          A[k] = L[i]\n 16          i = i + 1\n 17      else A[k] = R[j]\n 18          j = j + 1\n 19      k = k + 1\n 20  // Having gone through one of L and R entirely, copy the\n 21  // remainder of the other to the end of A[p:r].\n 22  while i < n_L\n 23      A[k] = L[i]\n 24      i = i + 1\n 25      k = k + 1\n 26  while j < n_R\n 27      A[k] = R[j]\n 28      j = j + 1\n 29      k = k + 1\n```\n\n## C2 MERGE-SORT\n```\nMERGE-SORT(A, p, r)\n  1 if p < r\n  2     q = floor((p+r)/2)\n  3     MERGE-SORT(A, p, q)\n  4     MERGE-SORT(A, q+1, r)\n  5     MERGE(A, p, q, r)\n```"
            },
            {
                "file": "PartI Foundations/C1 The Role of Algorithms in Computing.md",
                "contents": "# 1 The Role of Algorithms in Computing\n\nThis chapter introduces what algorithms are, why their study is important, and their role relative to other computer technologies.\n\n## 1.1 Algorithms\nInformally, an *algorithm* is any well-defined computational procedure that takes some value, or set of values, as *input* and produces some value, or set of values, as *output* in a finite amount of time. An algorithm is thus a sequence of computational steps that transform the input into the output.\nAn algorithm can also be viewed as a tool for solving a well-specified *computational problem*. The problem statement specifies the desired input/output relationship. An algorithm describes a specific procedure for achieving this relationship.\n\nFor example, the *sorting problem* is:\nInput: A sequence of $n$ numbers $\\\\(a_1, a_2, ..., a_n\\\\)$.\nOutput: A permutation (reordering) $\\\\(a'_1, a'_2, ..., a'_n\\\\)$ of the input sequence such that $a'_1 \\\\le a'_2 \\\\le \\\\dots \\\\le a'_n$.\nAn *instance* of a problem consists of the input (satisfying constraints) needed to compute a solution.\n\nAn algorithm is *correct* if for every input instance, it halts with the correct output. A correct algorithm *solves* the given computational problem. Incorrect algorithms might not halt or might halt with an incorrect answer; they can sometimes be useful if their error rate is controllable.\nAlgorithms can be specified in English, as a computer program, or as a hardware design. The specification must be precise.\n\n### What kinds of problems are solved by algorithms?\nAlgorithms are used in diverse applications:\n-   The Human Genome Project uses algorithms for identifying genes, sequencing DNA, storing information, and data analysis (e.g., dynamic programming for sequence similarity).\n-   The internet relies on algorithms for routing data (e.g., shortest path algorithms) and for search engines to find information (e.g., indexing and ranking techniques).\n-   Electronic commerce depends on algorithms for privacy and security, such as public-key cryptography and digital signatures, which are based on numerical algorithms and number theory.\n-   Manufacturing and commercial enterprises use algorithms for resource allocation (e.g., linear programming for maximizing profit or minimizing cost).\n\nSpecific problems with algorithmic solutions include:\n-   Finding the shortest path in a road map (modeled as a graph problem).\n-   Topological sorting for ordering tasks with dependencies in a mechanical design.\n-   Clustering algorithms to categorize medical images (e.g., cancerous vs. benign tumors).\n-   Data compression techniques like Huffman coding.\n\nMany algorithmic problems share two characteristics:\n1.  They have many candidate solutions, making it challenging to find an optimal one without exhaustive search.\n2.  They have practical applications.\n\n### Data structures\nA *data structure* is a way to store and organize data to facilitate access and modifications. Appropriate data structure choice is crucial for algorithm design, as different structures suit different purposes.\n\n### Technique\nThis book teaches techniques of algorithm design and analysis, enabling development of new algorithms, proving their correctness, and analyzing their efficiency.\n\n### Hard problems\nSome problems have no known efficient algorithms. *NP-complete* problems are a class of such problems. No efficient algorithm has been found for any NP-complete problem, yet it hasn't been proven that one cannot exist. If an efficient algorithm exists for one NP-complete problem, then efficient algorithms exist for all of them. Understanding NP-completeness helps identify problems where seeking an exact, efficient solution might be futile, guiding efforts towards *approximation algorithms* that find good, but not necessarily optimal, solutions. The traveling-salesperson problem is an example.\n\n### Alternative computing models\nTraditional analysis often assumes sequential processing. However, modern processors are often multicore, requiring algorithms designed for *parallel computers*. Chapter 26 discusses task-parallel algorithms. *Online algorithms* process input that arrives over time, making decisions without full knowledge of future input (e.g., in data centers, internet routing, emergency room triage).\n\n## 1.2 Algorithms as a technology\nAlgorithms are a crucial technology, as important as fast hardware. Efficient algorithms can lead to more significant performance gains than hardware improvements, especially for large input sizes.\n\n### Efficiency\nDifferent algorithms for the same problem can have vastly different efficiencies. For example, Chapter 2 introduces insertion sort (roughly $c_1n^2$ time) and merge sort (roughly $c_2n \\\\lg n$ time).\nEven if computer A (running insertion sort) is much faster (e.g., $10^{10}$ instructions/sec) than computer B (running merge sort, $10^7$ instructions/sec), and insertion sort is expertly coded ($2n^2$ instructions) while merge sort is poorly coded ($50n \\\\lg n$ instructions), for a large input like $10^7$ numbers:\n- Computer A: $2 \\\\cdot (10^7)^2 / 10^{10} = 20,000$ seconds (over 5.5 hours).\n- Computer B: $50 \\\\cdot 10^7 \\\\lg 10^7 / 10^7 \\\\approx 50 \\\\cdot 10^7 \\\\cdot (7 \\\\lg 10) / 10^7 \\\\approx 50 \\\\cdot 23.25 \\\\approx 1163$ seconds (under 20 minutes).\nComputer B with the asymptotically more efficient algorithm is significantly faster. The advantage of merge sort's $O(n \\\\lg n)$ over insertion sort's $O(n^2)$ becomes more pronounced as problem size increases.\n\n### Algorithms and other technologies\nAlgorithms are fundamental even in systems that also rely on advanced hardware, GUIs, networking, etc. For instance, a web service for travel directions uses algorithms for route finding and map rendering. Even applications without explicit algorithmic content at the application level rely on algorithms in the underlying hardware design, GUI implementation, network routing, compilers, and interpreters. Machine learning itself is a collection of algorithms. Data science heavily uses algorithmic techniques."
            },
            {
                "file": "PartI Foundations/C2 Getting Started.md",
                "contents": "# 2 Getting Started\n\nThis chapter introduces a framework for thinking about algorithm design and analysis, using sorting as an initial example. It covers insertion sort and merge sort, their analysis, and the divide-and-conquer technique.\n\n## 2.1 Insertion sort\nInsertion sort is an efficient algorithm for sorting a small number of elements. It solves the sorting problem:\nInput: A sequence of $n$ numbers (keys) $\\\\(a_1, a_2, ..., a_n\\\\)$.\nOutput: A permutation $\\\\(a'_1, a'_2, ..., a'_n\\\\)$ such that $a'_1 \\\\le a'_2 \\\\le \\\\dots \\\\le a'_n$.\nKeys are often associated with *satellite data*, which form a *record* along with the key. Sorting moves records with their keys.\n\nAlgorithms are typically described in pseudocode, similar to C, C++, Java, Python, or JavaScript, but emphasizing clarity and conciseness over software engineering details like data abstraction or error handling.\nInsertion sort works like sorting a hand of playing cards: start with an empty left hand and cards face down on a table. Pick up cards one by one from the table and insert them into the correct position in the left hand. To find the correct position, compare the new card with cards already in hand, from right to left.\n\nThe pseudocode for insertion sort is [[PartI Foundations Algorithms.md#C2 INSERTION-SORT]]. It sorts an array $A[1..n]$ in place.\n\n### Loop invariants and the correctness of insertion sort\nA *loop invariant* is a property that holds before each iteration of a loop. For the `for` loop (lines 1-8) of `INSERTION-SORT`, the loop invariant is:\n*At the start of each iteration of the `for` loop, the subarray $A[1..i-1]$ consists of the elements originally in $A[1..i-1]$, but in sorted order.*\n\nTo prove correctness using a loop invariant, we show three properties:\n1.  **Initialization**: The invariant is true prior to the first loop iteration. Before the first iteration, $i=2$. The subarray $A[1..i-1]$ is $A[1]$, which contains the single original element $A[1]$. A one-element array is trivially sorted. So, the invariant holds.\n2.  **Maintenance**: If the invariant is true before an iteration, it remains true before the next. The `for` loop body (lines 2-8) works by moving elements $A[i-1], A[i-2], \\\\dots$ one position to the right until the correct position for $A[i]$ (stored in `key`) is found, and then `key` is inserted. This ensures that $A[1..i]$ now contains the original elements of $A[1..i]$ in sorted order. Incrementing $i$ for the next iteration then makes $A[1..i-1]$ (using the new $i$) the sorted subarray $A[1..\\text{old } i]$, maintaining the invariant.\n3.  **Termination**: When the loop terminates, the invariant provides a useful property that helps show the algorithm is correct. The `for` loop terminates when $i > n$, i.e., when $i = n+1$. Substituting $n+1$ for $i$ in the invariant, we find that the subarray $A[1..n]$ consists of the elements originally in $A[1..n]$, but in sorted order. This means the entire array is sorted.\n\n### Pseudocode conventions\n-   Indentation indicates block structure.\n-   Looping constructs (`while`, `for`, `repeat-until`) and conditional constructs (`if-else`) have standard interpretations. Loop counters retain their value after exiting a `for` loop (e.g., $i=n+1$ after the loop in `INSERTION-SORT`). `to` implies incrementing, `downto` implies decrementing.\n-   `//` indicates a comment.\n-   Variables are local to the procedure unless specified.\n-   Array elements are accessed by `A[i]`. Subarrays by `A[i..j]`. 1-origin indexing is common, but bounds will be specified if ambiguous.\n-   Objects have attributes accessed by `object.attribute`.\n-   Parameters are passed by value. For objects/arrays, the pointer/reference is passed by value, so modifications to object attributes or array elements are visible to the caller.\n-   `return` transfers control and optionally a value. Multiple values can be returned.\n-   Boolean operators `and` and `or` are short-circuiting.\n-   `error` indicates a pre-condition violation and immediate termination of the procedure.\n\n## 2.2 Analyzing algorithms\nAnalyzing an algorithm means predicting the resources it requires, typically computational time. We use the *random-access machine (RAM)* model: a generic single-processor model where instructions execute sequentially. Each instruction (arithmetic, data movement, control) takes constant time. Memory access also takes constant time. Data types are integers, floating-point numbers, and characters. Word size is assumed to accommodate input values and indices (e.g., $c \\\\lg n$ bits for input size $n$).\n\n### Analysis of insertion sort\nLet $c_k$ be the cost of line $k$. Let $t_i$ be the number of times the `while` loop test in line 5 is executed for a given $i$. The running time $T(n)$ is the sum of (cost $\\\\times$ times) for each line.\n\n`INSERTION-SORT(A, n)`\n| Line | Cost | Times executed                                    |\n|------|------|---------------------------------------------------|\n| 1    | $c_1$  | $n$                                               |\n| 2    | $c_2$  | $n-1$                                             |\n| 3    | 0    | $n-1$                                             |\n| 4    | $c_4$  | $n-1$                                             |\n| 5    | $c_5$  | $\\\\\\\\sum_{i=2}^{n} t_i$                               |\n| 6    | $c_6$  | $\\\\\\\\sum_{i=2}^{n} (t_i - 1)$                         |\n| 7    | $c_7$  | $\\\\\\\\sum_{i=2}^{n} (t_i - 1)$                         |\n| 8    | $c_8$  | $n-1$                                             |\n\n$T(n) = c_1n + c_2(n-1) + c_4(n-1) + c_5\\\\sum_{i=2}^{n} t_i + c_6\\\\sum_{i=2}^{n} (t_i-1) + c_7\\\\sum_{i=2}^{n} (t_i-1) + c_8(n-1)$.\n\n**Best case**: Array is already sorted. $t_i=1$ for all $i=2, \\\\dots, n$. Line 5 test fails immediately.\n$T(n) = (c_1+c_2+c_4+c_5+c_8)n - (c_2+c_4+c_5+c_8)$. This is a linear function of $n$, so $T(n) = \\\\Theta(n)$.\n\n**Worst case**: Array is reverse sorted. `key` must be compared with all elements in $A[1..i-1]$. So $t_i=i$ for $i=2, \\\\dots, n$.\n$\\\\\\\\sum_{i=2}^{n} i = n(n+1)/2 - 1$.\n$\\\\\\\\sum_{i=2}^{n} (i-1) = n(n-1)/2$.\n$T(n) = (c_5/2+c_6/2+c_7/2)n^2 + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n - (c_2+c_4+c_5+c_8)$.\nThis is a quadratic function of $n$, so $T(n) = \\\\Theta(n^2)$.\n\n### Worst-case and average-case analysis\nWe usually concentrate on the *worst-case running time*: the longest time for any input of size $n$. Reasons:\n1.  It's an upper bound on running time.\n2.  For some algorithms, the worst case occurs often.\n3.  The average case is often roughly as bad as the worst case (e.g., for insertion sort, average time is also $\\\\Theta(n^2)$).\n*Average-case analysis* requires assumptions about the statistical distribution of inputs.\n\n### Order of growth\nWe are interested in the *rate of growth* or *order of growth* of the running time. We consider only the leading term (e.g., $an^2$ from $an^2+bn+c$) and ignore its constant coefficient, as for large $n$, these are less significant than the rate of growth itself. For insertion sort, the worst-case running time is $\\\\Theta(n^2)$.\nWe generally consider an algorithm with a lower order of growth in its worst-case running time to be more efficient.\n\n## 2.3 Designing algorithms\n\n### 2.3.1 The divide-and-conquer method\nMany algorithms are recursive, often following the divide-and-conquer approach:\n1.  **Divide**: Divide the problem into several subproblems that are smaller instances of the original problem.\n2.  **Conquer**: Solve the subproblems recursively. If subproblems are small enough (base case), solve them directly.\n3.  **Combine**: Combine the solutions to the subproblems into the solution for the original problem.\n\n*Merge sort* is a divide-and-conquer algorithm for sorting A[p..r]:\n1.  **Divide**: If $p < r$, find the midpoint $q = \\\\lfloor(p+r)/2\\\\rfloor$ to divide A[p..r] into A[p..q] and A[q+1..r].\n2.  **Conquer**: Recursively sort the two subarrays A[p..q] and A[q+1..r] using merge sort.\n3.  **Combine**: Merge the two sorted subarrays A[p..q] and A[q+1..r] to produce a single sorted subarray A[p..r]. The `MERGE(A, p, q, r)` procedure performs this step. It assumes A[p..q] and A[q+1..r] are already sorted.\n\nThe base case is when $p \\\\ge r$, meaning the subarray has at most 1 element, which is trivially sorted.\n\nPseudocode for `MERGE` is [[PartI Foundations Algorithms.md#C2 MERGE]]. It takes $\\\\Theta(n)$ time to merge $n$ elements.\nPseudocode for `MERGE-SORT` is [[PartI Foundations Algorithms.md#C2 MERGE-SORT]].\n\n### 2.3.2 Analyzing divide-and-conquer algorithms\nRunning time is often described by a *recurrence equation* or *recurrence*. Let $T(n)$ be the worst-case running time for an input of size $n$.\n-   If $n$ is small (e.g., $n=1$), $T(n) = \\\\Theta(1)$ (constant time).\n-   For merge sort on $n$ elements:\n    -   **Divide**: Computing $q$ takes $\\\\Theta(1)$ time.\n    -   **Conquer**: Recursively solving two subproblems of size $n/2$ (ignoring floors/ceilings for simplicity) takes $2T(n/2)$ time.\n    -   **Combine**: Merging $n$ elements takes $\\\\Theta(n)$ time using `MERGE`.\nSo, $T(n) = 2T(n/2) + \\\\Theta(n)$.\nThe $\\\\Theta(1)$ for division is absorbed into $\\\\Theta(n)$ for combining. This recurrence solves to $T(n) = \\\\Theta(n \\\\lg n)$.\nA recursion tree (Figure 2.5 in CLRS) can visualize this. The tree has $\\\\lg n + 1$ levels. Each level sums to $cn$ (for the non-leaf levels) or $c_1n$ (for the leaf level, if $T(1)=c_1$). Total cost is $cn \\\\lg n + c_1n = \\\\Theta(n \\\\lg n)$.\nMerge sort's $\\\\Theta(n \\\\lg n)$ worst-case time is asymptotically better than insertion sort's $\\\\Theta(n^2)$."
            },
            {
                "file": "PartI Foundations/C3 Characterizing Running Times.md",
                "contents": "# 3 Characterizing Running Times\n\nThis chapter focuses on methods for simplifying the asymptotic analysis of algorithms, formalizing the notation for order of growth.\n\n## 3.1 O-notation, $\\\\Omega$-notation, and $\\\\Theta$-notation\nWhen analyzing insertion sort's worst-case running time (Chapter 2), we started with a complex expression: $(c_5/2+c_6/2+c_7/2)n^2 + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n - (c_2+c_4+c_5+c_8)$.\nWe simplified this by discarding lower-order terms and the leading term's coefficient, resulting in $\\\\Theta(n^2)$. This style characterizes running times by focusing on the rate of growth.\n\n### O-notation (Big-O)\n$O$-notation characterizes an *asymptotic upper bound*. It states that a function grows no faster than a certain rate. For $f(n) = 7n^3 + 100n^2 - 20n + 6$, its rate of growth is $n^3$. So, $f(n) = O(n^3)$. It's also $O(n^4)$, $O(n^5)$, etc., as it grows no faster than these rates.\n\n### $\\\\Omega$-notation (Big-Omega)\n$\\\\Omega$-notation characterizes an *asymptotic lower bound*. It states that a function grows at least as fast as a certain rate. For $f(n) = 7n^3 + 100n^2 - 20n + 6$, $f(n) = \\\\Omega(n^3)$. It's also $\\\\Omega(n^2)$, $\\\\Omega(n)$, etc.\n\n### $\\\\Theta$-notation (Theta)\n$\\\\Theta$-notation characterizes an *asymptotic tight bound*. It states that a function grows precisely at a certain rate (within constant factors). If $f(n)$ is $O(g(n))$ and $f(n)$ is $\\\\Omega(g(n))$, then $f(n) = \\\\Theta(g(n))$. For $f(n) = 7n^3 + 100n^2 - 20n + 6$, $f(n) = \\\\Theta(n^3)$.\n\n### Example: Insertion sort\nRevisiting [[PartI Foundations Algorithms.md#C2 INSERTION-SORT]]:\nThe outer `for` loop runs $n-1$ times. The inner `while` loop's iterations depend on the input. In any case, the `while` loop iterates at most $i-1$ times for each $i$. Since $i \\\\le n$, the total `while` loop iterations are less than $n \\\\cdot (n-1) < n^2$. Each `while` loop iteration takes constant time. So, the total time is $O(n^2)$ for any input.\nFor the worst case, consider an input where the first $n/3$ elements are the largest $n/3$ values (assuming $n$ is a multiple of 3). Each of these $n/3$ values must move past the middle $n/3$ positions to reach the final $n/3$ positions. Moving one value past $n/3$ positions takes $\\\\Omega(n/3)$ time. Since there are $n/3$ such values, the total time is at least $(n/3) \\\\cdot \\\\Omega(n/3) = \\\\Omega(n^2/9) = \\\\Omega(n^2)$.\nSince insertion sort's worst-case running time is $O(n^2)$ and $\\\\Omega(n^2)$, it is $\\\\Theta(n^2)$.\nThis does not mean insertion sort is $\\\\Theta(n^2)$ in all cases; its best-case running time is $\\\\Theta(n)$.\n\n## 3.2 Asymptotic notation: formal definitions\nAsymptotic notations are defined in terms of functions whose domains are typically natural numbers $\\\\mathbb{N}$ or real numbers $\\\\mathbb{R}$.\n\n### $O$-notation\n$O(g(n)) = \\\\{ f(n) : \\\\text{there exist positive constants } c \\\\text{ and } n_0 \\\\text{ such that } 0 \\\\le f(n) \\\\le cg(n) \\\\text{ for all } n \\\\ge n_0 \\\\}$.\n$f(n)$ must be asymptotically nonnegative. $g(n)$ must also be asymptotically nonnegative.\nExample: $4n^2 + 100n + 500 = O(n^2)$. Let $c=5.05, n_0=100$. Then for $n \\\\ge 100$, $4 + 100/n + 500/n^2 \\\\le 4 + 1 + 0.05 = 5.05$. So $4n^2 + 100n + 500 \\\\le 5.05n^2$.\n\n### $\\\\Omega$-notation\n$\\\\Omega(g(n)) = \\\\{ f(n) : \\\\text{there exist positive constants } c \\\\text{ and } n_0 \\\\text{ such that } 0 \\\\le cg(n) \\\\le f(n) \\\\text{ for all } n \\\\ge n_0 \\\\}$.\nExample: $4n^2 + 100n + 500 = \\\\Omega(n^2)$. Let $c=4, n_0=1$. For $n \\\\ge 1$, $4 + 100/n + 500/n^2 \\\\ge 4$. So $4n^2 + 100n + 500 \\\\ge 4n^2$.\n\n### $\\\\Theta$-notation\n$\\\\Theta(g(n)) = \\\\{ f(n) : \\\\text{there exist positive constants } c_1, c_2, \\\\text{ and } n_0 \\\\text{ such that } 0 \\\\le c_1g(n) \\\\le f(n) \\\\le c_2g(n) \\\\text{ for all } n \\\\ge n_0 \\\\}$.\n\n**Theorem 3.1**: For any two functions $f(n)$ and $g(n)$, $f(n) = \\\\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\\\Omega(g(n))$.\n\n### Asymptotic notation in equations and inequalities\n$f(n) = O(g(n))$ means $f(n) \\\\in O(g(n))$.\nWhen asymptotic notation appears in a formula, e.g., $2n^2 + 3n + 1 = 2n^2 + \\\\Theta(n)$, it means $2n^2 + 3n + 1 = 2n^2 + f(n)$ for some $f(n) \\\\in \\\\Theta(n)$. Here, $f(n) = 3n+1$.\nFor $2n^2 + \\\\Theta(n) = \\\\Theta(n^2)$: for any $f(n) \\\\in \\\\Theta(n)$, there is some $g(n) \\\\in \\\\Theta(n^2)$ such that $2n^2 + f(n) = g(n)$.\n\n### $o$-notation (little-o)\n$o(g(n)) = \\\\{ f(n) : \\\\text{for any positive constant } c > 0, \\\\text{ there exists a constant } n_0 > 0 \\\\text{ such that } 0 \\\\le f(n) < cg(n) \\\\text{ for all } n \\\\ge n_0 \\\\}$.\nThis means $f(n)$ becomes insignificant relative to $g(n)$ as $n$ grows large: $\\\\lim_{n \\\\to \\\\infty} f(n)/g(n) = 0$.\nExample: $2n = o(n^2)$, but $2n^2 \\\\ne o(n^2)$.\n\n### $\\\\omega$-notation (little-omega)\n$\\\\omega(g(n)) = \\\\{ f(n) : \\\\text{for any positive constant } c > 0, \\\\text{ there exists a constant } n_0 > 0 \\\\text{ such that } 0 \\\\le cg(n) < f(n) \\\\text{ for all } n \\\\ge n_0 \\\\}$.\nThis means $f(n)$ becomes arbitrarily large relative to $g(n)$ as $n$ grows large: $\\\\lim_{n \\\\to \\\\infty} f(n)/g(n) = \\\\infty$.\nExample: $n^2/2 = \\\\omega(n)$, but $n^2/2 \\\\ne \\\\omega(n^2)$.\nAlso, $f(n) \\\\in \\\\omega(g(n))$ if and only if $g(n) \\\\in o(f(n))$.\n\n### Comparing functions\nProperties (for asymptotically positive $f(n), g(n)$):\n-   **Transitivity**: Holds for $\\\\Theta, O, \\\\Omega, o, \\\\omega$.\n-   **Reflexivity**: $f(n)=\\\\Theta(f(n))$, $f(n)=O(f(n))$, $f(n)=\\\\Omega(f(n))$.\n-   **Symmetry**: $f(n)=\\\\Theta(g(n)) \\\\iff g(n)=\\\\Theta(f(n))$.\n-   **Transpose symmetry**: $f(n)=O(g(n)) \\\\iff g(n)=\\\\Omega(f(n))$; $f(n)=o(g(n)) \\\\iff g(n)=\\\\omega(f(n))$.\nNot all functions are asymptotically comparable (e.g., $n$ and $n^{1+\\\\sin n}$).\n\n## 3.3 Standard notations and common functions\n\n### Monotonicity\n-   $f(n)$ is *monotonically increasing* if $m \\\\le n \\\\implies f(m) \\\\le f(n)$.\n-   $f(n)$ is *monotonically decreasing* if $m \\\\le n \\\\implies f(m) \\\\ge f(n)$.\n-   $f(n)$ is *strictly increasing* if $m < n \\\\implies f(m) < f(n)$.\n-   $f(n)$ is *strictly decreasing* if $m < n \\\\implies f(m) > f(n)$.\n\n### Floors and ceilings\n-   $\\\\\\lfloor x \\\\\\rfloor$ (floor): greatest integer $\\\\le x$.\n-   $\\\\\\lceil x \\\\\\rceil$ (ceiling): least integer $\\\\ge x$.\nProperties: $x-1 < \\\\\\lfloor x \\\\\\rfloor \\\\le x \\\\le \\\\\\lceil x \\\\\\rceil < x+1$. For integer $n$, $\\\\\\\\\rfloor n/2 \\\\\\rfloor + \\\\\\lceil n/2 \\\\\\rceil = n$.\n\n### Modular arithmetic\n$a \\\\pmod n = a - n \\\\\\lfloor a/n \\\\\\rfloor$. $0 \\\\le a \\\\pmod n < n$.\n$a \\\\equiv b \\\\pmod n$ if $(a \\\\pmod n) = (b \\\\pmod n)$, or equivalently, $n$ divides $(b-a)$.\n\n### Polynomials\n$p(n) = \\\\sum_{i=0}^d a_i n^i$, where $a_d \\\\ne 0$, is a polynomial of degree $d$. If $a_d > 0$, $p(n) = \\\\Theta(n^d)$.\nA function $f(n)$ is *polynomially bounded* if $f(n) = O(n^k)$ for some constant $k$.\n\n### Exponentials\n$a^0=1, a^1=a, a^{-1}=1/a, (a^m)^n=a^{mn}, (a^m)^n=(a^n)^m, a^m a^n = a^{m+n}$.\nFor $a>1, b$, $\\\\lim_{n \\\\to \\\\infty} n^b/a^n = 0$, so $n^b = o(a^n)$. Any positive exponential function grows faster than any polynomial.\nFor all real $x$, $e^x = 1+x+x^2/2!+x^3/3!+\\\\dots = \\\\sum_{i=0}^\\\\infty x^i/i!$. Also $1+x \\\\le e^x$. For $|x| \\\\le 1$, $1+x \\\\le e^x \\\\le 1+x+x^2$.\n\n### Logarithms\nNotations: $\\\\lg n = \\\\log_2 n$ (binary logarithm), $\\\\ln n = \\\\log_e n$ (natural logarithm), $\\\\lg^k n = (\\\\lg n)^k$, $\\\\lg \\\\lg n = \\\\lg(\\\\lg n)$.\nConvention: $\\\\lg n+k = (\\\\lg n)+k$.\nProperties (for $a,b,c>0$, bases not 1):\n$a = b^{\\\\log_b a}$, $\\\\log_c(ab) = \\\\log_c a + \\\\log_c b$, $\\\\log_b a^n = n \\\\log_b a$, $\\\\log_b a = \\\\log_c a / \\\\log_c b$.\n$\\\\log_b (1/a) = -\\\\log_b a$, $a^{\\\\log_b c} = c^{\\\\log_b a}$.\nSince $\\\\log_b a$ is a constant factor times $\\\\log_c a$, we often use $\\\\lg n$ in asymptotic notation when base doesn't matter (e.g., $O(\\\\lg n)$).\nFor $x > -1$, $x/(1+x) \\\\le \\\\ln(1+x) \\\\le x$.\n$f(n)$ is *polylogarithmically bounded* if $f(n) = O(\\\\lg^k n)$ for some $k$.\nFor $a>0, b>0$, $\\\\lg^b n = o(n^a)$. Any positive polynomial function grows faster than any polylogarithmic function.\n\n### Factorials\n$n! = 1 \\\\cdot 2 \\\\cdot \\\\dots \\\\cdot n$ for $n>0$, and $0!=1$.\n$n! = o(n^n)$, $n! = \\\\omega(2^n)$. $\\\\lg(n!) = \\\\Theta(n \\\\lg n)$.\nStirling's approximation: $n! = \\\\sqrt{2\\\\pi n} (n/e)^n (1 + \\\\Theta(1/n))$.\n\n### Functional iteration\n$f^{(i)}(n)$ is $f(n)$ applied $i$ times to $n$. $f^{(0)}(n)=n$. If $f(n)=2n$, $f^{(i)}(n)=2^i n$.\n\n### Iterated logarithm\n$\\\\lg^* n = \\\\min \\\\{ i \\\\ge 0 : \\\\lg^{(i)} n \\\\le 1 \\\\}$. This function grows extremely slowly.\n$\\\\lg^* 2 = 1, \\\\lg^* 4 = 2, \\\\lg^* 16 = 3, \\\\lg^* 65536 = 4, \\\\lg^*(2^{65536}) = 5$.\nFor practical purposes, $\\\\lg^* n \\\\le 5$.\n\n### Fibonacci numbers\n$F_0=0, F_1=1, F_i = F_{i-1} + F_{i-2}$ for $i \\\\ge 2$. Sequence: $0,1,1,2,3,5,8,\\\\dots$\nRelated to golden ratio $\\\\phi = (1+\\\\sqrt{5})/2 \\\\approx 1.61803$ and its conjugate $\\\\hat{\\\\phi} = (1-\\\\sqrt{5})/2 \\\\approx -0.61803$. They are roots of $x^2=x+1$.\n$F_i = (\\\\phi^i - \\\\hat{\\\\phi}^i)/\\\\sqrt{5}$. Since $|\\\\hat{\\\\phi}| < 1$, $F_i$ is $\\\\lfloor \\\\phi^i/\\\\sqrt{5} + 1/2 \\\\rfloor$. Fibonacci numbers grow exponentially."
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartII Sorting and Order Statistics/Introduction to Part II.md",
                "contents": "# Introduction to Part II: Sorting and Order Statistics\n\nThis part presents several algorithms that solve the sorting problem and the related problem of order statistics.\n\n## The Sorting Problem\n**Input**: A sequence of $n$ numbers $(a_1, a_2, \tune2 \text{ellipsis}, a_n)$.\n**Output**: A permutation (reordering) $(a'_1, a'_2, \tune2 \text{ellipsis}, a'_n)$ of the input sequence such that $a'_1 \topicoperator{\tcomp}{\textle} a'_2 \topicoperator{\tcomp}{\textle} \tune2 \text{ellipsis} \topicoperator{\tcomp}{\textle} a'_n$.\nThe input sequence is usually an $n$-element array, though it may be represented in other ways, such as a linked list.\n\n## The Structure of the Data\nIn practice, numbers to be sorted are rarely isolated values. Each is usually part of a collection of data called a **record**. Each record contains a **key**, which is the value to be sorted. The remainder of the record consists of **satellite data**, which are carried around with the key. Sorting algorithms typically permute the keys, and must permute the satellite data as well. If satellite data is large, it's often more efficient to permute an array of pointers to the records.\nWhen focusing on the problem of sorting, we typically assume the input consists only of numbers.\n\n## Why Sorting?\nSorting is a fundamental problem in computer science for several reasons:\n1.  Applications inherently need to sort information (e.g., bank statements).\n2.  Algorithms often use sorting as a key subroutine (e.g., rendering graphical objects).\n3.  A wide variety of sorting algorithms exist, employing a rich set of techniques common in algorithm design.\n4.  We can prove a nontrivial lower bound for sorting ($\topicOmega(n \topicoperator{\tprod}{\textlg} n)$ for comparison sorts). Some algorithms are asymptotically optimal.\n5.  Many engineering issues arise in implementation, depending on factors like prior knowledge of data, memory hierarchy, and software environment.\n\n## Sorting Algorithms Overview\nThis part covers several sorting algorithms:\n-   **Insertion sort**: Takes $\topicTheta(n^2)$ time in the worst case. Efficient for small input sizes. Sorts in place.\n-   **Merge sort**: Takes $\topicTheta(n \topicoperator{\tprod}{\textlg} n)$ time. Does not sort in place (standard version).\n-   **Heapsort**: Sorts $n$ numbers in place in $O(n \topicoperator{\tprod}{\textlg} n)$ time. Uses a heap data structure.\n-   **Quicksort**: Also sorts $n$ numbers in place. Worst-case time is $\topicTheta(n^2)$, but expected running time is $\topicTheta(n \topicoperator{\tprod}{\textlg} n)$. Often outperforms heapsort in practice.\n\nInsertion sort, merge sort, heapsort, and quicksort are **comparison sorts**: they determine the sorted order by comparing elements.\nChapter 8 introduces the decision-tree model to study limitations of comparison sorts, proving an $\topicOmega(n \topicoperator{\tprod}{\textlg} n)$ lower bound.\n\nAlgorithms that can beat this lower bound by using means other than comparisons include:\n-   **Counting sort**: Assumes input numbers are in a set {$0, 1, \tune2 \text{ellipsis}, k$}. Sorts in $\topicTheta(k+n)$ time. If $k=O(n)$, it runs in linear time.\n-   **Radix sort**: Extends counting sort. Sorts $n$ $d$-digit numbers (each digit up to $k$ values) in $\topicTheta(d(n+k))$ time. Linear if $d$ is constant and $k=O(n)$.\n-   **Bucket sort**: Requires knowledge of probabilistic distribution of input. Can sort $n$ real numbers uniformly distributed in $[0,1)$ in average-case $O(n)$ time.\n\n## Order Statistics\nThe **$i$th order statistic** of a set of $n$ numbers is the $i$th smallest number in the set.\n-   The minimum is the 1st order statistic.\n-   The maximum is the $n$th order statistic.\n-   A median is the \"halfway point\".\nChapter 9 shows how to find the $i$th smallest element in $O(n)$ time, even for arbitrary real numbers.\n\n## Background\nMost of this part does not rely on difficult mathematics, but analyses of quicksort, bucket sort, and the order-statistic algorithm use probability (reviewed in Appendix C) and material on probabilistic analysis and randomized algorithms (Chapter 5)."
            },
            {
                "file": "PartII Sorting and Order Statistics Algorithms.md",
                "contents": "# C6 Heapsort Algorithms\n\n## 6.1 Heap Operations (Conceptual)\n```\nPARENT(i)\n  1 return floor(i/2)\n```\n\n```\nLEFT(i)\n  1 return 2i\n```\n\n```\nRIGHT(i)\n  1 return 2i + 1\n```\n\n## 6.2 MAX-HEAPIFY(A, i)\n(Note: Actual pseudocode usually assumes A.heap-size is accessible globally or passed.)\n```\nMAX-HEAPIFY(A, i)\n  1  l = LEFT(i)\n  2  r = RIGHT(i)\n  3  if l <= A.heap-size and A[l] > A[i]\n  4      largest = l\n  5  else largest = i\n  6  if r <= A.heap-size and A[r] > A[largest]\n  7      largest = r\n  8  if largest != i\n  9      exchange A[i] with A[largest]\n  10     MAX-HEAPIFY(A, largest)\n```\n\n## 6.3 BUILD-MAX-HEAP(A, n)\n(Note: A.heap-size is set here, assuming A is an array of length n)\n```\nBUILD-MAX-HEAP(A)\n  1  A.heap-size = A.length\n  2  for i = floor(A.length/2) downto 1\n  3      MAX-HEAPIFY(A, i)\n```\n(The book's text version takes A and n, and sets A.heap-size = n)\n```\nBUILD-MAX-HEAP(A, n) // As per Fig 6.3 context and text pg 167\n  1 A.heap-size = n\n  2 for i = floor(n/2) downto 1\n  3     MAX-HEAPIFY(A, i)\n```\n\n## 6.4 HEAPSORT(A, n)\n```\nHEAPSORT(A)\n  1  BUILD-MAX-HEAP(A)\n  2  for i = A.length downto 2\n  3      exchange A[1] with A[i]\n  4      A.heap-size = A.heap-size - 1\n  5      MAX-HEAPIFY(A, 1)\n```\n(The book's text version takes A and n)\n```\nHEAPSORT(A, n) // As per text pg 170\n  1 BUILD-MAX-HEAP(A, n) // Assuming A is 1-indexed, and n is its used length\n  2 for i = n downto 2\n  3     exchange A[1] with A[i]\n  4     A.heap-size = A.heap-size - 1\n  5     MAX-HEAPIFY(A, 1)\n```\n\n## 6.5 MAX-HEAP-MAXIMUM(A)\n```\nMAX-HEAP-MAXIMUM(A)\n  1  if A.heap-size < 1\n  2      error \"heap underflow\"\n  3  return A[1]\n```\n\n## 6.5 MAX-HEAP-EXTRACT-MAX(A)\n```\nMAX-HEAP-EXTRACT-MAX(A)\n  1  max = MAX-HEAP-MAXIMUM(A) // includes underflow check\n  2  A[1] = A[A.heap-size]\n  3  A.heap-size = A.heap-size - 1\n  4  MAX-HEAPIFY(A, 1)\n  5  return max\n```\n\n## 6.5 MAX-HEAP-INCREASE-KEY(A, x, k)\n(Note: x is an object, k is the new key. This implies A stores objects or indices, and we need a way to find object x's index i. The pseudocode is slightly abstract here.)\n```\nMAX-HEAP-INCREASE-KEY(A, object_x_ref, new_key_k)\n  1  if new_key_k < object_x_ref.key\n  2      error \"new key is smaller than current key\"\n  3  object_x_ref.key = new_key_k\n  4  i = find index of object_x_ref in array A // This mapping is implementation dependent\n  5  while i > 1 and A[PARENT(i)].key < A[i].key\n  6      exchange A[i] with A[PARENT(i)] // also update mapping if necessary\n  7      i = PARENT(i)\n```\n\n## 6.5 MAX-HEAP-INSERT(A, x, n_max_size_of_A)\n(Note: x is an object to insert. A.heap-size tracks current elements. n_max_size_of_A is array capacity)\n```\nMAX-HEAP-INSERT(A, object_x, array_capacity_n)\n  1  if A.heap-size == array_capacity_n\n  2      error \"heap overflow\"\n  3  A.heap-size = A.heap-size + 1\n  4  original_key_of_x = object_x.key\n  5  object_x.key = -infinity // Sentinel key for MAX-HEAP-INCREASE-KEY logic\n  6  A[A.heap-size] = object_x // Place new object at the end\n  7  // map object_x to index A.heap-size in the array (implementation specific)\n  8  MAX-HEAP-INCREASE-KEY(A, object_x, original_key_of_x) // using the reference to object_x\n```\n\n# C7 Quicksort Algorithms\n\n## 7.1 QUICKSORT(A, p, r)\n```\nQUICKSORT(A, p, r)\n  1  if p < r\n  2      // Partition the subarray around the pivot, which ends up in A[q].\n  3      q = PARTITION(A, p, r)\n  4      QUICKSORT(A, p, q - 1)  // recursively sort the low side\n  5      QUICKSORT(A, q + 1, r) // recursively sort the high side\n```\n\n## 7.1 PARTITION(A, p, r)\n```\nPARTITION(A, p, r)\n  1  x = A[r]                // the pivot\n  2  i = p - 1               // highest index into the low side\n  3  for j = p to r - 1      // process each element other than the pivot\n  4      if A[j] <= x        // does this element belong on the low side?\n  5          i = i + 1       // index of a new slot in the low side\n  6          exchange A[i] with A[j] // put this element there\n  7  exchange A[i + 1] with A[r] // pivot goes just to the right of the low side\n  8  return i + 1             // new index of the pivot\n```\n\n## 7.3 RANDOMIZED-PARTITION(A, p, r)\n```\nRANDOMIZED-PARTITION(A, p, r)\n  1  i = RANDOM(p, r)\n  2  exchange A[r] with A[i]\n  3  return PARTITION(A, p, r)\n```\n\n## 7.3 RANDOMIZED-QUICKSORT(A, p, r)\n```\nRANDOMIZED-QUICKSORT(A, p, r)\n  1  if p < r\n  2      q = RANDOMIZED-PARTITION(A, p, r)\n  3      RANDOMIZED-QUICKSORT(A, p, q - 1)\n  4      RANDOMIZED-QUICKSORT(A, q + 1, r)\n```\n\n## P7-1 HOARE-PARTITION(A, p, r)\n```\nHOARE-PARTITION(A, p, r)\n  1  x = A[p]\n  2  i = p - 1\n  3  j = r + 1\n  4  while TRUE\n  5      repeat\n  6          j = j - 1\n  7      until A[j] <= x\n  8      repeat\n  9          i = i + 1\n  10     until A[i] >= x\n  11     if i < j\n  12         exchange A[i] with A[j]\n  13     else return j\n```\n\n## P7-4 STOOGE-SORT(A, p, r)\n```\nSTOOGE-SORT(A, p, r)\n  1  if A[p] > A[r]\n  2      exchange A[p] with A[r]\n  3  if p + 1 < r\n  4      k = floor((r - p + 1)/3)  // round down\n  5      STOOGE-SORT(A, p, r - k)    // first two-thirds\n  6      STOOGE-SORT(A, p + k, r)    // last two-thirds\n  7      STOOGE-SORT(A, p, r - k)    // first two-thirds again\n```\n\n## P7-5 TRE-QUICKSORT(A, p, r)\n```\nTRE-QUICKSORT(A, p, r)\n  1  while p < r\n  2      // Partition and then sort the low side.\n  3      q = PARTITION(A, p, r)\n  4      TRE-QUICKSORT(A, p, q - 1)\n  5      p = q + 1\n```\n\n# C8 Sorting in Linear Time Algorithms\n\n## 8.2 COUNTING-SORT(A, B, n, k)\n(Note: B is the output array, A is input array of length n, k is max value)\n```\nCOUNTING-SORT(A, B, k) // Assuming A is 1-indexed, B is 1-indexed, A.length is n\n  1  let C[0..k] be a new array\n  2  for i = 0 to k\n  3      C[i] = 0\n  4  for j = 1 to A.length\n  5      C[A[j]] = C[A[j]] + 1\n  6  // C[i] now contains the number of elements equal to i.\n  7  for i = 1 to k\n  8      C[i] = C[i] + C[i-1]\n  9  // C[i] now contains the number of elements less than or equal to i.\n  10 // Copy A to B, starting from the end of A for stability.\n  11 for j = A.length downto 1\n  12     B[C[A[j]]] = A[j]\n  13     C[A[j]] = C[A[j]] - 1\n  // 14 return B (if B is created inside and returned)\n```\n(The text on pg 209 has `COUNTING-SORT(A,n,k)` and B is created inside, let's use that for clarity)\n```\nCOUNTING-SORT(A, n, k)\n  1  let B[1..n] and C[0..k] be new arrays\n  2  for i = 0 to k\n  3      C[i] = 0\n  4  for j = 1 to n\n  5      C[A[j]] = C[A[j]] + 1\n  6  // C[i] now contains the number of elements equal to i.\n  7  for i = 1 to k\n  8      C[i] = C[i] + C[i-1]\n  9  // C[i] now contains the number of elements less than or equal to i.\n  10 // Copy A to B, starting from the end of A.\n  11 for j = n downto 1\n  12     B[C[A[j]]] = A[j]\n  13     C[A[j]] = C[A[j]] - 1 // to handle duplicate values\n  14 return B\n```\n\n## 8.3 RADIX-SORT(A, n, d)\n```\nRADIX-SORT(A, d) // Assuming A is an array of length n, d is number of digits\n  1  for i = 1 to d\n  2      use a stable sort to sort array A on digit i\n```\n\n## 8.4 BUCKET-SORT(A, n)\n```\nBUCKET-SORT(A) // Assuming A has n elements\n  1  let B[0..n-1] be a new array of lists (buckets)\n  2  for i = 0 to n - 1\n  3      make B[i] an empty list\n  4  for i = 1 to A.length // Assuming A is 1-indexed, A.length = n\n  5      insert A[i] into list B[floor(n * A[i])]\n  6  for i = 0 to n - 1\n  7      sort list B[i] with insertion sort\n  8  concatenate the lists B[0], B[1], ..., B[n-1] together in order\n  9  return the concatenated list\n```\n\n## P8-7 COMPARE-EXCHANGE(A, i, j)\n```\nCOMPARE-EXCHANGE(A, i, j)\n  1  if A[i] > A[j]\n  2      exchange A[i] with A[j]\n```\n\n## P8-7 COMPARE-EXCHANGE-INSERTION-SORT(A, n)\n```\nCOMPARE-EXCHANGE-INSERTION-SORT(A, n)\n  1  for i = 2 to n\n  2      for j = i - 1 downto 1\n  3          COMPARE-EXCHANGE(A, j, j + 1)\n```\n\n# C9 Medians and Order Statistics Algorithms\n\n## 9.1 MINIMUM(A, n)\n```\nMINIMUM(A) // Assuming A is 1-indexed array of length n\n  1  min = A[1]\n  2  for i = 2 to A.length\n  3      if min > A[i]\n  4          min = A[i]\n  5  return min\n```\n\n## 9.2 RANDOMIZED-SELECT(A, p, r, i)\n```\nRANDOMIZED-SELECT(A, p, r, i)\n  1  if p == r\n  2      return A[p]         // 1 <= i <= r-p+1 when p==r means i=1\n  3  q = RANDOMIZED-PARTITION(A, p, r)\n  4  k = q - p + 1           // k is the (relative) rank of pivot A[q]\n  5  if i == k               // the pivot value is the answer\n  6      return A[q]\n  7  else if i < k\n  8      return RANDOMIZED-SELECT(A, p, q - 1, i)\n  9  else return RANDOMIZED-SELECT(A, q + 1, r, i - k)\n```\n\n## 9.3 SELECT(A, p, r, i)\n```\nSELECT(A, p, r, i)\n  1  while (r - p + 1) mod 5 != 0\n  2      for j = p + 1 to r\n  3          if A[p] > A[j]\n  4              exchange A[p] with A[j] // put the minimum into A[p]\n  5      // If we want the minimum of A[p:r], we're done.\n  6      if i == 1\n  7          return A[p]\n  8      // Otherwise, we want the (i-1)st element of A[p+1:r].\n  9      p = p + 1\n  10     i = i - 1\n  11 g = (r - p + 1) / 5                         // number of 5-element groups\n  12 for j = p to p + g - 1                     // sort each group\n  13     sort (A[j], A[j+g], A[j+2g], A[j+3g], A[j+4g]) in place\n  14 // All group medians now lie in the middle fifth of A[p:r]. (Elements A[j+2g])\n  15 // Find the pivot x recursively as the median of the group medians.\n  16 x = SELECT(A, p + 2g, p + 3g - 1, ceil(g/2)) // Median of medians\n  17 q = PARTITION-AROUND(A, p, r, x)         // Partition around the pivot x. Assumed procedure.\n  18 // The rest is just like lines 3-9 of RANDOMIZED-SELECT.\n  19 k = q - p + 1\n  20 if i == k\n  21     return A[q]                           // the pivot value is the answer\n  22 else if i < k\n  23     return SELECT(A, p, q - 1, i)\n  24 else return SELECT(A, q + 1, r, i - k)\n```\n\n## P9-2 SIMPLER-RANDOMIZED-SELECT(A, p, r, i)\n```\nSIMPLER-RANDOMIZED-SELECT(A, p, r, i)\n  1  if p == r\n  2      return A[p] // 1 <= i <= r - p + 1 means that i = 1\n  3  q = RANDOMIZED-PARTITION(A, p, r)\n  4  k = q - p + 1\n  5  if i <= k // Changed from i == k and i < k to just i <= k\n  6      return SIMPLER-RANDOMIZED-SELECT(A, p, q, i)\n  7  else return SIMPLER-RANDOMIZED-SELECT(A, q + 1, r, i - k)\n```\n\n## P9-6 SELECT3(A, p, r, i)\n```\nSELECT3(A, p, r, i)\n  1  while (r - p + 1) mod 9 != 0\n  2      for j = p + 1 to r\n  3          if A[p] > A[j]\n  4              exchange A[p] with A[j]\n  5      if i == 1\n  6          return A[p]\n  7      p = p + 1\n  8      i = i - 1\n  9  g = (r - p + 1) / 3                // number of 3-element groups\n  10 for j = p to p + g - 1            // run through the groups\n  11     sort (A[j], A[j + g], A[j + 2g]) in place\n  12 // All group medians now lie in the middle third of A[p:r].\n  13 g_prime = g / 3                      // number of 3-element subgroups (of medians)\n  14 for j = p + g to p + g + g_prime - 1 // sort the subgroups of medians\n  15     // A[j] here refers to A[p+g + (idx_in_medians)*1] for medians\n  16     // The medians are at A[p+g], A[p+g+1], ..., A[p+2g-1]\n  17     // The actual elements being sorted for subgroup medians are A[p+g + k_subgroup*g_prime + offset_within_subgroup]\n  18     // The pseudocode in the book has A[j], A[j+g'], A[j+2g'] which is likely an error and should refer to indices within the list of medians.\n  19     // Correct interpretation: sort the medians of the 3-element groups (A[p+g .. p+2g-1]) into groups of 3 and find their medians.\n  20     // For simplicity, let M be the array of g medians from line 11.\n  21     // Effectively, we'd call something like:\n  22     // x = SELECT3(M, 1, g, ceil(g/2)) to find median of medians.\n  23     // The book's provided pseudocode for SELECT3 has errors in indexing for lines 16-17.\n  24     // The simplified view from problem description (pg 246 for general Select with groups of 3):\n  25     // "
            },
            {
                "file": "      Part II Sorting and order statistics\nIntroduction\nThis part presents several algorithms that solve the following sorting problem:                                   Input: A sequence of n numbers (a_1, a1, ..., an).\nOutput: A permutation (reordering) (a'\u2081, a'\u2082, ..., a'n) of the Transacting the input sequence such that a', \u2264 a', \u2264... \u2264 an.\nThe input sequence is usually an n-                element array, although it might be represented\nin some other fashion, such as a linked list.\nThe structure of the data\nIn practice, the numbers to be sorted are rarely isolated values. Each is usually part\nof a collection of data called a record. Each record contains a key, which is the\nvalue to be sorted. The remainder of the record consists of satellite data, which are\nusually carried around with the key. In practice, when a sorting algorithm permutes\nthe keys, it must permute the satellite data as well. If each record includes a large\namount of satellite data, it often pays to permute an array of pointers to the records\nrather than the records themselves in order to minimize data movement.\nIn a sense, it is these implementation details that distinguish an algorithm from\na full-blown program. A sorting algorithm describes the method to determine the\nsorted order, regardless of whether what's being sorted are individual numbers or\nlarge records containing many bytes of satellite data. Thus, when focusing on the\nproblem of sorting, we typically assume that the input consists only of numbers.\nTranslating an algorithm for sorting numbers into a program for sorting records\nis conceptually straightforward, although in a given engineering situation other\nsubtleties may make the actual programming task a challenge.\n",
                "contents": "This part presents several algorithms that solve the following sorting problem:\n**Input:** A sequence of n numbers (a_1, a1, ..., an).\n**Output:** A permutation (reordering) (a'\u2081, a'\u2082, ..., a'n) of the input sequence such\nthat a'\u2081 \u2264 a'\u2082 \u2264 ... \u2264 a'n.\nThe input sequence is usually an n- element array, although it may be represented\nin some other fashion, such as a linked list.\n### The structure of the data\nIn practice, the numbers to be sorted are rarely isolated values. Each is usually part\nof a collection of data called a record. Each record contains a key, which is the\nvalue to be sorted. The remainder of the record consists of satellite data, which are\nusually carried around with the key. In practice, when a sorting algorithm permutes\nthe keys, it must permute the satellite data as well. If each record includes a large\namount of satellite data, it often pays to permute an array of pointers to the records\nrather than the records themselves in order to minimize data movement.\nIn a sense, it is these implementation details that distinguish an algorithm from\na full-blown program. A sorting algorithm describes the method to determine the\nsorted order, regardless of whether what's being sorted are individual numbers or\nlarge records containing many bytes of satellite data. Thus, when focusing on the\nproblem of sorting, we typically assume that the input consists only of numbers.\nTranslating an algorithm for sorting numbers into a program for sorting records\nis conceptually straightforward, although in a given engineering situation other\nsubtleties may make the actual programming task a challenge.\n"
            }
        ]
    },
    {
        "items": [
            {
                "file": "foundations overview.md",
                "contents": "# Part I: Foundations Overview\n\nThis document provides an overview of foundational concepts covered in Part I of Introduction to Algorithms, focusing on the role of algorithms, methods for characterizing running times, the divide-and-conquer strategy, and probabilistic analysis with randomized algorithms.\n\n# 1 The Role of Algorithms in Computing\n\nAn *algorithm* is a sequence of computational steps that transform *input* into *output*. It is a tool for solving a well-defined *computational problem*.\nA *problem statement* defines a relationship between input and output, possibly with additional constraints. An *instance of a problem* is any input that satisfies these constraints.\nA *data structure* is a way to store and organize data to facilitate access and modifications.\n\n# 2 Characterizing Running Times\n\nThe *running time* of an algorithm on a particular input is the number of primitive operations or \u201csteps\u201d executed. The *input size* depends on the problem; it could be the number of items in the input, the total number of bits, etc.\n\nWe focus on *worst-case running time* because it gives an upper bound, occurs fairly often for some algorithms, and the average case is often as bad as the worst case. The *order of growth* or *rate of growth* of the running time is of primary interest, allowing us to abstract away from machine-dependent constants and focus on how running time scales with input size.\n\n## 2.1 Asymptotic Notation\n\nAsymptotic notation describes behavior for large input sizes.\n\n*Theta ($\\\\\\Theta$): Exact bound*\n$\\\\\\Theta(g(n)) = \\\\\\{f(n): \\\\\\text{there exist positive constants } c_1, c_2, \\\\\\text{ and } n_0 \\\\\\text{ such that } 0 \\\\\\le c_1 g(n) \\\\\\le f(n) \\\\\\le c_2 g(n) \\\\\\text{ for all } n \\\\\\ge n_0\\\\\\}$.\n\n*Big O ($O$): Asymptotic upper bound*\n$O(g(n)) = \\\\\\{f(n): \\\\\\text{there exist positive constants } c \\\\\\text{ and } n_0 \\\\\\text{ such that } 0 \\\\\\le f(n) \\\\\\le c g(n) \\\\\\text{ for all } n \\\\\\ge n_0\\\\\\}$.\n\n*Big Omega ($\\\\\\\\\\Omega$): Asymptotic lower bound*\n$\\\\\\\\\\\\\\Omega(g(n)) = \\\\\\{f(n): \\\\\\text{there exist positive constants } c \\\\\\text{ and } n_0 \\\\\\text{ such that } 0 \\\\\\le c g(n) \\\\\\le f(n) \\\\\\text{ for all } n \\\\\\ge n_0\\\\\\}$.\n\n*Little o ($o$): Upper bound that is not asymptotically tight*\n$o(g(n)) = \\\\\\{f(n): \\\\\\text{for any positive constant } c > 0, \\\\\\text{ there exists a constant } n_0 > 0 \\\\\\text{ such that } 0 \\\\\\le f(n) < c g(n) \\\\\\text{ for all } n \\\\\\ge n_0\\\\\\}$.\nEquivalently, $\\\\\\\\lim_{n\\\\\\\\to\\\\\\\\infty} f(n)/g(n) = 0$.\n\n*Little omega ($\\\\\\\\\\\\\\\\omega$): Lower bound that is not asymptotically tight*\n$\\\\\\\\\\\\\\\\omega(g(n)) = \\\\\\{f(n): \\\\\\text{for any positive constant } c > 0, \\\\\\text{ there exists a constant } n_0 > 0 \\\\\\text{ such that } 0 \\\\\\le c g(n) < f(n) \\\\\\text{ for all } n \\\\\\ge n_0\\\\\\}$.\nEquivalently, $\\\\\\\\lim_{n\\\\\\\\to\\\\\\\\infty} f(n)/g(n) = \\\\\\\\\\infty$.\n\n# 3 Divide-and-Conquer\n\nThe divide-and-conquer strategy involves three steps:\n1.  **Divide** the problem into one or more subproblems that are smaller instances of the same problem.\n2.  **Conquer** the subproblems by solving them recursively. If a subproblem size is small enough (base case), solve it directly.\n3.  **Combine** the solutions to the subproblems to form a solution to the original problem.\n\nRecurrence equations, or recurrences, are used to describe the running time of divide-and-conquer algorithms. A recurrence for the running time $T(n)$ is an equation or inequality that describes $T(n)$ in terms of its value on smaller inputs.\nFor example, merge sort's recurrence:\n$T(n) = \\\\\\begin{cases} \\\\\\Theta(1) & \\\\\\text{if } n=1 \\\\\\\\ 2T(n/2) + \\\\\\Theta(n) & \\\\\\text{if } n > 1 \\\\\\end{cases}$\nThis solves to $T(n) = \\\\\\Theta(n \\\\\\lg n)$.\n\nMethods for solving recurrences:\n1.  **Substitution method**: Guess a bound and use mathematical induction to prove it.\n2.  **Recursion-tree method**: Convert the recurrence into a tree whose nodes represent costs at various levels of recursion. Sum costs to obtain a guess.\n3.  **Master method**: Provides a cookbook for solving recurrences of the form $T(n) = aT(n/b) + f(n)$, where $a \\\\\\ge 1, b > 1$. It compares $f(n)$ with $n^{\\\\\\\\log_b a}$.\n\n# 4 Probabilistic Analysis and Randomized Algorithms\n\n**Probabilistic analysis** uses probability to analyze problems, typically by assuming a distribution of inputs and computing an average-case running time. For example, the hiring problem.\n\n*The Hiring Problem*: Hire the best candidate seen so far. Cost is dominated by hiring fee $c_h$. If $n$ candidates arrive in random order, the expected number of hires is $\\\\\\\\ln n$. Expected hiring cost is $O(c_h \\\\\\ln n)$. This uses *indicator random variables*.\n\nAn **indicator random variable** $I\\\\\\{A\\\\\\}$ for an event $A$ is defined as:\n$I\\\\\\{A\\\\\\} = \\\\\\begin{cases} 1 & \\\\\\text{if } A \\\\\\text{ occurs} \\\\\\\\ 0 & \\\\\\text{if } A \\\\\\text{ does not occur} \\\\\\end{cases}$\nA key property is $E[I\\\\\\{A\\\\\\}] = Pr\\\\\\{A\\\\\\}$. Linearity of expectation is powerful: $E[\\\\\\\\sum X_i] = \\\\\\\\\nsum E[X_i]$.\n\n**Randomized algorithms** make random choices during execution. Their behavior is determined by input and random numbers. We analyze their *expected running time*.\nExample: `RANDOMIZED-HIRE-ASSISTANT` randomly permutes candidates first, then runs the deterministic hiring algorithm. Expected hiring cost is $O(c_h \\\\\\ln n)$ for any input.\n\n*Birthday Paradox*: With $k$ people, what's the probability two share a birthday? If $k \\\\\\ge 23$, probability is $> 0.5$. Using indicator random variables, expected number of pairs with same birthday is $k(k-1)/(2n)$. When this is $\\\\\\\\ge 1$, we expect a match. This happens for $k = \\\\\\Theta(\\\\\\\\sqrt{n})$.\n\n*Balls and Bins*: Tossing $n$ balls into $b$ bins. Expected number of balls to get all bins hit is $b \\\\\\ln b$ (coupon collector's problem).\n"
            },
            {
                "file": "PartI Foundations Algorithms.md",
                "contents": "# Part I Foundations Algorithms\n\nThis file contains pseudocode for algorithms discussed in Part I: Foundations.\n\n# C4 Divide-and-Conquer Algorithms\n\n## C4.1 MATRIX-MULTIPLY\n```\nMATRIX-MULTIPLY(A, B, C, n)\n  // A, B, C are n x n matrices. Computes C = C + A * B.\n  // Assumes C is initialized (e.g., to zero if C = A * B is desired).\n1 for i = 1 to n\n2   for j = 1 to n\n3     for k = 1 to n\n4       C[i,j] = C[i,j] + A[i,k] * B[k,j]\n```\n\n## C4.1 MATRIX-MULTIPLY-RECURSIVE\n```\nMATRIX-MULTIPLY-RECURSIVE(A, B, C, n)\n  // A, B, C are n x n matrices. Computes C = C + A * B.\n  // n is an exact power of 2.\n1 if n == 1\n2   // Base case.\n3   C[1,1] = C[1,1] + A[1,1] * B[1,1]\n4   return\n5 // Divide.\n6 partition A, B, and C into n/2 x n/2 submatrices\n  // A11, A12, A21, A22; B11, B12, B21, B22;\n  // and C11, C12, C21, C22; respectively\n7 // Conquer.\n8 MATRIX-MULTIPLY-RECURSIVE(A11, B11, C11, n/2)\n9 MATRIX-MULTIPLY-RECURSIVE(A11, B12, C12, n/2)\n10 MATRIX-MULTIPLY-RECURSIVE(A21, B11, C21, n/2)\n11 MATRIX-MULTIPLY-RECURSIVE(A21, B12, C22, n/2)\n12 MATRIX-MULTIPLY-RECURSIVE(A12, B21, C11, n/2)\n13 MATRIX-MULTIPLY-RECURSIVE(A12, B22, C12, n/2)\n14 MATRIX-MULTIPLY-RECURSIVE(A22, B21, C21, n/2)\n15 MATRIX-MULTIPLY-RECURSIVE(A22, B22, C22, n/2)\n```\n\n# C5 Probabilistic Analysis and Randomized Algorithms\n\n## C5.1 HIRE-ASSISTANT\n```\nHIRE-ASSISTANT(n)\n  // n is the number of candidates.\n1 best = 0  // candidate 0 is a least-qualified dummy candidate\n2 for i = 1 to n\n3   interview candidate i\n4   if candidate i is better than candidate best\n5     best = i\n6     hire candidate i\n```\n\n## C5.3 RANDOMIZED-HIRE-ASSISTANT\n```\nRANDOMIZED-HIRE-ASSISTANT(n)\n1 randomly permute the list of candidates\n2 HIRE-ASSISTANT(n)\n```\n\n## C5.3 RANDOMLY-PERMUTE\n```\nRANDOMLY-PERMUTE(A, n)\n  // A is an array of n elements.\n1 for i = 1 to n\n2   swap A[i] with A[RANDOM(i, n)]\n```\n\n## C5.4 ONLINE-MAXIMUM\n```\nONLINE-MAXIMUM(k, n)\n  // k is the number of initial candidates to interview and reject.\n  // n is the total number of candidates.\n1 best-score = -\\\\\\\\\\\\\\\\infty\n2 for i = 1 to k\n3   if score(i) > best-score\n4     best-score = score(i)\n5 for i = k + 1 to n\n6   if score(i) > best-score\n7     return i // hire candidate i and stop\n8 return n // hire candidate n (either best or forced choice)\n```\n"
            },
            {
                "file": "PartI Foundations/C4 Divide-and-Conquer.md",
                "contents": "# 4 Divide-and-Conquer\n\nThe divide-and-conquer method is a powerful strategy for designing asymptotically efficient algorithms. It involves breaking a problem into smaller subproblems, solving them recursively, and then combining their solutions.\n\nFor divide-and-conquer, a problem instance is solved recursively. \nIf the problem is small enough\u2014the *base case*\u2014it's solved directly.\nOtherwise\u2014the *recursive case*\u2014three steps are performed:\n1.  **Divide** the problem into one or more subproblems that are smaller instances of the same problem.\n2.  **Conquer** the subproblems by solving them recursively.\n3.  **Combine** the subproblem solutions to form a solution to the original problem.\n\nThe recursion bottoms out when a subproblem reaches a base case and is solved directly.\n\n## 4.1 Recurrences\n\nA *recurrence* is an equation or inequality that describes a function in terms of its value on smaller arguments. Recurrences naturally characterize the running times of divide-and-conquer algorithms.\nFor example, the merge sort algorithm (Section 2.3.1) has a running time described by the recurrence $T(n) = 2T(n/2) + \\\\\\Theta(n)$, with solution $T(n) = \\\\\\Theta(n \\\\\\lg n)$.\n\nThe general form of a recurrence involves one or more base cases and recursive cases. A recurrence is *well defined* if there is at least one function that satisfies it.\n\n*Algorithmic recurrences* describe running times of divide-and-conquer algorithms. Such a recurrence $T(n)$ is typically algorithmic if:\n1.  For all $n < n_0$ (for some threshold $n_0 > 0$), $T(n) = \\\\\\Theta(1)$.\n2.  For all $n \\\\\\ge n_0$, every path of recursion terminates in a defined base case within a finite number of recursive invocations.\n\nA convention is adopted: Whenever a recurrence is stated without an explicit base case, it is assumed to be algorithmic, implying $T(n) = \\\\\\Theta(1)$ for sufficiently small $n$.\nAsymptotic solutions often remain unchanged if floors and ceilings are dropped from arguments (e.g., $T(\\\\\\lfloor n/2 \\\\\\rfloor)$ becomes $T(n/2)$). Section 4.7 discusses conditions for this.\n\nExamples of recurrences:\n-   $T(n) = 8T(n/2) + \\\\\\Theta(1)$ (simple matrix multiplication D&C) $\\\\\\\\implies T(n) = \\\\\\Theta(n^3)$.\n-   $T(n) = 7T(n/2) + \\\\\\Theta(n^2)$ (Strassen's algorithm) $\\\\\\\\implies T(n) = \\\\\\Theta(n^{\\\\\\\\lg 7}) \\\\\\approx \\\\\\Theta(n^{2.81})$.\n-   $T(n) = T(n/3) + T(2n/3) + \\\\\\Theta(n) \\\\\\implies T(n) = \\\\\\Theta(n \\\\\\lg n)$.\n-   $T(n) = T(n-1) + \\\\\\Theta(1)$ (recursive linear search) $\\\\\\\\implies T(n) = \\\\\\Theta(n)$.\n\n## 4.2 Solving recurrences\n\nThis chapter section (in the book, referring to sections 4.3-4.7) explores methods for solving recurrences, i.e., obtaining asymptotic $\\\\\\\\Theta$-, $O$-, or $\\\\\\\\\\\\\\Omega$-bounds.\n\n1.  **Substitution method** (Section 4.3): Guess the form of the solution and use mathematical induction to prove correctness and solve for constants. This is a robust method but requires a good guess.\n\n2.  **Recursion-tree method** (Section 4.4): Models the recurrence as a tree where nodes represent costs at various recursion levels. Sum costs per level and then across levels. Useful for generating guesses for the substitution method.\n\n3.  **Master method** (Sections 4.5 and 4.6): Provides bounds for recurrences of the form $T(n) = aT(n/b) + f(n)$, where $a \\\\\\ge 1, b > 1$ are constants, and $f(n)$ is a driving function. Requires memorizing three cases based on comparing $f(n)$ with $n^{\\\\\\\\log_b a}$ (the watershed function).\n\n4.  **Akra-Bazzi method** (Section 4.7): A more general method for solving divide-and-conquer recurrences, including those with unequal subproblem sizes. It involves calculus.\n\n## 4.1 Multiplying square matrices\n\nLet $A=(a_{ik})$ and $B=(b_{jk})$ be $n \\\\\\times n$ matrices. Their product $C=A \\\\\\cdot B$ is an $n \\\\\\times n$ matrix where $c_{ij} = \\\\\\sum_{k=1}^{n} a_{ik}b_{kj}$.\n\nA straightforward iterative algorithm, [[PartI Foundations Algorithms.md#C4.1 MATRIX-MULTIPLY]], computes this using three nested loops, resulting in a $\\\\\\\\Theta(n^3)$ running time.\n\n### A simple divide-and-conquer algorithm\nMatrices $A, B, C$ are partitioned into four $n/2 \\\\\\times n/2$ submatrices each.\n$A = \\\\\\begin{pmatrix} A_{11} & A_{12} \\\\\\\\ A_{21} & A_{22} \\\\\\end{pmatrix}$, $B = \\\\\\begin{pmatrix} B_{11} & B_{12} \\\\\\\\ B_{21} & B_{22} \\\\\\end{pmatrix}$, $C = \\\\\\begin{pmatrix} C_{11} & C_{12} \\\\\\\\ C_{21} & C_{22} \\\\\\end{pmatrix}$.\nThe product $C = A \\\\\\cdot B$ implies:\n$C_{11} = A_{11}B_{11} + A_{12}B_{21}$\n$C_{12} = A_{11}B_{12} + A_{12}B_{22}$\n$C_{21} = A_{21}B_{11} + A_{22}B_{21}$\n$C_{22} = A_{21}B_{12} + A_{22}B_{22}$\nThis involves 8 recursive multiplications of $n/2 \\\\\\times n/2$ matrices and 4 matrix additions (costing $\\\\\\\\Theta(n^2)$).\nAssuming $n$ is a power of 2, and partitioning takes $\\\\\\\\Theta(1)$ time using index calculations (rather than $\\\\\\\\Theta(n^2)$ for copying), the recurrence for [[PartI Foundations Algorithms.md#C4.1 MATRIX-MULTIPLY-RECURSIVE]] is:\n$T(n) = 8T(n/2) + \\\\\\Theta(1)$ (if matrix additions are incorporated into the recursive calls for $C = C + A \\\\\\cdot B$ rather than $C=A \\\\\\cdot B$).\nIf additions are explicit: $T(n) = 8T(n/2) + \\\\\\Theta(n^2)$.\nThe book's `MATRIX-MULTIPLY-RECURSIVE` uses the $C=C+A \\\\\\cdot B$ form and describes its recurrence as $T(n) = 8T(n/2) + \\\\\\Theta(1)$ based on the structure of its recursive calls (lines 8-15 on p.83, adding product terms one by one to submatrices of C). This version has $T(1) = \\\\\\Theta(1)$.\nThe solution to $T(n) = 8T(n/2) + \\\\\\Theta(1)$ is $T(n) = \\\\\\Theta(n^3)$ (by master theorem, case 1, where $n^{\\\\\\\\log_2 8} = n^3$ and $f(n)=1 = O(n^{3-\\\\\\\\epsilon})$ for $\\\\\\\\epsilon=1$). This offers no asymptotic improvement over the iterative algorithm.\n\nIf copying is used for partitioning ($\\\\\\\\Theta(n^2)$ cost), then $T(n) = 8T(n/2) + \\\\\\Theta(n^2)$. This still yields $T(n) = \\\\\\Theta(n^3)$ (master theorem, case 1 or 2 depending on interpretation, but $n^3$ dominates $n^2$).\n\n### Strassen's algorithm for matrix multiplication\nStrassen's method (1969) reduces the number of recursive multiplications from 8 to 7, at the cost of more matrix additions and subtractions. It runs in $O(n^{\\\\\\\\lg 7}) \\\\\\approx O(n^{2.81})$ time.\nThe algorithm involves four steps for $n > 1$ (assuming $n$ is a power of 2):\n1.  **Divide**: Partition $A, B, C$ into $n/2 \\\\\\times n/2$ submatrices. Cost $\\\\\\\\Theta(1)$ (index calculation).\n2.  **Create intermediate matrices**: Create 10 matrices $S_1, \\\\\\dots, S_{10}$, each the sum or difference of two submatrices from step 1. Create 7 matrices $P_1, \\\\\\dots, P_7$ to hold products. This costs $\\\\\\\\Theta(n^2)$ (10 additions/subtractions of $n/2 \\\\\\times n/2$ matrices).\n    $S_1 = B_{12} - B_{22}$\n    $S_2 = A_{11} + A_{12}$\n    ...\n    $S_{10} = B_{11} + B_{12}$\n3.  **Conquer**: Recursively compute 7 matrix products $P_1, \\\\\\dots, P_7$ using submatrices from step 1 and $S_i$ matrices. Cost $7T(n/2)$.\n    $P_1 = A_{11} S_1$\n    $P_2 = S_2 B_{22}$\n    ...\n    $P_7 = S_9 S_{10}$\n4.  **Combine**: Compute $C_{11}, C_{12}, C_{21}, C_{22}$ by adding/subtracting various $P_i$ matrices. Cost $\\\\\\\\Theta(n^2)$ (e.g., $C_{11} = P_5 + P_4 - P_2 + P_6$, requiring 4 additions/subtractions for $C_{11}$ and similarly for others, totaling 18 $n/2 \\\\\\times n/2$ additions/subtractions across all $C_{ij}$). The text simplifies and says step 4 takes $\\\\\\\\Theta(n^2)$ time as it adds/subtracts matrices 12 times (page 89).\n\nThe recurrence for Strassen's algorithm is $T(n) = 7T(n/2) + \\\\\\Theta(n^2)$.\nUsing the master theorem: $a=7, b=2$. $n^{\\\\\\\\log_b a} = n^{\\\\\\\\lg 7}$. Since $f(n) = n^2 = O(n^{\\\\\\\\lg 7 - \\\\\\\\\\epsilon})$ for some $\\\\\\\\\\\\\\epsilon > 0$ (as $\\\\\\\\lg 7 \\\\\\approx 2.80735 > 2$), case 1 applies. So, $T(n) = \\\\\\Theta(n^{\\\\\\\\lg 7})$.\n\n## 4.3 The substitution method for solving recurrences\n\nThe substitution method involves two steps:\n1.  Guess the form of the solution (e.g., $T(n) = O(g(n))$).\n2.  Use mathematical induction to find constants and show that the solution works.\n\nExample: $T(n) = 2T(\\\\\\lfloor n/2 \\\\\\rfloor) + \\\\\\Theta(n)$. Guess $T(n) = O(n \\\\\\lg n)$.\nInductive hypothesis: Assume $T(k) \\\\\\le ck \\\\\\lg k$ for $k < n$, specifically for $k = \\\\\\lfloor n/2 \\\\\\rfloor$.\n$T(n) \\\\\\le 2(c \\\\\\lfloor n/2 \\\\\\rfloor \\\\\\lg(\\\\\\\\lfloor n/2 \\\\\\rfloor)) + dn$ (where $\\\\\\\\Theta(n)$ means $\\\\\\\\le dn$ for some $d>0$ for large $n$).\n$T(n) \\\\\\le 2(c (n/2) \\\\\\lg(n/2)) + dn = cn \\\\\\lg(n/2) + dn = cn \\\\\\lg n - cn \\\\\\lg 2 + dn = cn \\\\\\lg n - cn + dn$.\nWe need $cn \\\\\\lg n - cn + dn \\\\\\le cn \\\\\\lg n$. This holds if $-cn + dn \\\\\\le 0$, or $d \\\\\\le c$. We can choose $c$ large enough for this and to handle base cases.\nBase cases: For $T(n) = \\\\\\Theta(1)$ for small $n$, we need to show $T(n_0) \\\\\\le cn_0 \\\\\\lg n_0$. This can usually be satisfied by choosing $c$ large enough.\n\n*A trick: subtracting a lower-order term*. If an inductive proof fails because the inductive hypothesis is too weak, try subtracting a lower-order term. Example: $T(n) = 2T(n/2) + \\\\\\Theta(1)$. Guess $T(n) = O(n)$. IH: $T(k) \\\\\\le ck$. $T(n) \\\\\\le 2(c(n/2)) + d_0 = cn + d_0$. This is not $\\\\\\\\le cn$. Try $T(n) \\\\\\le cn - d$ for $d \\\\\\ge 0$. $T(n) \\\\\\le 2(c(n/2) - d) + d_0 = cn - 2d + d_0$. We want $cn - 2d + d_0 \\\\\\le cn - d$. This means $-2d + d_0 \\\\\\le -d$, or $d_0 \\\\\\le d$. We can pick $d$ large enough.\n\n*Avoiding pitfalls*: Do not use asymptotic notation in the inductive hypothesis (e.g., $T(n) = O(n)$). Be explicit with constants.\n\n## 4.4 The recursion-tree method for solving recurrences\n\nA recursion tree visualizes the costs incurred at each level of recursion. Each node represents the cost of a single subproblem. Sum costs within each level, then sum all per-level costs.\n\nExample: $T(n) = 3T(n/4) + \\\\\\Theta(n^2)$. Assume $T(n) = 3T(n/4) + cn^2$.\nRoot: $cn^2$\nLevel 1: 3 nodes, each cost $c(n/4)^2 = cn^2/16$. Total: $3cn^2/16 = (3/16)cn^2$.\nLevel 2: $3^2$ nodes, each cost $c(n/4^2)^2 = cn^2/256$. Total: $9cn^2/256 = (3/16)^2 cn^2$.\nLevel $i$: $3^i$ nodes, cost $c(n/4^i)^2$. Total: $3^i c(n/4^i)^2 = (3/16)^i cn^2$.\nTree height: Recursion stops when $n/4^h = 1 \\\\\\\\implies h = \\\\\\log_4 n$. Number of levels is $\\\\\\\\log_4 n + 1$.\nTotal cost: $T(n) = \\\\\\sum_{i=0}^{\\\\\\\\log_4 n - 1} (3/16)^i cn^2 + \\\\\\text{cost of leaves}$.\nLeaves are at depth $\\\\\\\\log_4 n$. Number of leaves: $3^{\\\\\\\\log_4 n} = n^{\\\\\\\\log_4 3}$. Each leaf costs $\\\\\\\\Theta(1)$. Total leaf cost: $\\\\\\\\Theta(n^{\\\\\\\\log_4 3})$.\nSum: $cn^2 \\\\\\sum_{i=0}^{\\\\\\\\log_4 n - 1} (3/16)^i + \\\\\\Theta(n^{\\\\\\\\log_4 3})$.\nThe sum is a geometric series, bounded by $cn^2 \\\\\\frac{1}{1-3/16} = cn^2 \\\\\\frac{16}{13}$.\nSo $T(n) = \\\\\\Theta(n^2) + \\\\\\Theta(n^{\\\\\\\\log_4 3})$. Since $\\\\\\\\log_4 3 < 2$, $T(n) = \\\\\\Theta(n^2)$.\n\nExample: $T(n) = T(n/3) + T(2n/3) + \\\\\\Theta(n)$. Assume $T(n) = T(n/3) + T(2n/3) + cn$.\nRoot: $cn$\nLevel 1: Costs $c(n/3)$ and $c(2n/3)$. Total: $c(n/3+2n/3) = cn$.\nEach level sums to $cn$. Longest path is $n \\\\\\to (2/3)n \\\\\\to (2/3)^2 n \\\\\\dots \\\\\\to 1$. Height is $\\\\\\\\log_{3/2} n$.\nTotal cost guess: $cn \\\\\\cdot (\\\\\\\nlog_{3/2} n + 1) = O(n \\\\\\lg n)$.\nNumber of leaves is $L(n) = L(n/3)+L(2n/3)$ if $n \\\\\\ge n_0$, $L(n)=1$ if $n<n_0$. Solution $L(n) = \\\\\\Theta(n)$. Leaf cost is $\\\\\\\\Theta(n)$.\nOverall $T(n) = \\\\\\Theta(n \\\\\\lg n)$.\n\n## 4.5 The master method for solving recurrences\n\nThe master method solves recurrences of the form $T(n) = aT(n/b) + f(n)$, where $a \\\\\\ge 1, b > 1$ are constants, and $f(n)$ is an asymptotically positive function. $T(n)$ is defined on nonnegative integers (implicitly handling floors/ceilings).\nCompare $f(n)$ with $n^{\\\\\\\\log_b a}$ (the watershed function).\n\n**Theorem 4.1 (Master Theorem)**\n1.  If $f(n) = O(n^{\\\\\\\\log_b a - \\\\\\\\\\epsilon})$ for some constant $\\\\\\\\\\\\\\epsilon > 0$, then $T(n) = \\\\\\Theta(n^{\\\\\\\\log_b a})$. (Function $f(n)$ is polynomially smaller than $n^{\\\\\\\\log_b a}$).\n2.  If $f(n) = \\\\\\Theta(n^{\\\\\\\\log_b a} \\\\\\lg^k n)$ for some constant $k \\\\\\ge 0$, then $T(n) = \\\\\\Theta(n^{\\\\\\\\log_b a} \\\\\\lg^{k+1} n)$.\n    (If $k=0$, $f(n) = \\\\\\Theta(n^{\\\\\\\\log_b a})$, then $T(n) = \\\\\\Theta(n^{\\\\\\\\log_b a} \\\\\\lg n)$).\n3.  If $f(n) = \\\\\\Omega(n^{\\\\\\\\log_b a + \\\\\\\\\\epsilon})$ for some constant $\\\\\\\\\\\\\\epsilon > 0$, and if $af(n/b) \\\\\\le cf(n)$ for some constant $c < 1$ and all sufficiently large $n$ (regularity condition), then $T(n) = \\\\\\Theta(f(n))$. (Function $f(n)$ is polynomially larger than $n^{\\\\\\\\log_b a}$).\n\nExamples:\n-   $T(n) = 9T(n/3) + n$. $a=9, b=3. n^{\\\\\\\\log_3 9} = n^2$. $f(n)=n = O(n^{2-\\\\\\\\epsilon})$ with $\\\\\\\\\\\\\\epsilon=1$. Case 1: $T(n) = \\\\\\Theta(n^2)$.\n-   $T(n) = T(2n/3) + 1$. $a=1, b=3/2. n^{\\\\\\\\log_{3/2} 1} = n^0 = 1$. $f(n)=1 = \\\\\\Theta(n^0 \\\\\\lg^0 n)$. Case 2 ($k=0$): $T(n) = \\\\\\Theta(1 \\\\\\cdot \\\\\\lg n) = \\\\\\Theta(\\\\\\\\lg n)$.\n-   $T(n) = 3T(n/4) + n \\\\\\lg n$. $a=3, b=4. n^{\\\\\\\\log_4 3} \\\\\\approx n^{0.793}$. $f(n)=n \\\\\\lg n = \\\\\\Omega(n^{\\\\\\\\log_4 3 + \\\\\\\\\\epsilon})$ for $\\\\\\\\\\\\\\epsilon \\\\\\approx 0.2$. Check regularity: $3(n/4)\\\\\\\\lg(n/4) \\\\\\le (3/4)n \\\\\\lg n = cf(n)$ with $c=3/4 < 1$. Case 3: $T(n) = \\\\\\Theta(n \\\\\\lg n)$.\n-   $T(n) = 2T(n/2) + n \\\\\\lg n$. $a=2, b=2. n^{\\\\\\\\log_2 2} = n$. $f(n)=n \\\\\\lg n = \\\\\\Theta(n \\\\\\lg^1 n)$. Case 2 ($k=1$): $T(n) = \\\\\\Theta(n \\\\\\lg^2 n)$.\n\nMaster method does not cover all cases. Gaps exist between cases, e.g., $f(n) = n/\\\\\\\nlg n$ for $T(n) = 2T(n/2) + n/\\\\\\\nlg n$. Here $f(n)$ is smaller than $n^{\\\\\\\\log_b a}=n$, but not polynomially smaller.\n\n## 4.6 Proof of the continuous master theorem\n(Details of proof omitted here, focuses on recurrence $T(n) = aT(n/b) + f(n)$ for real $n \\\\\\ge 1$, with base case $T(n)=\\\\\\\\Theta(1)$ for $0 \\\\\\le n < 1$.)\nLemma 4.2: $T(n) = \\\\\\Theta(n^{\\\\\\\\log_b a}) + \\\\\\sum_{j=0}^{\\\\\\\\lfloor \\\\\\log_b n \\\\\\rfloor} a^j f(n/b^j)$.\nLemma 4.3 provides bounds for the summation based on the three cases.\nTheorem 4.4 (Continuous Master Theorem) uses these lemmas and adjusts for general threshold $n_0$.\n\n## 4.7 Akra-Bazzi recurrences\nDeals with recurrences of the form $T(n) = f(n) + \\\\\\sum_{i=1}^k a_i T(n/b_i)$, where $a_i > 0, b_i > 1$. Generalizes master theorem.\nRequires $f(n)$ to satisfy the *polynomial-growth condition*: for any $\\\\\\\\phi \\\\\\ge 1$, there's $d>1$ s.t. $f(n)/d \\\\\\le f(\\\\\\\\psi n) \\\\\\le df(n)$ for $1 \\\\\\le \\\\\\psi \\\\\\le \\\\\\phi$ and $n \\\\\\ge \\\\\\tilde{n}$. If this holds, floors/ceilings can be ignored.\nSolution involves finding unique real $p$ such that $\\\\\\\\sum_{i=1}^k a_i / b_i^p = 1$.\nThen $T(n) = \\\\\\Theta(n^p (1 + \\\\\\int_1^n \\\\\\frac{f(x)}{x^{p+1}} dx))$.\nExample: $T(n) = T(n/5) + T(7n/10) + n$. Here $a_1=1, b_1=5, a_2=1, b_2=10/7, f(n)=n$.\nFind $p$ such that $(1/5)^p + (7/10)^p = 1$. $0 < p < 1$ (since for $p=0$, sum is 2; for $p=1$, sum is $1/5+7/10 = 9/10$).\nIntegral: $\\\\\\\\int_1^n \\\\\\frac{x}{x^{p+1}} dx = \\\\\\int_1^n x^{-p} dx = [\\\\\\\frac{x^{-p+1}}{-p+1}]_1^n = \\\\\\Theta(n^{1-p})$ since $1-p > 0$.\n$T(n) = \\\\\\Theta(n^p (1 + n^{1-p})) = \\\\\\Theta(n^p + n) = \\\\\\Theta(n)$ since $p < 1$.\n"
            },
            {
                "file": "PartI Foundations/C5 Probabilistic Analysis and Randomized Algorithms.md",
                "contents": "# 5 Probabilistic Analysis and Randomized Algorithms\n\nThis chapter introduces probabilistic analysis and randomized algorithms. Probabilistic analysis uses probability theory to analyze algorithms, typically assuming a distribution for inputs. Randomized algorithms use randomness as part of their logic.\n\n## 5.1 The hiring problem\n\nScenario: Hire a new office assistant. An employment agency sends one candidate per day. Interviewing costs $c_i$ (small), hiring costs $c_h$ (large, includes firing previous assistant). Goal: always have the best person seen so far.\nStrategy: If candidate $i$ is better than current assistant, fire current and hire $i$.\n[[PartI Foundations Algorithms.md#C5.1 HIRE-ASSISTANT]] describes this strategy.\nTotal cost: $O(c_i n + c_h m)$, where $n$ is total candidates, $m$ is number hired. We focus on analyzing $c_h m$.\n\n*Worst-case analysis*: If candidates arrive in increasing order of quality, we hire all $n$ candidates. Cost $O(c_h n)$.\n\n*Probabilistic analysis*: Assume candidates arrive in a *random order*, meaning any permutation of ranks is equally likely (uniform random permutation).\n\n*Randomized algorithms*: If we cannot assume random input order, we can *impose* it by randomly permuting the input before processing. The algorithm's behavior then depends on its random choices, not input order.\n\n## 5.2 Indicator random variables\n\nAn *indicator random variable* $I\\\\\\{A\\\\\\}$ associated with an event $A$ is:\n$I\\\\\\{A\\\\\\} = \\\\\\begin{cases} 1 & \\\\\\text{if } A \\\\\\text{ occurs} \\\\\\\\ 0 & \\\\\\text{if } A \\\\\\text{ does not occur} \\\\\\end{cases}$\n**Lemma 5.1**: For an event $A$, $E[I\\\\\\{A\\\\\\}] = Pr\\\\\\{A\\\\\\}$.\n\nIndicator random variables are useful for calculating expected values. *Linearity of expectation* states $E[\\\\\\\\sum X_i] = \\\\\\\\\nsum E[X_i]$, even if variables are dependent.\n\n*Analysis of the hiring problem using indicator random variables*:\nLet $X$ be the random variable for the number of times we hire. Let $X_i = I\\\\\\{\\\\\\\\text{candidate } i \\\\\\text{ is hired}\\\\\\\\}$.\nThen $X = \\\\\\sum_{i=1}^n X_i$.\n$E[X] = E[\\\\\\\\sum_{i=1}^n X_i] = \\\\\\sum_{i=1}^n E[X_i]$.\n$E[X_i] = Pr\\\\\\{\\\\\\\\text{candidate } i \\\\\\text{ is hired}\\\\\\\\}$.\nCandidate $i$ is hired if $i$ is better than candidates $1, \\\\\\dots, i-1$. With random order, any of these first $i$ candidates is equally likely to be the best among them. So, $Pr\\\\\\{\\\\\\\\text{candidate } i \\\\\\text{ is hired}\\\\\\\\} = 1/i$.\n$E[X] = \\\\\\sum_{i=1}^n (1/i) = H_n$, the $n$-th harmonic number.\n$H_n = \\\\\\ln n + O(1)$.\n**Lemma 5.2**: Assuming candidates are presented in random order, HIRE-ASSISTANT has an average-case total hiring cost of $O(c_h \\\\\\ln n)$. This is much better than $O(c_h n)$ worst-case.\n\n## 5.3 Randomized algorithms\n\nIf input distribution is unknown, probabilistic analysis isn't possible. A *randomized algorithm* makes random choices to achieve good expected performance for any input.\n[[PartI Foundations Algorithms.md#C5.3 RANDOMIZED-HIRE-ASSISTANT]] first randomly permutes the candidate list, then applies the deterministic HIRE-ASSISTANT.\n**Lemma 5.3**: The expected hiring cost of RANDOMIZED-HIRE-ASSISTANT is $O(c_h \\\\\\ln n)$.\nThe analysis is identical to the probabilistic analysis, but now the expectation is over the algorithm's random choices, not an assumed input distribution.\n\n*Randomly permuting arrays*:\nTo produce a *uniform random permutation* (each of $n!$ permutations equally likely).\n[[PartI Foundations Algorithms.md#C5.3 RANDOMLY-PERMUTE]] permutes array $A[1..n]$ in place in $\\\\\\\\Theta(n)$ time.\nIn iteration $i$, it swaps $A[i]$ with a random element from $A[i..n]$.\n**Lemma 5.4**: RANDOMLY-PERMUTE computes a uniform random permutation.\nProof uses a loop invariant:\n-*Loop Invariant:* Just prior to iteration $i$, for each possible $(i-1)$-permutation, the subarray $A[1..i-1]$ contains this $(i-1)$-permutation with probability $(n-i+1)!/n!$.\n-*Initialization:* For $i=1$, $A[1..0]$ is empty. The 0-permutation has probability $n!/n! = 1$. True.\n-*Maintenance:* An $(i-1)$-permutation $(x_1, \\\\\\dots, x_{i-1})$ is in $A[1..i-1]$ with probability $P_1 = (n-i+1)!/n!$. Element $x_i$ is chosen for $A[i]$ from $A[i..n]$ (which has $n-i+1$ elements) with probability $P_2 = 1/(n-i+1)$. The probability that $A[1..i]$ contains $(x_1, \\\\\\dots, x_i)$ is $P_1 P_2 = (n-i)!/n!$.\n-*Termination:* When $i=n+1$, the loop terminates. $A[1..n]$ is a given $n$-permutation with probability $(n-(n+1)+1)!/n! = 0!/n! = 1/n!$.\n\n## 5.4 Probabilistic analysis and further uses of indicator random variables\n\n### 5.4.1 The birthday paradox\nHow many people $k$ must be in a room for a 50% chance that two share a birthday (out of $n=365$ days)?\nProbability that $k$ people have distinct birthdays $B_k$: $Pr\\\\\\{B_k\\\\\\} = 1 \\\\\\cdot (1-1/n) \\\\\\cdot (1-2/n) \\\\\\dots (1-(k-1)/n)$.\nUsing $1+x \\\\\\le e^x$, $Pr\\\\\\{B_k\\\\\\} \\\\\\le e^0 e^{-1/n} e^{-2/n} \\\\\\dots e^{-(k-1)/n} = e^{-\\\\\\\\sum_{i=0}^{k-1} i/n} = e^{-k(k-1)/(2n)}$.\nFor $Pr\\\\\\{B_k\\\\\\} \\\\\\le 1/2$, we need $e^{-k(k-1)/(2n)} \\\\\\le 1/2$, so $-k(k-1)/(2n) \\\\\\le \\\\\\ln(1/2) = -\\\\\\\nln 2$. $k(k-1) \\\\\\ge 2n \\\\\\ln 2$. So $k \\\\\\approx \\\\\\sqrt{2n \\\\\\ln 2}$. For $n=365$, $k \\\\\\approx 22.9$, so $k=23$.\n\nUsing indicator random variables: Let $X_{ij} = I\\\\\\{\\\\\\\\text{person } i \\\\\\text{ and } j \\\\\\text{ share a birthday}\\\\\\\\}$. $Pr\\\\\\{\\\\\\\\text{match}\\\\\\\\} = 1/n$. So $E[X_{ij}] = 1/n$.\nLet $X$ be the number of pairs with matching birthdays. $X = \\\\\\sum_{1 \\\\\\le i < j \\\\\\le k} X_{ij}$.\n$E[X] = \\\\\\sum_{1 \\\\\\le i < j \\\\\\le k} E[X_{ij}] = \\\\\\binom{k}{2} \\\\\\frac{1}{n} = \\\\\\frac{k(k-1)}{2n}$.\nWhen $E[X] \\\\\\ge 1$, we expect at least one match. $k(k-1)/(2n) \\\\\\ge 1 \\\\\\\\implies k \\\\\\approx \\\\\\sqrt{2n}$. For $n=365$, $k \\\\\\approx \\\\\\sqrt{730} \\\\\\approx 27$. The two analyses give $\\\\\\\\Theta(\\\\\\\\sqrt{n})$.\n\n### 5.4.2 Balls and bins\nProcess: Randomly toss identical balls into $b$ bins. Tosses are independent.\n1.  *How many balls in a given bin?* If $n$ balls are tossed, expected number in a bin is $n/b$.\n2.  *How many balls until a given bin contains a ball?* This follows a geometric distribution with success $p=1/b$. Expected tosses is $1/p = b$.\n3.  *How many balls until every bin contains at least one ball?* (Coupon collector's problem)\n    Let $n_i$ be tosses for $i$-th hit (ball in new empty bin). $n = \\\\\\sum_{i=1}^b n_i$. Success probability for $i$-th hit is $(b-i+1)/b$. $E[n_i] = b/(b-i+1)$.\n    $E[n] = \\\\\\sum_{i=1}^b \\\\\\frac{b}{b-i+1} = b \\\\\\sum_{j=1}^b \\\\\\frac{1}{j} = b H_b = b(\\\\\\\\ln b + O(1))$.\n    Approximately $b \\\\\\ln b$ tosses.\n\n### 5.4.3 Streaks\nLongest streak of heads in $n$ fair coin flips. Expected length is $\\\\\\\\Theta(\\\\\\\\lg n)$.\n*Upper bound*: Probability of streak of length $k$ starting at $i$ is $1/2^k$. For $k=2 \\\\\\lceil \\\\\\lg n \\\\\\rceil$, $Pr\\\\\\{A_{i,k}\\\\\\\\} \\\\\\le 1/n^2$. Probability of any streak of length $2 \\\\\\lceil \\\\\\lg n \\\\\\rceil$ is $\\\\\\\\le (n-k+1)/n^2 < 1/n$. Let $L$ be length of longest streak. $E[L] = \\\\\\sum j Pr\\\\\\{L_j\\\\\\}$. $E[L] \\\\\\le 2 \\\\\\lceil \\\\\\lg n \\\\\\rceil \\\\\\sum Pr\\\\\\{L_j\\\\\\} + n \\\\\\sum_{j>2\\\\\\\\lceil \\\\\\lg n \\\\\\rceil} Pr\\\\\\{L_j\\\\\\} \\\\\\approx 2 \\\\\\lceil \\\\\\lg n \\\\\\rceil \\\\\\cdot 1 + n \\\\\\cdot (1/n) = O(\\\\\\\\lg n)$. (Simplified argument).\n*Lower bound*: Divide $n$ flips into $n/s$ groups of $s = \\\\\\lfloor (\\\\\\\nlg n)/2 \\\\\\rfloor$ flips. Probability a group is all heads is $1/2^s \\\\\\approx 1/\\\\\\\nsqrt{n}$. Probability no group is all heads is $(1-1/2^s)^{n/s} \\\\\\approx e^{-(n/s)/2^s} \\\\\\approx e^{-\\\\\\\nsqrt{n}/\\\\\\\nlg n} = O(1/n)$. So probability of at least one such streak is $1-O(1/n)$. Longest streak is $\\\\\\\\Omega(\\\\\\\\lg n)$.\n\n### 5.4.4 The online hiring problem\nMust hire exactly one candidate. Decision on candidate $i$ must be made immediately after interview. Cannot go back.\n[[PartI Foundations Algorithms.md#C5.4 ONLINE-MAXIMUM]] strategy: Interview first $k$ candidates and reject them. Then hire the first subsequent candidate better than all first $k$. If no such candidate, hire the $n$-th.\nGoal: Maximize probability of hiring the best candidate.\nLet $S$ be event of success. $S_i$ is success when best is $i$. $Pr\\\\\\{S\\\\\\} = \\\\\\sum_{i=1}^n Pr\\\\\\{S_i\\\\\\}$. $Pr\\\\\\{S_i\\\\\\}=0$ for $i \\\\\\le k$.\nFor $i > k$, $S_i$ occurs if: (1) candidate $i$ is best ($B_i$, $Pr\\\\\\{B_i\\\\\\}=1/n$) AND (2) algorithm picks $i$. Algorithm picks $i$ if $i$ is better than $M(k)$ (best of first $k$) AND candidates $k+1, \\\\\\dots, i-1$ are not chosen. This happens if $M(k)$ is also $M(i-1)$ (best of first $i-1$ is among first $k$). Let this be event $O_i$. $Pr\\\\\\{O_i\\\\\\} = k/(i-1)$.\n$Pr\\\\\\{S_i\\\\\\} = Pr\\\\\\{B_i\\\\\\} Pr\\\\\\{O_i\\\\\\} = (1/n) \\\\\\cdot k/(i-1)$ for $i > k$.\n$Pr\\\\\\{S\\\\\\} = \\\\\\sum_{i=k+1}^n \\\\\\frac{k}{n(i-1)} = \\\\\\frac{k}{n} \\\\\\sum_{j=k}^{n-1} \\\\\\frac{1}{j}$.\nSum is $H_{n-1} - H_{k-1} \\\\\\approx \\\\\\ln((n-1)/(k-1))$. So $Pr\\\\\\{S\\\\\\} \\\\\\approx \\\\\\frac{k}{n} \\\\\\ln(\\\\\\\\frac{n}{k})$.\nTo maximize, let $x=k/n$. Maximize $x \\\\\\ln(1/x) = -x \\\\\\ln x$. Derivative is $-\\\\\\(1 + \\\\\\ln x)$. Set to 0: $\\\\\\\\ln x = -1$, $x=1/e$. So $k \\\\\\approx n/e$.\nSuccess probability is $\\\\\\\\approx (1/e) \\\\\\ln e = 1/e \\\\\\approx 0.368$.\n"
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartVII Selected Topics Algorithms.md",
                "contents": "# C26 Parallel Algorithms Algorithms\n## 26.1 P-FIB(n)\n```\nP-FIB(n)\n1  if n \u2264 1\n2      return n\n3  else x = spawn P-FIB(n - 1)  // don't wait for subroutine to return\n4       y = P-FIB(n - 2)       // in parallel with spawned subroutine\n5       sync                   // wait for spawned subroutine to finish\n6       return x + y\n```\n\n## 26.1 P-MAT-VEC(A, x, y, n)\n```\nP-MAT-VEC(A, x, y, n)\n1  parallel for i = 1 to n  // parallel loop\n2      for j = 1 to n        // serial loop\n3          y_i = y_i + a_ij * x_j\n```\n\n## 26.1 P-MAT-VEC-RECURSIVE(A, x, y, n, i, i')\n```\nP-MAT-VEC-RECURSIVE(A, x, y, n, i, i')\n1  if i == i'  // just one iteration to do?\n2      for j = 1 to n  // mimic P-MAT-VEC serial loop\n3          y_i = y_i + a_ij * x_j\n4  else mid = floor((i + i') / 2)  // parallel divide-and-conquer\n5       spawn P-MAT-VEC-RECURSIVE(A, x, y, n, i, mid)\n6       P-MAT-VEC-RECURSIVE(A, x, y, n, mid + 1, i')\n7       sync\n```\n\n## 26.1 RACE-EXAMPLE()\n```\nRACE-EXAMPLE()\n1  x = 0\n2  parallel for i = 1 to 2\n3      x = x + 1  // determinacy race\n4  print x\n```\n\n## 26.1 P-MAT-VEC-WRONG(A, x, y, n)\n```\nP-MAT-VEC-WRONG(A, x, y, n)\n1  parallel for i = 1 to n\n2      parallel for j = 1 to n\n3          y_i = y_i + a_ij * x_j  // determinacy race\n```\n\n## 26.1 P-TRANSPOSE(A, n) (Exercise 26.1-8)\n```\nP-TRANSPOSE(A, n)\n1  parallel for j = 2 to n\n2      parallel for i = 1 to j - 1\n3          exchange a_ij with a_ji\n```\n\n## 26.2 P-MATRIX-MULTIPLY(A, B, C, n)\n```\nP-MATRIX-MULTIPLY(A, B, C, n)\n1  parallel for i = 1 to n  // compute entries in each of n rows\n2      parallel for j = 1 to n  // compute n entries in row i\n3          for k = 1 to n\n4              c_ij = c_ij + a_ik * b_kj  // add in another term of equation (4.1)\n```\n\n## 26.2 P-MATRIX-MULTIPLY-RECURSIVE(A, B, C, n)\n```\nP-MATRIX-MULTIPLY-RECURSIVE(A, B, C, n)\n1  if n == 1  // just one element in each matrix?\n2      c_11 = c_11 + a_11 * b_11\n3      return\n4  let D be a new n x n matrix  // temporary matrix\n5  parallel for i = 1 to n  // set D = 0\n6      parallel for j = 1 to n\n7          d_ij = 0\n8  partition A, B, C, and D into n/2 x n/2 submatrices\n   A_11, A_12, A_21, A_22; B_11, B_12, B_21, B_22; C_11, C_12, C_21, C_22;\n   and D_11, D_12, D_21, D_22; respectively\n9  spawn P-MATRIX-MULTIPLY-RECURSIVE(A_11, B_11, C_11, n/2)\n10 spawn P-MATRIX-MULTIPLY-RECURSIVE(A_11, B_12, C_12, n/2)\n11 spawn P-MATRIX-MULTIPLY-RECURSIVE(A_21, B_11, C_21, n/2)\n12 spawn P-MATRIX-MULTIPLY-RECURSIVE(A_21, B_12, C_22, n/2)\n13 spawn P-MATRIX-MULTIPLY-RECURSIVE(A_12, B_21, D_11, n/2)\n14 spawn P-MATRIX-MULTIPLY-RECURSIVE(A_12, B_22, D_12, n/2)\n15 spawn P-MATRIX-MULTIPLY-RECURSIVE(A_22, B_21, D_21, n/2)\n16 spawn P-MATRIX-MULTIPLY-RECURSIVE(A_22, B_22, D_22, n/2)\n17 sync  // wait for spawned submatrix products\n18 parallel for i = 1 to n  // update C = C + D\n19     parallel for j = 1 to n\n20         c_ij = c_ij + d_ij\n```\n\n## 26.3 P-MERGE-SORT(A, p, r)\n```\nP-MERGE-SORT(A, p, r)\n1  if p < r  // zero or one element?\n2      q = floor((p + r) / 2)  // midpoint of A[p:r]\n3      // Recursively sort A[p:q] in parallel.\n4      spawn P-MERGE-SORT(A, p, q)\n5      // Recursively sort A[q+1:r] in parallel.\n6      spawn P-MERGE-SORT(A, q + 1, r)\n7      sync  // wait for spawns\n8      // Merge A[p:q] and A[q+1:r] into A[p:r].\n9      P-MERGE(A, p, q, r)\n```\n\n## 26.3 FIND-SPLIT-POINT(A, p, r, x)\n```\nFIND-SPLIT-POINT(A, p, r, x)\n1  low = p  // low end of search range\n2  high = r + 1  // high end of search range\n3  while low < high  // more than one element?\n4      mid = floor((low + high) / 2)  // midpoint of range\n5      if x <= A[mid]  // is answer q <= mid?\n6          high = mid  // narrow search to A[low:mid]\n7      else low = mid + 1  // narrow search to A[mid+1:high]\n8  return low\n```\n\n## 26.3 P-MERGE(A, p, q, r)\n```\nP-MERGE(A, p, q, r)\n1  let B[p:r] be a new array  // allocate scratch array\n2  P-MERGE-AUX(A, p, q, q + 1, r, B, p)  // merge from A into B\n3  parallel for i = p to r  // copy B back to A in parallel\n4      A[i] = B[i]\n```\n\n## 26.3 P-MERGE-AUX(A, p1, r1, p2, r2, B, p3)\n```\nP-MERGE-AUX(A, p1, r1, p2, r2, B, p3)\n1  if p1 > r1 and p2 > r2  // are both subarrays empty?\n2      return\n3  if r1 - p1 < r2 - p2  // second subarray bigger?\n4      exchange p1 with p2  // swap subarray roles\n5      exchange r1 with r2\n6  q1 = floor((p1 + r1) / 2)  // midpoint of A[p1:r1]\n7  x = A[q1]  // median of A[p1:r1] is pivot x\n8  q2 = FIND-SPLIT-POINT(A, p2, r2, x)  // split A[p2:r2] around x\n9  q3 = p3 + (q1 - p1) + (q2 - p2)  // where x belongs in B ...\n10 B[q3] = x  // ... put it there\n11 // Recursively merge A[p1:q1-1] and A[p2:q2-1] into B[p3:q3-1].\n12 spawn P-MERGE-AUX(A, p1, q1 - 1, p2, q2 - 1, B, p3)\n13 // Recursively merge A[q1+1:r1] and A[p2:r2] into B[q3+1:r3].\n14 spawn P-MERGE-AUX(A, q1 + 1, r1, q2, r2, B, q3 + 1)\n15 sync  // wait for spawns\n```\n\n## Problem 26-1 SUM-ARRAYS(A, B, C, n)\n```\nSUM-ARRAYS(A, B, C, n)\n1  parallel for i = 1 to n\n2      C[i] = A[i] + B[i]\n```\n\n## Problem 26-1 SUM-ARRAYS'(A, B, C, n)\n```\nSUM-ARRAYS'(A, B, C, n)\n1  grain-size = ?  // to be determined\n2  r = ceil(n / grain-size)\n3  for k = 0 to r - 1\n4      spawn ADD-SUBARRAY(A, B, C, k * grain-size + 1, min((k + 1) * grain-size, n))\n5  sync\n\nADD-SUBARRAY(A, B, C, i, j)\n1  for k = i to j\n2      C[k] = A[k] + B[k]\n```\n\n# C28 Matrix Operations Algorithms\n## 28.1 LUP-SOLVE(L, U, pi, b, n)\n```\nLUP-SOLVE(L, U, pi, b, n)\n1  let x and y be new vectors of length n\n2  for i = 1 to n\n3      y_i = b_pi[i] - sum_{j=1}^{i-1} l_ij * y_j\n4  for i = n downto 1\n5      x_i = (y_i - sum_{j=i+1}^{n} u_ij * x_j) / u_ii\n6  return x\n```\n\n## 28.1 LU-DECOMPOSITION(A, n)\n```\nLU-DECOMPOSITION(A, n)\n1  let L and U be new n x n matrices\n2  initialize U with 0s below the diagonal\n3  initialize L with 1s on the diagonal and 0s above the diagonal\n4  for k = 1 to n\n5      u_kk = a_kk\n6      for i = k + 1 to n\n7          l_ik = a_ik / a_kk  // a_ik holds v_i\n8          u_ki = a_ki       // a_ki holds w_i\n9      for i = k + 1 to n  // compute the Schur complement ...\n10         for j = k + 1 to n\n11             a_ij = a_ij - l_ik * u_kj  // ... and store it back into A\n12 return L and U\n```\n\n## 28.1 LUP-DECOMPOSITION(A, n)\n```\nLUP-DECOMPOSITION(A, n)\n1  let pi[1:n] be a new array\n2  for i = 1 to n\n3      pi[i] = i  // initialize pi to the identity permutation\n4  for k = 1 to n\n5      p = 0\n6      for i = k to n  // find largest absolute value in column k\n7          if |a_ik| > p\n8              p = |a_ik|\n9              k' = i  // row number of the largest found so far\n10     if p == 0\n11         error \"singular matrix\"\n12     exchange pi[k] with pi[k']\n13     for i = 1 to n\n14         exchange a_ki with a_k'i  // exchange rows k and k'\n15     for i = k + 1 to n\n16         a_ik = a_ik / a_kk\n17         for j = k + 1 to n\n18             a_ij = a_ij - a_ik * a_kj  // compute L and U in place in A\n```\n\n# C29 Linear Programming Algorithms\n\n## Problem 27-2 COMPLETION-TIME-SCHEDULE(S) (from chapter 27)\n```\nCOMPLETION-TIME-SCHEDULE(S)\n1  compute an optimal schedule for the preemptive version of the problem\n2  renumber the tasks so that the completion times in the optimal\n   preemptive schedule are ordered by their completion times\n   C_1^P < C_2^P < ... < C_n^P in SRPT order\n3  greedily schedule the tasks nonpreemptively in the renumbered\n   order a_1, ..., a_n\n4  let C_1, ..., C_n be the completion times of renumbered tasks a_1, ..., a_n\n   in this nonpreemptive schedule\n5  return C_1, ..., C_n\n```\n\n# C30 Polynomials and the FFT Algorithms\n## 30.2 FFT(a, n)\n```\nFFT(a, n)\n1  if n == 1\n2      return a  // DFT of 1 element is the element itself\n3  omega_n = e^(2 * pi * i / n)\n4  omega = 1\n5  a_even = (a_0, a_2, ..., a_{n-2})\n6  a_odd = (a_1, a_3, ..., a_{n-1})\n7  y_even = FFT(a_even, n/2)\n8  y_odd = FFT(a_odd, n/2)\n9  for k = 0 to n/2 - 1  // at this point, omega = omega_n^k\n10     y_k = y_even_k + omega * y_odd_k\n11     y_{k + n/2} = y_even_k - omega * y_odd_k\n12     omega = omega * omega_n\n13 return y\n```\n\n# C31 Number-Theoretic Algorithms Algorithms\n## 31.2 EUCLID(a, b)\n```\nEUCLID(a, b)\n1  if b == 0\n2      return a\n3  else return EUCLID(b, a mod b)\n```\n\n## 31.2 EXTENDED-EUCLID(a, b)\n```\nEXTENDED-EUCLID(a, b)\n1  if b == 0\n2      return (a, 1, 0)\n3  else (d', x', y') = EXTENDED-EUCLID(b, a mod b)\n4       (d, x, y) = (d', y', x' - floor(a/b) * y')\n5       return (d, x, y)\n```\n\n## 31.4 MODULAR-LINEAR-EQUATION-SOLVER(a, b, n)\n```\nMODULAR-LINEAR-EQUATION-SOLVER(a, b, n)\n1  (d, x', y') = EXTENDED-EUCLID(a, n)\n2  if d | b\n3      x_0 = x' * (b/d) mod n\n4      for i = 0 to d - 1\n5          print (x_0 + i * (n/d)) mod n\n6  else print \"no solutions\"\n```\n\n## 31.6 MODULAR-EXPONENTIATION(a, b, n)\n```\nMODULAR-EXPONENTIATION(a, b, n)\n1  if b == 0\n2      return 1\n3  elseif b mod 2 == 0\n4      d = MODULAR-EXPONENTIATION(a, b/2, n)  // b is even\n5      return (d * d) mod n\n6  else d = MODULAR-EXPONENTIATION(a, b - 1, n)  // b is odd\n7      return (a * d) mod n\n```\n\n## 31.8 PSEUDOPRIME(n)\n```\nPSEUDOPRIME(n)\n1  if MODULAR-EXPONENTIATION(2, n - 1, n) != 1 (mod n)\n2      return COMPOSITE  // definitely\n3  else return PRIME       // we hope!\n```\n\n## 31.8 MILLER-RABIN(n, s)\n```\nMILLER-RABIN(n, s)  // n > 2 is odd\n1  for j = 1 to s\n2      a = RANDOM(2, n - 2)\n3      if WITNESS(a, n)\n4          return COMPOSITE  // definitely\n5  return PRIME           // almost surely\n```\n\n## 31.8 WITNESS(a, n)\n```\nWITNESS(a, n)\n1  let t and u be such that t >= 1, u is odd, and n - 1 = 2^t * u\n2  x_0 = MODULAR-EXPONENTIATION(a, u, n)\n3  for i = 1 to t\n4      x_i = x_{i-1}^2 mod n\n5      if x_i == 1 and x_{i-1} != 1 and x_{i-1} != n - 1\n6          return TRUE  // found a nontrivial square root of 1\n7  if x_t != 1\n8      return TRUE  // composite, as in PSEUDOPRIME\n9  return FALSE\n```\n\n# C32 String Matching Algorithms\n## 32.1 NAIVE-STRING-MATCHER(T, P, n, m)\n```\nNAIVE-STRING-MATCHER(T, P, n, m)\n1  for s = 0 to n - m\n2      if P[1:m] == T[s+1 : s+m]\n3          print \"Pattern occurs with shift\" s\n```\n\n## 32.2 RABIN-KARP-MATCHER(T, P, n, m, d, q)\n```\nRABIN-KARP-MATCHER(T, P, n, m, d, q)\n1  h = d^(m-1) mod q\n2  p = 0\n3  t_0 = 0\n4  for i = 1 to m  // preprocessing\n5      p = (d*p + P[i]) mod q\n6      t_0 = (d*t_0 + T[i]) mod q\n7  for s = 0 to n - m  // matching - try all possible shifts\n8      if p == t_s  // a hit?\n9          if P[1:m] == T[s+1 : s+m]  // valid shift?\n10             print \"Pattern occurs with shift\" s\n11     if s < n - m\n12         t_{s+1} = (d*(t_s - T[s+1]*h) + T[s+m+1]) mod q\n```\n\n## 32.3 FINITE-AUTOMATON-MATCHER(T, delta, n, m)\n```\nFINITE-AUTOMATON-MATCHER(T, delta, n, m)\n1  q = 0\n2  for i = 1 to n\n3      q = delta(q, T[i])\n4      if q == m\n5          print \"Pattern occurs with shift\" i - m\n```\n\n## 32.3 COMPUTE-TRANSITION-FUNCTION(P, Sigma, m)\n```\nCOMPUTE-TRANSITION-FUNCTION(P, Sigma, m)\n1  for q = 0 to m\n2      for each character a in Sigma\n3          k = min(m, q + 1)\n4          while P[1:k] is not a suffix of P[1:q]a\n5              k = k - 1\n6          delta(q, a) = k\n7  return delta\n```\n\n## 32.4 KMP-MATCHER(T, P, n, m)\n```\nKMP-MATCHER(T, P, n, m)\n1  pi = COMPUTE-PREFIX-FUNCTION(P, m)\n2  q = 0  // number of characters matched\n3  for i = 1 to n  // scan the text from left to right\n4      while q > 0 and P[q+1] != T[i]\n5          q = pi[q]  // next character does not match\n6      if P[q+1] == T[i]\n7          q = q + 1  // next character matches\n8      if q == m  // is all of P matched?\n9          print \"Pattern occurs with shift\" i - m\n10         q = pi[q]  // look for the next match\n```\n\n## 32.4 COMPUTE-PREFIX-FUNCTION(P, m)\n```\nCOMPUTE-PREFIX-FUNCTION(P, m)\n1  let pi[1:m] be a new array\n2  pi[1] = 0\n3  k = 0\n4  for q = 2 to m\n5      while k > 0 and P[k+1] != P[q]\n6          k = pi[k]\n7      if P[k+1] == P[q]\n8          k = k + 1\n9      pi[q] = k\n10 return pi\n```\n\n## 32.5 COMPUTE-SUFFIX-ARRAY(T, n)\n```\nCOMPUTE-SUFFIX-ARRAY(T, n)\n1  allocate arrays substr-rank[1:n], rank[1:n], and SA[1:n]\n2  for i = 1 to n\n3      substr-rank[i].left-rank = ord(T[i])\n4      if i < n\n5          substr-rank[i].right-rank = ord(T[i+1])\n6      else substr-rank[i].right-rank = 0\n7      substr-rank[i].index = i\n8  sort the array substr-rank into monotonically increasing order based\n   on the left-rank attributes, using the right-rank attributes to break ties;\n   if still a tie, the order does not matter\n9  l = 2\n10 while l < n\n11     MAKE-RANKS(substr-rank, rank, n)\n12     for i = 1 to n\n13         substr-rank[i].left-rank = rank[i]\n14         if i + l <= n\n15             substr-rank[i].right-rank = rank[i+l]\n16         else substr-rank[i].right-rank = 0\n17         substr-rank[i].index = i\n18     sort the array substr-rank into monotonically increasing order based\n        on the left-rank attributes, using the right-rank attributes\n        to break ties; if still a tie, the order does not matter\n19     l = 2*l\n20 for i = 1 to n\n21     SA[i] = substr-rank[i].index\n22 return SA\n```\n\n## 32.5 MAKE-RANKS(substr-rank, rank, n)\n```\nMAKE-RANKS(substr-rank, rank, n)\n1  r = 1\n2  rank[substr-rank[1].index] = r\n3  for i = 2 to n\n4      if substr-rank[i].left-rank != substr-rank[i-1].left-rank\n         or substr-rank[i].right-rank != substr-rank[i-1].right-rank\n5          r = r + 1\n6      rank[substr-rank[i].index] = r\n```\n\n## 32.5 COMPUTE-LCP(T, SA, n)\n```\nCOMPUTE-LCP(T, SA, n)\n1  allocate arrays rank[1:n] and LCP[1:n]\n2  for i = 1 to n\n3      rank[SA[i]] = i  // by definition\n4  LCP[1] = 0  // also by definition\n5  l = 0  // initialize length of LCP\n6  for i = 1 to n\n7      if rank[i] > 1\n8          j = SA[rank[i] - 1]  // T[j:] precedes T[i:] lexicographically\n9          m = max(i, j)\n10         while m + l <= n and T[i+l] == T[j+l]\n11             l = l + 1  // next character is in common prefix\n12         LCP[rank[i]] = l  // length of LCP of T[j:] and T[i:]\n13         if l > 0\n14             l = l - 1  // peel off first character of common prefix\n15 return LCP\n```\n\n## Problem 32-1 REPETITION-MATCHER(T, P, n, m)\n```\nREPETITION-MATCHER(T, P, n, m)\n1  k = 1 + rho*(P)\n2  q = 0\n3  s = 0\n4  while s <= n - m\n5      if T[s+q+1] == P[q+1]\n6          q = q + 1\n7          if q == m\n8              print \"Pattern occurs with shift\" s\n9      if q == m or T[s+q+1] != P[q+1]\n10         s = s + max(1, floor(q/k))\n11         q = 0\n```\n\n# C33 Machine-Learning Algorithms Algorithms\n## 33.2 WEIGHTED-MAJORITY(E, T, n, gamma)\n```\nWEIGHTED-MAJORITY(E, T, n, gamma)\n1  for i = 1 to n\n2      w_i^(1) = 1  // trust each expert equally\n3  for t = 1 to T\n4      each expert E_i in E makes a prediction q_i^(t)\n5      U = {E_i : q_i^(t) == 1}  // experts who predicted 1\n6      upweight^(t) = sum_{i:E_i in U} w_i^(t)  // sum of weights of who predicted 1\n7      D = {E_i : q_i^(t) == 0}  // experts who predicted 0\n8      downweight^(t) = sum_{i:E_i in D} w_i^(t)  // sum of weights of who predicted 0\n9      if upweight^(t) >= downweight^(t)\n10         p^(t) = 1  // algorithm predicts 1\n11     else p^(t) = 0  // algorithm predicts 0\n12     outcome o^(t) is revealed\n13     // If p^(t) != o^(t), the algorithm made a mistake.\n14     for i = 1 to n\n15         if q_i^(t) != o^(t)  // if expert E_i made a mistake ...\n16             w_i^(t+1) = (1 - gamma) * w_i^(t)  // ... then decrease that expert's weight\n17         else w_i^(t+1) = w_i^(t)\n18 return p^(t)\n```\n\n## 33.3 GRADIENT-DESCENT(f, x_0, gamma, T)\n```\nGRADIENT-DESCENT(f, x^(0), gamma, T)\n1  sum = 0  // n-dimensional vector, initially all 0\n2  for t = 0 to T - 1\n3      sum = sum + x^(t)  // add each of n dimensions into sum\n4      x^(t+1) = x^(t) - gamma * (nabla f)(x^(t))  // (nabla f)(x^(t)), x^(t+1) are n-dimensional\n5  x-avg = sum / T  // divide each of n dimensions by T\n6  return x-avg\n```\n\n## 33.3 GRADIENT-DESCENT-CONSTRAINED(f, x_0, gamma, T, K)\n```\nGRADIENT-DESCENT-CONSTRAINED(f, x^(0), gamma, T, K)\n1  sum = 0  // n-dimensional vector, initially all 0\n2  for t = 0 to T - 1\n3      sum = sum + x^(t)  // add each of n dimensions into sum\n4      x'^(t+1) = x^(t) - gamma * (nabla f)(x^(t))  // (nabla f)(x^(t)), x'^(t+1) are n-dimensional\n5      x^(t+1) = Pi_K(x'^(t+1))  // project onto K\n6  x-avg = sum / T  // divide each of n dimensions by T\n7  return x-avg\n```\n\n# C35 Approximation Algorithms Algorithms\n## 35.1 APPROX-VERTEX-COVER(G)\n```\nAPPROX-VERTEX-COVER(G)\n1  C = {}\n2  E' = G.E\n3  while E' != {}\n4      let (u,v) be an arbitrary edge of E'\n5      C = C U {u,v}\n6      remove from E' edge (u,v) and every edge incident on either u or v\n7  return C\n```\n\n## 35.2 APPROX-TSP-TOUR(G, c)\n```\nAPPROX-TSP-TOUR(G, c)\n1  select a vertex r in G.V to be a \"root\" vertex\n2  compute a minimum spanning tree T for G from root r\n   using MST-PRIM(G, c, r)\n3  let H be a list of vertices, ordered according to when they are first visited\n   in a preorder tree walk of T\n4  return the hamiltonian cycle H\n```\n\n## 35.3 GREEDY-SET-COVER(X, F)\n```\nGREEDY-SET-COVER(X, F)\n1  U_0 = X\n2  C_bar = {}\n3  i = 0\n4  while U_i != {}\n5      select S in F that maximizes |S intersect U_i|\n6      U_{i+1} = U_i - S\n7      C_bar = C_bar U {S}\n8      i = i + 1\n9  return C_bar\n```\n\n## 35.4 APPROX-MIN-WEIGHT-VC(G, w)\n```\nAPPROX-MIN-WEIGHT-VC(G, w)\n1  C_bar = {}\n2  compute x_bar, an optimal solution to the linear-programming relaxation\n   in lines (35.15)-(35.18)\n3  for each vertex v in V\n4      if x_bar(v) >= 1/2\n5          C_bar = C_bar U {v}\n6  return C_bar\n```\n\n## 35.5 EXACT-SUBSET-SUM(S, n, t)\n```\nEXACT-SUBSET-SUM(S, n, t)\n1  L_0 = <0>\n2  for i = 1 to n\n3      L_i = MERGE-LISTS(L_{i-1}, L_{i-1} + x_i)\n4      remove from L_i every element that is greater than t\n5  return the largest element in L_n\n```\n\n## 35.5 TRIM(L, delta)\n```\nTRIM(L, delta)\n1  let m be the length of L\n2  L' = <y_1>\n3  last = y_1\n4  for i = 2 to m\n5      if y_i > last * (1 + delta)  // y_i >= last because L is sorted\n6          append y_i onto the end of L'\n7          last = y_i\n8  return L'\n```\n\n## 35.5 APPROX-SUBSET-SUM(S, n, t, epsilon)\n```\nAPPROX-SUBSET-SUM(S, n, t, epsilon)\n1  L_0 = <0>\n2  for i = 1 to n\n3      L_i = MERGE-LISTS(L_{i-1}, L_{i-1} + x_i)\n4      L_i = TRIM(L_i, epsilon / (2*n))\n5      remove from L_i every element that is greater than t\n6  let z* be the largest value in L_n\n7  return z*\n```\n\n## Problem 32-1 REPETITION-MATCHER (from chapter 32 problems)\n```\nREPETITION-MATCHER(T, P, n, m)\n1  k = 1 + rho*(P)\n2  q = 0\n3  s = 0\n4  while s <= n - m\n5      if T[s+q+1] == P[q+1]\n6          q = q + 1\n7          if q == m\n8              print \"Pattern occurs with shift\" s\n9      if q == m or T[s+q+1] != P[q+1]\n10         s = s + max(1, floor(q/k))\n11         q = 0\n```"
            },
            {
                "file": "PartVII Selected Topics/C26 Parallel Algorithms.md",
                "contents": "# 26 Parallel Algorithms\nThis part contains a selection of algorithmic topics that extend and complement earlier material. Some chapters introduce new models of computation such as circuits or parallel computers. This chapter extends our algorithmic model to encompass **parallel algorithms**, where multiple instructions can execute simultaneously. We explore the model of task-parallel algorithms, focusing on fork-join parallel algorithms.\n\nParallel computers, with multiple processing units (multicores), are ubiquitous. Programming multicores can be approached with **thread parallelism**, using software abstractions called **threads** that share common memory. Each thread maintains its own program counter. The operating system loads threads onto processing cores.\n\n## 26.1 The basics of fork-join parallelism\nProgramming shared-memory parallel computers using threads can be difficult and error-prone due to complexities in dynamically partitioning work and implementing load-balancing schedulers. **Task-parallel platforms** provide a software layer on top of threads to manage multicore processors. **Task-parallel programming** allows specifying parallelism in a \"processor-oblivious\" fashion. The platform's scheduler automatically load-balances tasks.\n\n**Fork-join parallelism** typically involves two linguistic features: **spawning** and **parallel loops**. Spawning allows a subroutine to be \"forked,\" meaning the caller can continue executing while the spawned subroutine computes its result. A parallel loop allows multiple iterations to execute concurrently.\n\n**Parallel keywords**\nWe introduce three keywords to pseudocode: `spawn`, `sync`, and `parallel`.\n- `spawn`: Precedes a procedure call. The parent procedure may continue in parallel with the spawned child subroutine.\n- `sync`: Indicates the procedure must wait for all its spawned children to finish before proceeding. Every procedure implicitly executes a `sync` before it returns.\n- `parallel`: Precedes a `for` loop keyword to indicate that the loop's iterations may run in parallel.\n\nThe **serial projection** of a parallel algorithm is the serial algorithm resulting from omitting these parallel keywords. This property ensures that parallel pseudocode always has a corresponding serial version for the same problem.\n\nA Fibonacci example, `P-FIB(n)`, demonstrates these keywords:\n```\nP-FIB(n)\n1  if n \u2264 1\n2      return n\n3  else x = spawn P-FIB(n - 1)\n4       y = P-FIB(n - 2)\n5       sync\n6       return x + y\n```\n[[PartVII Selected Topics Algorithms.md#C26 P-FIB(n)]]\n\n**A graph model for parallel execution**\nA parallel computation's execution can be modeled as a directed acyclic graph $G = (V, E)$, called a **(parallel) trace** or **computation dag**. Vertices are instructions, and edges represent dependencies. A chain of instructions without parallel or procedural control is grouped into a single **strand**. Edges represent dependencies between strands induced by parallel and procedural control (calls, spawns, syncs, returns).\n\nAn **ideal parallel computer** consists of a set of processors and a **sequentially consistent** shared memory. Sequential consistency means memory behaves as if instructions from different processors are interleaved in some global linear order that preserves individual processor orders. For task-parallel computations, this means instructions execute as if in a topological sort of the trace.\n\n**Performance measures**\n**Work/span analysis** uses two metrics:\n-   **Work** ($T_1$): Total time to execute on one processor. Sum of times for all strands.\n-   **Span** ($T_\text{1}$ or $T_\text{\\infty}$): Fastest possible time on an unlimited number of processors. Weight of the **critical path** (longest weighted path) in the trace.\n\nRunning time $T_P$ on $P$ processors has lower bounds:\n-   **Work law**: $T_P \text{\\ge} T_1 / P$.\n-   **Span law**: $T_P \text{\\ge} T_\text{\\infty}$.\n\n**Speedup** on $P$ processors is $T_1 / T_P$. It's at most $P$. **Linear speedup** is when $T_1/T_P = \text{\\Theta}(P)$. **Perfect linear speedup** is $T_1/T_P = P$.\n**Parallelism** is the ratio $T_1 / T_\text{\\infty}$, representing average work per step on the critical path or maximum possible speedup.\n**Parallel slackness** is $(T_1 / T_\text{\\infty}) / P = T_1 / (P T_\text{\\infty})$. If slackness < 1, perfect linear speedup is impossible.\n\n**Scheduling**\nA runtime scheduler maps strands to processors. **Greedy schedulers** assign as many ready strands as possible in each time step. A step is:\n-   **Complete step**: At least $P$ strands are ready; scheduler assigns $P$ of them.\n-   **Incomplete step**: Fewer than $P$ strands are ready; scheduler assigns all of them.\n\nTheorem 26.1: A greedy scheduler executes a task-parallel computation in time $T_P \text{\\le} T_1/P + T_\text{\\infty}$.\nCorollary 26.2: A greedy scheduler is within a factor of 2 of optimal.\nCorollary 26.3: If $P \text{\\ll} T_1/T_\text{\\infty}$ (high slackness), then $T_P \text{\\approx} T_1/P$ (near-perfect linear speedup).\n\n**Analyzing parallel algorithms**\n-   Work $T_1(n)$ is the running time of the serial projection.\n-   Span $T_\text{\\infty}(n)$: For series composition, spans add. For parallel composition, span is the maximum of the parallel spans. (See Figure 26.3 of the book).\nFor `P-FIB(n)`: $T_1(n) = \text{\\Theta}(\\phi^n)$. The span recurrence is $T_\text{\\infty}(n) = \\max\\{T_\text{\\infty}(n-1), T_\text{\\infty}(n-2)\\} + \text{\\Theta}(1) = T_\text{\\infty}(n-1) + \text{\\Theta}(1)$, so $T_\text{\\infty}(n) = \text{\\Theta}(n)$. Parallelism is $\text{\\Theta}(\\phi^n / n)$.\n\n**Parallel loops**\nThe `parallel for` keyword indicates iterations may run in parallel. Compilers implement `parallel for` using recursive spawning.\nExample: `P-MAT-VEC` [[PartVII Selected Topics Algorithms.md#C26 P-MAT-VEC(A, x, y, n)]]. It can be implemented by `P-MAT-VEC-RECURSIVE` [[PartVII Selected Topics Algorithms.md#C26 P-MAT-VEC-RECURSIVE(A, x, y, n, i, i')]].\nWork of `P-MAT-VEC` is $\text{\\Theta}(n^2)$. Span for a `parallel for` loop with $n$ iterations, where iteration $i$ has span $\\text{iter}_\text{\\infty}(i)$, is $T_\text{\\infty}(n) = \text{\\Theta}(\\lg n) + \\max_{1 \text{\\le} i \text{\\le} n} \\{\\text{iter}_\text{\\infty}(i)\\}$. For `P-MAT-VEC`, the outer `parallel for` has $\text{\\Theta}(\\lg n)$ span for control, and each inner serial `for` loop has $\text{\\Theta}(n)$ span. Thus, $T_\text{\\infty}(n) = \text{\\Theta}(n)$. Parallelism is $\text{\\Theta}(n^2/n) = \text{\\Theta}(n)$.\n\n**Race conditions**\nA parallel algorithm is **deterministic** if it always produces the same result on the same input, regardless of scheduling. It's **nondeterministic** if its behavior varies. A **determinacy race** occurs when two logically parallel instructions access the same memory location, and at least one modifies it. Example: `RACE-EXAMPLE` [[PartVII Selected Topics Algorithms.md#C26 RACE-EXAMPLE()]].\nIncrementing $x$ (e.g., $x = x+1$) is not atomic: load $x$, increment register, store $x$. Interleaving these can lead to incorrect results.\nTo ensure determinism, parallel strands should be **mutually noninterfering**: they only read, and do not modify, any memory locations accessed by both.\nExample of faulty parallelization: `P-MAT-VEC-WRONG` [[PartVII Selected Topics Algorithms.md#C26 P-MAT-VEC-WRONG(A, x, y, n)]] (parallelizing inner loop leads to races on $y_i$).\n\n**A chess lesson**\nWork/span analysis can be superior to measured running times for extrapolating scalability. An optimization that reduced $T_{32}$ from 65s to 40s for a chess program was abandoned because work/span analysis showed it would be slower on 512 processors ($T_{512}$ from 5s to 10s). Original: $T_1=2048, T_\text{\\infty}=1$. Optimized: $T'_1=1024, T'_\text{\\infty}=8$. Using $T_P \text{\\approx} T_1/P + T_\text{\\infty}$.\n\n## 26.2 Parallel matrix multiplication\n**A parallel algorithm for matrix multiplication using parallel loops**\nProcedure `P-MATRIX-MULTIPLY` [[PartVII Selected Topics Algorithms.md#C26 P-MATRIX-MULTIPLY(A, B, C, n)]] parallelizes the two outer loops of standard matrix multiplication.\nWork: $T_1(n) = \text{\\Theta}(n^3)$. Span: The two `parallel for` loops contribute $\text{\\Theta}(\\lg n)$ each for control. The inner serial loop contributes $\text{\\Theta}(n)$. So $T_\text{\\infty}(n) = \text{\\Theta}(\\lg n) + \text{\\Theta}(\\lg n) + \text{\\Theta}(n) = \text{\\Theta}(n)$.\nParallelism: $T_1(n)/T_\text{\\infty}(n) = \text{\\Theta}(n^3/n) = \text{\\Theta}(n^2)$.\n\n**A parallel divide-and-conquer algorithm for matrix multiplication**\nProcedure `P-MATRIX-MULTIPLY-RECURSIVE` [[PartVII Selected Topics Algorithms.md#C26 P-MATRIX-MULTIPLY-RECURSIVE(A, B, C, n)]] parallelizes the recursive matrix multiplication from Section 4.1 using `spawn` for the 8 recursive subproblems. It uses a temporary matrix $D$ to avoid races.\nWork: $M_1(n) = 8 M_1(n/2) + \text{\\Theta}(n^2)$ (for allocation, zeroing $D$, partitioning, and adding $C+D$). Solution $M_1(n) = \text{\\Theta}(n^3)$ by master theorem.\nSpan: $M_\text{\\infty}(n) = M_\text{\\infty}(n/2) + \text{\\Theta}(\\lg n)$ (span of parallelizing loops for zeroing $D$ and adding $C+D$, plus one recursive call since 8 are spawned in parallel). Solution $M_\text{\\infty}(n) = \text{\\Theta}(\\lg^2 n)$ by master theorem.\nParallelism: $M_1(n)/M_\text{\\infty}(n) = \text{\\Theta}(n^3 / \\lg^2 n)$.\n\n**Parallelizing Strassen\u2019s method**\nStrassen's algorithm can be parallelized using spawning for its 7 recursive calls. The matrix additions/subtractions can be parallelized using `parallel for` loops.\nWork: $T_1(n) = 7 T_1(n/2) + \text{\\Theta}(n^2)$, so $T_1(n) = \text{\\Theta}(n^{\\lg 7})$.\nSpan: The 7 recursive calls run in parallel. Each of the 10 matrix additions/subtractions of $n/2 \text{\\times} n/2$ matrices takes $\text{\\Theta}(\\lg n)$ span using doubly nested `parallel for` loops. So, $T_\text{\\infty}(n) = T_\text{\\infty}(n/2) + \text{\\Theta}(\\lg n)$. Solution $T_\text{\\infty}(n) = \text{\\Theta}(\\lg^2 n)$.\nParallelism: $\text{\\Theta}(n^{\\lg 7} / \\lg^2 n)$.\n\n## 26.3 Parallel merge sort\nStandard merge sort is a good candidate for parallelism. `P-MERGE-SORT` [[PartVII Selected Topics Algorithms.md#C26 P-MERGE-SORT(A, p, r)]] spawns the two recursive calls.\nIf we use a serial `MERGE` procedure (span $\text{\\Theta}(n)$), the span of `P-NAIVE-MERGE-SORT` is $T_\text{\\infty}(n) = T_\text{\\infty}(n/2) + \text{\\Theta}(n)$, which gives $T_\text{\\infty}(n) = \text{\\Theta}(n)$. Work is $T_1(n) = \text{\\Theta}(n \\lg n)$. Parallelism is $\text{\\Theta}(\\lg n)$, which is poor.\n\nTo improve, we need a parallel merge procedure. `P-MERGE` [[PartVII Selected Topics Algorithms.md#C26 P-MERGE(A, p, q, r)]] uses an auxiliary procedure `P-MERGE-AUX` [[PartVII Selected Topics Algorithms.md#C26 P-MERGE-AUX(A, p1, r1, p2, r2, B, p3)]].\nThe key idea in `P-MERGE-AUX` is to take the median $x$ of the larger subarray (say $A[p_1:r_1]$), find its split point $q_2$ in the other subarray $A[p_2:r_2]$ using binary search (`FIND-SPLIT-POINT` [[PartVII Selected Topics Algorithms.md#C26 FIND-SPLIT-POINT(A, p, r, x)]]), place $x$ in the output array $B$, and then recursively merge the two lower parts and the two upper parts in parallel.\n\nWork/span analysis of parallel merging (`P-MERGE-AUX`):\n- `FIND-SPLIT-POINT` takes $\text{\\Theta}(\\lg n)$ work and span.\n- The recursive calls operate on at most $3n/4$ elements in the worst case.\n- Span recurrence: $T_\text{\\infty}(n) = T_\text{\\infty}(3n/4) + \text{\\Theta}(\\lg n)$, solution $T_\text{\\infty}(n) = \text{\\Theta}(\\lg^2 n)$.\n- Work recurrence: $T_1(n) = T_1(\text{\\alpha} n) + T_1((1-\text{\\alpha})n) + \text{\\Theta}(\\lg n)$ for $1/4 \text{\\le} \text{\\alpha} \text{\\le} 3/4$. Solution $T_1(n) = \text{\\Theta}(n)$.\nThe `P-MERGE` wrapper copies the result from $B$ back to $A$ using a `parallel for` loop, which takes $\text{\\Theta}(\\lg n)$ span and $\text{\\Theta}(n)$ work. So, overall span for `P-MERGE` is $\text{\\Theta}(\\lg^2 n)$ and work is $\text{\\Theta}(n)$.\n\nAnalysis of `P-MERGE-SORT` with parallel merge:\n- Work: $T_1(n) = 2 T_1(n/2) + \text{\\Theta}(n)$, so $T_1(n) = \text{\\Theta}(n \\lg n)$.\n- Span: $T_\text{\\infty}(n) = T_\text{\\infty}(n/2) + \text{\\Theta}(\\lg^2 n)$. Using master theorem case 2 (or by iterating), $T_\text{\\infty}(n) = \text{\\Theta}(\\lg^3 n)$.\n- Parallelism: $\text{\\Theta}(n \\lg n / \\lg^3 n) = \text{\\Theta}(n / \\lg^2 n)$, which is much better."
            },
            {
                "file": "PartVII Selected Topics/C27 Online Algorithms.md",
                "contents": "# 27 Online Algorithms\nMost problems assume entire input is available beforehand. Online algorithms receive input progressively. This chapter adopts a worst-case approach, designing online algorithms that guarantee solution quality for all future inputs, using **competitive analysis**.\n\n## 27.1 Waiting for an elevator\nProblem: Decide whether to wait for an elevator (arrives in $m$ minutes, travel 1 min) or take stairs (takes $k$ minutes). Elevator arrival time $m$ is unknown, but $0 \\le m \\le B-1$.\nSeer (optimal offline algorithm): Spends $t^*(m)$ time. If $m \\le k-1$, seer waits, total $m+1$. If $m \\ge k$, seer takes stairs, total $k$.\nAn online algorithm A produces solution $A(I)$ for input $I$. Seer produces $F(I)$. The **competitive ratio** for a minimization problem is $\\max_I \\{A(I)/F(I)\\}$. An algorithm is **c-competitive** if its ratio is $c$.\n\nStrategies:\n1.  \"Always take the stairs\": Cost is $k$. Competitive ratio is $k / t^*(m)$. Max occurs when $m=0$, $t^*(0)=1$. Ratio is $k$.\n2.  \"Always take the elevator\": Cost is $m+1$. Competitive ratio is $(m+1) / t^*(m)$. Max occurs when $m=B-1$ and $t^*(B-1)=k$ (assuming $B-1 \\ge k$). Ratio is $B/k$.\n3.  \"Hedging strategy\": Wait $k$ minutes, then take stairs if elevator hasn't arrived. Cost $h(m)$:\n    - If $m \\le k$: $h(m) = m+1$ (take elevator).\n    - If $m > k$: $h(m) = k+k = 2k$ (wait $k$, then climb $k$).\n    Competitive ratio: $\\max_m \\{h(m)/t^*(m)\\}$.\n    - If $m \\le k-1$: Ratio is $(m+1)/(m+1) = 1$.\n    - If $m = k$: Ratio is $(k+1)/k$ if we still take elevator (book implies if $m \\le k$, take elevator), or $2k/k=2$ if we start stairs.\n      If $m=k$: $t^*(k)=k$. $h(k)=k+1$ (if elevator is taken). Ratio $(k+1)/k$. Or $h(k)=2k$ (if stairs taken after waiting $k$). Ratio $2k/k=2$.\n      The book states $h(m)=2k$ if $m>k$. And $h(m)=m+1$ if $m \\le k$.\n      The maximum ratio terms become: $1, 2/2, ..., k/k, (k+1)/k, (k+1)/k, ... (B-1+1)/k$ if $t^*(m)=k$ for $m \\ge k$.\n      The book's version of hedging means wait for time $p$. If elevator arrives at $m \\le p$, cost $m+1$. If $m > p$, cost $p+k$. If $p=k$: $h(m) = m+1$ if $m \\le k$, $h(m)=2k$ if $m > k$. Ratio terms: $1, ..., (k+1)/k, 2k/k, ..., 2k/k$. Max is $2$. This is independent of $k$ and $B$.\nThis illustrates guarding against any possible worst case.\n\n## 27.2 Maintaining a search list\nGiven a doubly linked list $L$ of $n$ elements. Cost to `LIST-SEARCH(L, x_i)` is $r_L(x_i)$, its rank. Cost to swap adjacent elements is 1. Goal: minimize total cost of searches + swaps.\n**MOVE-TO-FRONT** heuristic: After searching for $x$ at rank $r_L(x)$, move $x$ to front. Cost: $r_L(x)$ for search + $(r_L(x)-1)$ for swaps = $2r_L(x)-1$.\n\nFORESEE is a hypothetical optimal offline algorithm. $L^M_i$ is list for MOVE-TO-FRONT after $i$-th search, $L^F_i$ for FORESEE. $c^M_i, c^F_i$ are costs. If $i$-th search is for $x$ and FORESEE performs $t_i$ swaps:\n$c^M_i = 2r_{L^M_{i-1}}(x) - 1$\n$c^F_i = r_{L^F_{i-1}}(x) + t_i$\n\nAn **inversion** is a pair $(a,b)$ where $a$ is before $b$ in one list, but $b$ is before $a$ in another. $I(L, L')$ is inversion count.\nSubsets of elements relative to $x$ before $i$-th search:\n- $BB$: elements before $x$ in both $L^M_{i-1}$ and $L^F_{i-1}$.\n- $BA$: elements before $x$ in $L^M_{i-1}$ but after $x$ in $L^F_{i-1}$.\n- $AB$: elements after $x$ in $L^M_{i-1}$ but before $x$ in $L^F_{i-1}$.\n$r_{L^M_{i-1}}(x) = |BB| + |BA| + 1$\n$r_{L^F_{i-1}}(x) = |BB| + |AB| + 1$\n\nMoving $x$ to front in $L^M_{i-1}$ changes inversion count $I(L^M, L^F_{i-1})$. For each $y \\in BB$, $x$ moves before $y$, creating an inversion. For each $z \\in BA$, $x$ moves before $z$, resolving an inversion.\nChange in inversions: $I(L^M_i, L^F_{i-1}) - I(L^M_{i-1}, L^F_{i-1}) = |BB| - |BA|$.\n\nTheorem 27.1: Algorithm MOVE-TO-FRONT has a competitive ratio of 4.\nProof uses potential function $\\Phi_i = 2 I(L^M_i, L^F_i)$. Amortized cost $\\hat{c}^M_i = c^M_i + \\Phi_i - \\Phi_{i-1}$.\nAfter MOVE-TO-FRONT moves $x$: potential increases by $2(|BB| - |BA|)$.\nWhen FORESEE performs $t_i$ swaps: potential changes by at most $2t_i$.\nSo, $\\Phi_i - \\Phi_{i-1} \\le 2(|BB| - |BA|) + 2t_i$.\n$\\"
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartIII Data Structures Algorithms.md",
                "contents": "# Part III Data Structures Algorithms\n\n# C10 Elementary Data Structures Algorithms\n\n## 10.1 STACK-EMPTY(S)\n```\nSTACK-EMPTY(S)\n1  if S.top == 0\n2      return TRUE\n3  else return FALSE\n```\n\n## 10.1 PUSH(S, x)\n```\nPUSH(S, x)\n1  if S.top == S.size\n2      error \"overflow\"\n3  else S.top = S.top + 1\n4       S[S.top] = x\n```\n\n## 10.1 POP(S)\n```\nPOP(S)\n1  if STACK-EMPTY(S)\n2      error \"underflow\"\n3  else S.top = S.top - 1\n4       return S[S.top + 1]\n```\n\n## 10.1 ENQUEUE(Q, x)\n```\nENQUEUE(Q, x)\n1  Q[Q.tail] = x\n2  if Q.tail == Q.size\n3      Q.tail = 1\n4  else Q.tail = Q.tail + 1\n```\n\n## 10.1 DEQUEUE(Q)\n```\nDEQUEUE(Q)\n1  x = Q[Q.head]\n2  if Q.head == Q.size\n3      Q.head = 1\n4  else Q.head = Q.head + 1\n5  return x\n```\n\n## 10.2 LIST-SEARCH(L, k)\n```\nLIST-SEARCH(L, k)\n1  x = L.head\n2  while x != NIL and x.key != k\n3      x = x.next\n4  return x\n```\n\n## 10.2 LIST-PREPEND(L, x)\n```\nLIST-PREPEND(L, x)\n1  x.next = L.head\n2  x.prev = NIL\n3  if L.head != NIL\n4      L.head.prev = x\n5  L.head = x\n```\n\n## 10.2 LIST-INSERT(x, y)\n```\nLIST-INSERT(x, y)\n1  x.next = y.next\n2  x.prev = y\n3  if y.next != NIL\n4      y.next.prev = x\n5  y.next = x\n```\n\n## 10.2 LIST-DELETE(L, x)\n```\nLIST-DELETE(L, x)\n1  if x.prev != NIL\n2      x.prev.next = x.next\n3  else L.head = x.next\n4  if x.next != NIL\n5      x.next.prev = x.prev\n```\n\n## 10.2 LIST-DELETE'(x) (Sentinel)\n```\nLIST-DELETE'(x)\n1  x.prev.next = x.next\n2  x.next.prev = x.prev\n```\n\n## 10.2 LIST-INSERT'(x, y) (Sentinel)\n```\nLIST-INSERT'(x, y)\n1  x.next = y.next\n2  x.prev = y\n3  y.next.prev = x\n4  y.next = x\n```\n\n## 10.2 LIST-SEARCH'(L, k) (Sentinel)\n```\nLIST-SEARCH'(L, k)\n1  L.nil.key = k  // store the key in the sentinel to guarantee it is in list\n2  x = L.nil.next // start at the head of the list\n3  while x.key != k\n4      x = x.next\n5  if x == L.nil    // found k in the sentinel\n6      return NIL   // k was not really in the list\n7  else return x    // found k in element x\n```\n\n# C11 Hash Tables Algorithms\n\n## 11.1 DIRECT-ADDRESS-SEARCH(T, k)\n```\nDIRECT-ADDRESS-SEARCH(T, k)\n1  return T[k]\n```\n\n## 11.1 DIRECT-ADDRESS-INSERT(T, x)\n```\nDIRECT-ADDRESS-INSERT(T, x)\n1  T[x.key] = x\n```\n\n## 11.1 DIRECT-ADDRESS-DELETE(T, x)\n```\nDIRECT-ADDRESS-DELETE(T, x)\n1  T[x.key] = NIL\n```\n\n## 11.2 CHAINED-HASH-INSERT(T, x)\n```\nCHAINED-HASH-INSERT(T, x)\n1  LIST-PREPEND(T[h(x.key)], x)\n```\n\n## 11.2 CHAINED-HASH-SEARCH(T, k)\n```\nCHAINED-HASH-SEARCH(T, k)\n1  return LIST-SEARCH(T[h(k)], k)\n```\n\n## 11.2 CHAINED-HASH-DELETE(T, x)\n```\nCHAINED-HASH-DELETE(T, x)\n1  LIST-DELETE(T[h(x.key)], x)\n```\n\n## 11.4 HASH-INSERT(T, k)\n```\nHASH-INSERT(T, k)\n1  i = 0\n2  repeat\n3      q = h(k, i)\n4      if T[q] == NIL\n5          T[q] = k\n6          return q\n7      else i = i + 1\n8  until i == m\n9  error \"hash table overflow\"\n```\n\n## 11.4 HASH-SEARCH(T, k)\n```\nHASH-SEARCH(T, k)\n1  i = 0\n2  repeat\n3      q = h(k, i)\n4      if T[q] == k\n5          return q\n6      i = i + 1\n7  until T[q] == NIL or i == m\n8  return NIL\n```\n\n## 11.5 LINEAR-PROBING-HASH-DELETE(T, q)\n```\nLINEAR-PROBING-HASH-DELETE(T, q)\n1  while TRUE\n2      T[q] = NIL                            // make slot q empty\n3      q' = q                                // starting point for search\n4      repeat\n5          q' = (q' + 1) mod m               // next slot number with linear probing\n6          k' = T[q']                        // next key to try to move\n7          if k' == NIL\n8              return                        // return when an empty slot is found\n9      until g(k', q) < g(k', q')             // was empty slot q probed before q'?\n10     T[q] = k'                             // move k' into slot q\n11     q = q'                                // free up slot q'\n```\n\n## 11.5 WEE(k, a, b, t, r, m)\n```\nWEE(k, a, b, t, r, m)\n1  u = ceil(t/w)\n2  <k1, k2, ..., ku> = chop(k)\n3  q = b\n4  for i = 1 to u\n5      q = f^(r)_(a+2t)(ki + q)\n6  return q mod m\n```\n\n## Problem 10-3 COMPACT-LIST-SEARCH(key, next, head, n, k)\n```\nCOMPACT-LIST-SEARCH(key, next, head, n, k)\n1  i = head\n2  while i != NIL and key[i] < k\n3      j = RANDOM(1, n)\n4      if key[i] < key[j] and key[j] <= k\n5          i = j\n6      if key[i] == k\n7          return i\n8      i = next[i]\n9  if i == NIL or key[i] > k\n10     return NIL\n11 else return i\n```\n\n## Problem 10-3 COMPACT-LIST-SEARCH'(key, next, head, n, k, t)\n```\nCOMPACT-LIST-SEARCH'(key, next, head, n, k, t)\n1  i = head\n2  for q = 1 to t\n3      j = RANDOM(1, n)\n4      if key[i] < key[j] and key[j] <= k\n5          i = j\n6      if key[i] == k\n7          return i\n8  while i != NIL and key[i] < k\n9      i = next[i]\n10 if i == NIL or key[i] > k\n11     return NIL\n12 else return i\n```\n\n# C12 Binary Search Trees Algorithms\n\n## 12.1 INORDER-TREE-WALK(x)\n```\nINORDER-TREE-WALK(x)\n1  if x != NIL\n2      INORDER-TREE-WALK(x.left)\n3      print x.key\n4      INORDER-TREE-WALK(x.right)\n```\n\n## 12.2 TREE-SEARCH(x, k)\n```\nTREE-SEARCH(x, k)\n1  if x == NIL or k == x.key\n2      return x\n3  if k < x.key\n4      return TREE-SEARCH(x.left, k)\n5  else return TREE-SEARCH(x.right, k)\n```\n\n## 12.2 ITERATIVE-TREE-SEARCH(x, k)\n```\nITERATIVE-TREE-SEARCH(x, k)\n1  while x != NIL and k != x.key\n2      if k < x.key\n3          x = x.left\n4      else x = x.right\n5  return x\n```\n\n## 12.2 TREE-MINIMUM(x)\n```\nTREE-MINIMUM(x)\n1  while x.left != NIL\n2      x = x.left\n3  return x\n```\n\n## 12.2 TREE-MAXIMUM(x)\n```\nTREE-MAXIMUM(x)\n1  while x.right != NIL\n2      x = x.right\n3  return x\n```\n\n## 12.2 TREE-SUCCESSOR(x)\n```\nTREE-SUCCESSOR(x)\n1  if x.right != NIL\n2      return TREE-MINIMUM(x.right)  // leftmost node in right subtree\n3  else // find the lowest ancestor of x whose left child is an ancestor of x\n4      y = x.p\n5      while y != NIL and x == y.right\n6          x = y\n7          y = y.p\n8      return y\n```\n\n## 12.3 TREE-INSERT(T, z)\n```\nTREE-INSERT(T, z)\n1  x = T.root            // node being compared with z\n2  y = NIL               // y will be parent of z\n3  while x != NIL        // descend until reaching a leaf\n4      y = x\n5      if z.key < x.key\n6          x = x.left\n7      else x = x.right\n8  z.p = y               // found the location\u2014insert z with parent y\n9  if y == NIL\n10     T.root = z          // tree T was empty\n11 elseif z.key < y.key\n12     y.left = z\n13 else y.right = z\n```\n\n## 12.3 TRANSPLANT(T, u, v)\n```\nTRANSPLANT(T, u, v)\n1  if u.p == NIL\n2      T.root = v\n3  elseif u == u.p.left\n4      u.p.left = v\n5  else u.p.right = v\n6  if v != NIL\n7      v.p = u.p\n```\n\n## 12.3 TREE-DELETE(T, z)\n```\nTREE-DELETE(T, z)\n1  if z.left == NIL\n2      TRANSPLANT(T, z, z.right)    // replace z by its right child\n3  elseif z.right == NIL\n4      TRANSPLANT(T, z, z.left)     // replace z by its left child\n5  else y = TREE-MINIMUM(z.right) // y is z's successor\n6       if y != z.right              // is y farther down the tree?\n7           TRANSPLANT(T, y, y.right) // replace y by its right child\n8           y.right = z.right        // z's right child becomes\n9           y.right.p = y            // y's right child\n10      TRANSPLANT(T, z, y)          // replace z by its successor y\n11      y.left = z.left              // and give z's left child to y,\n12      y.left.p = y                 // which had no left child\n```\n\n# C13 Red-Black Trees Algorithms\n\n## 13.2 LEFT-ROTATE(T, x)\n```\nLEFT-ROTATE(T, x)\n1  y = x.right\n2  x.right = y.left             // turn y\u2019s left subtree into x\u2019s right subtree\n3  if y.left != T.nil           // if y\u2019s left subtree is not empty ...\n4      y.left.p = x             // ... then x becomes the parent of the subtree\u2019s root\n5  y.p = x.p                    // x\u2019s parent becomes y\u2019s parent\n6  if x.p == T.nil              // if x was the root ...\n7      T.root = y               // ... then y becomes the root\n8  elseif x == x.p.left         // otherwise, if x was a left child ...\n9      x.p.left = y             // ... then y becomes a left child\n10 else x.p.right = y            // otherwise, x was a right child, and now y is\n11 y.left = x                   // make x become y\u2019s left child\n12 x.p = y\n```\n\n## 13.3 RB-INSERT(T, z)\n```\nRB-INSERT(T, z)\n1  x = T.root                 // node being compared with z\n2  y = T.nil                  // y will be parent of z\n3  while x != T.nil           // descend until reaching the sentinel\n4      y = x\n5      if z.key < x.key\n6          x = x.left\n7      else x = x.right\n8  z.p = y                    // found the location\u2014insert z with parent y\n9  if y == T.nil\n10     T.root = z               // tree T was empty\n11 elseif z.key < y.key\n12     y.left = z\n13 else y.right = z\n14 z.left = T.nil             // both of z\u2019s children are the sentinel\n15 z.right = T.nil\n16 z.color = RED              // the new node starts out red\n17 RB-INSERT-FIXUP(T, z)    // correct any violations of red-black properties\n```\n\n## 13.3 RB-INSERT-FIXUP(T, z)\n```\nRB-INSERT-FIXUP(T, z)\n1  while z.p.color == RED\n2      if z.p == z.p.p.left            // is z\u2019s parent a left child?\n3          y = z.p.p.right             // y is z\u2019s uncle\n4          if y.color == RED           // are z\u2019s parent and uncle both red?\n5              z.p.color = BLACK       // Case 1\n6              y.color = BLACK\n7              z.p.p.color = RED\n8              z = z.p.p\n9          else \n10             if z == z.p.right       // Case 2\n11                 z = z.p\n12                 LEFT-ROTATE(T, z)\n13             z.p.color = BLACK       // Case 3\n14             z.p.p.color = RED\n15             RIGHT-ROTATE(T, z.p.p)\n16     else // same as lines 3-15, but with \"right\" and \"left\" exchanged\n17         y = z.p.p.left\n18         if y.color == RED\n19             z.p.color = BLACK\n20             y.color = BLACK\n21             z.p.p.color = RED\n22             z = z.p.p\n23         else \n24             if z == z.p.left\n25                 z = z.p\n26                 RIGHT-ROTATE(T, z)\n27             z.p.color = BLACK\n28             z.p.p.color = RED\n29             LEFT-ROTATE(T, z.p.p)\n30 T.root.color = BLACK\n```\n\n## 13.4 RB-TRANSPLANT(T, u, v)\n```\nRB-TRANSPLANT(T, u, v)\n1  if u.p == T.nil\n2      T.root = v\n3  elseif u == u.p.left\n4      u.p.left = v\n5  else u.p.right = v\n6  v.p = u.p\n```\n\n## 13.4 RB-DELETE(T, z)\n```\nRB-DELETE(T, z)\n1  y = z\n2  y-original-color = y.color\n3  if z.left == T.nil\n4      x = z.right\n5      RB-TRANSPLANT(T, z, z.right)     // replace z by its right child\n6  elseif z.right == T.nil\n7      x = z.left\n8      RB-TRANSPLANT(T, z, z.left)      // replace z by its left child\n9  else y = TREE-MINIMUM(z.right)     // y is z\u2019s successor\n10     y-original-color = y.color\n11     x = y.right\n12     if y != z.right                  // is y farther down the tree?\n13         RB-TRANSPLANT(T, y, y.right) // replace y by its right child\n14         y.right = z.right            // z\u2019s right child becomes\n15         y.right.p = y                // y\u2019s right child\n16     else x.p = y                     // in case x is T.nil\n17     RB-TRANSPLANT(T, z, y)           // replace z by its successor y\n18     y.left = z.left                  // and give z\u2019s left child to y,\n19     y.left.p = y                     // which had no left child\n20     y.color = z.color\n21 if y-original-color == BLACK        // if any red-black violations occurred,\n22     RB-DELETE-FIXUP(T, x)          // correct them\n```\n\n## 13.4 RB-DELETE-FIXUP(T, x)\n```\nRB-DELETE-FIXUP(T, x)\n1  while x != T.root and x.color == BLACK\n2      if x == x.p.left                // is x a left child?\n3          w = x.p.right               // w is x\u2019s sibling\n4          if w.color == RED           // Case 1\n5              w.color = BLACK\n6              x.p.color = RED\n7              LEFT-ROTATE(T, x.p)\n8              w = x.p.right\n9          if w.left.color == BLACK and w.right.color == BLACK  // Case 2\n10             w.color = RED\n11             x = x.p\n12         else \n13             if w.right.color == BLACK // Case 3\n14                 w.left.color = BLACK\n15                 w.color = RED\n16                 RIGHT-ROTATE(T, w)\n17                 w = x.p.right\n18             w.color = x.p.color       // Case 4\n19             x.p.color = BLACK\n20             w.right.color = BLACK\n21             LEFT-ROTATE(T, x.p)\n22             x = T.root\n23     else // same as lines 3-22, but with \"right\" and \"left\" exchanged\n24         w = x.p.left\n25         if w.color == RED\n26             w.color = BLACK\n27             x.p.color = RED\n28             RIGHT-ROTATE(T, x.p)\n29             w = x.p.left\n30         if w.right.color == BLACK and w.left.color == BLACK\n31             w.color = RED\n32             x = x.p\n33         else \n34             if w.left.color == BLACK\n35                 w.right.color = BLACK\n36                 w.color = RED\n37                 LEFT-ROTATE(T, w)\n38                 w = x.p.left\n39             w.color = x.p.color\n40             x.p.color = BLACK\n41             w.left.color = BLACK\n42             RIGHT-ROTATE(T, x.p)\n43             x = T.root\n44 x.color = BLACK\n```\n"
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": "# 10 Elementary Data Structures\n\nThis chapter examines the representation of dynamic sets by simple data structures that use pointers: arrays, matrices, stacks, queues, linked lists, and rooted trees.\n\n## 10.1 Simple array-based data structures: arrays, matrices, stacks, queues\n\n### 10.1.1 Arrays\nAn array is stored as a contiguous sequence of bytes in memory. If the first element has index $s$, starts at memory address $a$, and each element occupies $b$ bytes, the $i$th element occupies bytes $a + b(i-s)$ through $a + b(i-s+1) - 1$. For 1-origin indexing ($s=1$), the $i$th element is at $a+b(i-1)$ through $a+bi-1$. For 0-origin indexing ($s=0$), it's $a+bi$ through $a+b(i+1)-1$. Accessing any array element takes constant time in the RAM model.\nIf array elements vary in size, each array element typically stores a pointer to the object. The pointer size is constant, allowing the formulas to give the address of the pointer.\n\n### 10.1.2 Matrices\nA matrix (two-dimensional array) is typically represented by one or more one-dimensional arrays. For an $m \\times n$ matrix:\n- In **row-major order**, the matrix is stored row by row.\n- In **column-major order**, it's stored column by column.\nFor an element $M[i,j]$ in a single array starting at index $s$ (e.g., $s=0$ or $s=1$):\n- Row-major: index is $s + n(i-s) + (j-s)$. If $s=0$, it's $ni+j$.\n- Column-major: index is $s + m(j-s) + (i-s)$. If $s=0$, it's $i+mj$.\nMultiple-array strategies can also be used, e.g., an array of pointers to row arrays. These can be more flexible for ragged arrays but are often less efficient than single-array representations.\nBlock representation divides the matrix into blocks, storing each block contiguously.\n\n### 10.1.3 Stacks and queues\nStacks and queues are dynamic sets where the element removed by DELETE is prespecified.\n\n**Stacks**\nA stack implements a last-in, first-out (LIFO) policy. The INSERT operation is often called PUSH, and DELETE (which takes no element argument) is POP.\nAn array $S[1..n]$ can implement a stack of at most $n$ elements. Attribute $S.top$ indexes the most recently inserted element. Elements are $S[1..S.top]$.\n- $S.top = 0$: stack is empty.\n- Attempt to POP an empty stack: underflow (error).\n- $S.top > S.size$: overflow (error).\nOperations:\n- `STACK-EMPTY(S)`: Returns TRUE if $S.top == 0$, else FALSE. [[PartIII Data Structures Algorithms.md#C10.1 STACK-EMPTY]]\n- `PUSH(S, x)`: Increments $S.top$, then stores $x$ at $S[S.top]$. Checks for overflow. [[PartIII Data Structures Algorithms.md#C10.1 PUSH]]\n- `POP(S)`: Checks for underflow. Decrements $S.top$, returns $S[S.top+1]$. [[PartIII Data Structures Algorithms.md#C10.1 POP]]\nAll three operations take $O(1)$ time.\n\n**Queues**\nA queue implements a first-in, first-out (FIFO) policy. INSERT is ENQUEUE, DELETE is DEQUEUE (takes no element argument).\nA queue has a head and a tail. Elements are enqueued at the tail and dequeued from the head.\nAn array $Q[1..n]$ can implement a queue of at most $n-1$ elements (to distinguish full from empty when $Q.head = Q.tail$).\n- $Q.head$: indexes (or points to) the head.\n- $Q.tail$: indexes the next location for a new element.\nElements are in $Q[Q.head], Q[Q.head+1], \nucleon, Q[Q.tail-1]$, with wrap-around (location 1 follows $n$).\n- $Q.head = Q.tail$: queue is empty. Initially $Q.head = Q.tail = 1$.\n- Attempt to DEQUEUE an empty queue: underflow.\n- $Q.head = Q.tail + 1$ (or $Q.head=1$ and $Q.tail=n$ if $Q.size=n$): queue is full, attempt to ENQUEUE causes overflow.\nOperations:\n- `ENQUEUE(Q, x)`: Stores $x$ at $Q[Q.tail]$. Updates $Q.tail$ with wrap-around. [[PartIII Data Structures Algorithms.md#C10.1 ENQUEUE]]\n- `DEQUEUE(Q)`: Retrieves $x$ from $Q[Q.head]$. Updates $Q.head$ with wrap-around. Returns $x$. [[PartIII Data Structures Algorithms.md#C10.1 DEQUEUE]]\nBoth operations take $O(1)$ time (omitting error checking).\n\n## 10.2 Linked lists\nA linked list arranges objects in linear order determined by pointers.\nEach element of a doubly linked list $L$ is an object with attributes `key`, `next`, and `prev`.\n- `x.next`: points to successor.\n- `x.prev`: points to predecessor.\n- `x.prev = NIL`: $x$ is the head.\n- `x.next = NIL`: $x$ is the tail.\n- `L.head`: points to the first element. If `L.head = NIL`, list is empty.\nForms of lists: singly linked (no `prev`), doubly linked, sorted or not, circular or not. This section assumes unsorted, doubly linked lists.\n\n**Searching a linked list**\n`LIST-SEARCH(L, k)` finds the first element with key $k$ by linear search. [[PartIII Data Structures Algorithms.md#C10.2 LIST-SEARCH]]\nTime: $\\Theta(n)$ in the worst case for a list of $n$ objects.\n\n**Inserting into a linked list**\n`LIST-PREPEND(L, x)` adds element $x$ (key already set) to the front. [[PartIII Data Structures Algorithms.md#C10.2 LIST-PREPEND]]\nTime: $O(1)$.\n`LIST-INSERT(x, y)` (as presented in CLRS 4th ed., it implies inserting element $x$ *after* element $y$. The pseudocode shown on page 261 of CLRS 3rd ed. for LIST-INSERT(L,x) is for inserting x at the head, similar to LIST-PREPEND. The text on page 260 CLRS 3rd ed. (OCR page 13) describes inserting a new element $x$ *immediately following* $y$, which is what the procedure LIST-INSERT(x,y) on OCR page 14 does. It splices $x$ after $y$.) If a pointer $y$ to an object in the list is given, LIST-INSERT splices a new element $x$ into the list immediately following $y$. [[PartIII Data Structures Algorithms.md#C10.2 LIST-INSERT]]\nTime: $O(1)$. Does not reference list object $L$.\n\n**Deleting from a linked list**\n`LIST-DELETE(L, x)` removes element $x$ (given by a pointer) from list $L$. [[PartIII Data Structures Algorithms.md#C10.2 LIST-DELETE]]\nTime: $O(1)$. To delete an element with a given key, LIST-SEARCH must be called first, making it $\\Theta(n)$ worst-case.\nInsertion and deletion are $O(1)$ for doubly linked lists vs $\\Theta(n)$ for arrays (worst-case for first element).\n\n**Sentinels**\nA sentinel is a dummy object $L.nil$ that simplifies boundary conditions. It replaces NIL references and turns a doubly linked list into a circular, doubly linked list with a sentinel. $L.nil$ is between head and tail. $L.nil.next$ is head, $L.nil.prev$ is tail. Head and tail point to $L.nil$. $L.head$ attribute is eliminated.\n- `LIST-DELETE'(x)`: Simplified deletion. [[PartIII Data Structures Algorithms.md#C10.2 LIST-DELETE']]\n- `LIST-INSERT'(x, y)`: Simplified insertion. To insert at head, $y=L.nil$. To insert at tail, $y=L.nil.prev$. [[PartIII Data Structures Algorithms.md#C10.2 LIST-INSERT']]\n- `LIST-SEARCH'(L, k)`: Sentinel $L.nil.key$ is set to $k$ before search, simplifying the loop. [[PartIII Data Structures Algorithms.md#C10.2 LIST-SEARCH']]\nSentinels simplify code, may offer small constant factor speedup, but don't typically improve asymptotic running time. Use judiciously due to extra storage.\n\n## 10.3 Representing rooted trees\nEach node is an object with a `key` attribute and pointers.\n\n**Binary trees**\nAttributes `p` (parent), `left` (left child), `right` (right child). `T.root` points to tree root. If `x.p = NIL`, $x$ is root. If child missing, attribute is NIL. If `T.root = NIL`, tree is empty.\n\n**Rooted trees with unbounded branching**\nIf number of children per node is at most constant $k$, can use `child1, ..., childk`. Not suitable for unbounded or varying number of children.\n**Left-child, right-sibling representation**: Uses $O(n)$ space for $n$-node tree.\nEach node $x$ has:\n- `p`: parent pointer.\n- `T.root`: points to tree root.\n- `x.left-child`: points to leftmost child of $x$.\n- `x.right-sibling`: points to sibling of $x$ immediately to its right.\nIf $x$ has no children, `x.left-child = NIL`. If $x$ is rightmost child, `x.right-sibling = NIL`.\n\n**Other tree representations**\n- Heap (Chapter 6): single array for complete binary tree.\n- Chapter 19 trees: only parent pointers (traversed toward root).\nChoice depends on application."
            },
            {
                "file": "PartIII Data Structures/C11 Hash Tables.md",
                "contents": "# 11 Hash Tables\n\nHash tables are effective for implementing dictionaries (INSERT, SEARCH, DELETE). Average time for search is $O(1)$ under reasonable assumptions, though worst-case can be $\\Theta(n)$.\n\n## 11.1 Direct-address tables\nWorks well when the universe $U$ of keys is small. Uses an array $T[0..m-1]$ (direct-address table), where $m = |U|$. Each slot corresponds to a key. $T[k]$ points to an element with key $k$, or NIL if no such element.\nOperations:\n- `DIRECT-ADDRESS-SEARCH(T, k)`: `return T[k]`. [[PartIII Data Structures Algorithms.md#C11.1 DIRECT-ADDRESS-SEARCH]]\n- `DIRECT-ADDRESS-INSERT(T, x)`: `T[x.key] = x`. [[PartIII Data Structures Algorithms.md#C11.1 DIRECT-ADDRESS-INSERT]]\n- `DIRECT-ADDRESS-DELETE(T, x)`: `T[x.key] = NIL`. [[PartIII Data Structures Algorithms.md#C11.1 DIRECT-ADDRESS-DELETE]]\nEach takes $O(1)$ time. Elements can be stored directly in table slots or via pointers.\n\n## 11.2 Hash tables\nWhen the set $K$ of stored keys is much smaller than universe $U$, hash tables require less storage ($\\\theta(|K|)$) than direct addressing, maintaining $O(1)$ average-case search time (worst-case $\\Theta(n)$).\nA **hash function** $h: U \\rightarrow \\{0, 1, ..., m-1\\}$ computes the slot $h(k)$ for a key $k$. $m$ is table size, typically $m \\ll |U|$.\nA **collision** occurs when two keys hash to the same slot.\n\n**Independent uniform hashing**\nAn ideal hash function where each key $k$ is mapped to an output $h(k)$ that is an element randomly and independently chosen uniformly from $\\{0, ..., m-1\\}$. This is a theoretical ideal, also called a random oracle.\n\n**Collision resolution by chaining**\nEach slot $T[j]$ points to a linked list of all elements that hash to $j$. If no elements hash to $j$, $T[j]$ is NIL.\nOperations (using linked list procedures from Section 10.2):\n- `CHAINED-HASH-INSERT(T, x)`: Prepend $x$ to list $T[h(x.key)]$. $O(1)$ worst-case (assumes $x$ not already present). [[PartIII Data Structures Algorithms.md#C11.2 CHAINED-HASH-INSERT]]\n- `CHAINED-HASH-SEARCH(T, k)`: Search for key $k$ in list $T[h(k)]$. Worst-case proportional to list length. [[PartIII Data Structures Algorithms.md#C11.2 CHAINED-HASH-SEARCH]]\n- `CHAINED-HASH-DELETE(T, x)`: Delete $x$ from list $T[h(x.key)]$. $O(1)$ worst-case if list is doubly linked and $x$ is given by pointer. [[PartIII Data Structures Algorithms.md#C11.2 CHAINED-HASH-DELETE]]\n\n**Analysis of hashing with chaining**\nDefine **load factor** $\\alpha = n/m$, where $n$ elements are stored in $m$ slots.\nWorst-case: all $n$ keys hash to one slot, search time $\\Theta(n)$.\nAverage-case performance (assuming independent uniform hashing):\n- Let $n_j$ be length of list $T[j]$. $E[n_j] = \\alpha$.\n- **Theorem 11.1**: An unsuccessful search takes $\\Theta(1+\\alpha)$ time on average. Any key $k$ not in table is equally likely to hash to any of $m$ slots. Expected search time is to search to end of list $T[h(k)]$, expected length $\\alpha$.\n- **Theorem 11.2**: A successful search takes $\\Theta(1+\\alpha)$ time on average. Assumes searched element is equally likely to be any of $n$ stored elements. Expected number of elements examined is $1 + (\\alpha/2 - \\alpha/(2n))$.\nIf $n = O(m)$, then $\\alpha = O(1)$, so all dictionary operations take $O(1)$ time on average.\n\n## 11.3 Hash functions\nA good hash function should satisfy (approximately) independent uniform hashing.\nKeys can be integers, vectors, or strings. Initially, assume keys are short non-negative integers.\n\n### 11.3.1 Static hashing\nUses a single, fixed hash function.\n**The division method**: $h(k) = k \\pmod m$. Fast. Works well if $m$ is a prime not too close to an exact power of 2.\n**The multiplication method**: $h(k) = \\lfloor m (kA \\pmod 1) \\rfloor$, where $0 < A < 1$. $kA \\pmod 1$ is fractional part $kA - \\lfloor kA \\rfloor$. Value of $m$ is not critical.\n**The multiply-shift method**: A practical variant when $m=2^l$ for integer $l \\le w$ (word size). Let $a$ be a $w$-bit positive integer. $h_a(k) = (ka \\pmod{2^w}) \\gg (w-l)$. This extracts the $l$ most significant bits of the low-order $w$-bit word of $ka$. Implemented with multiplication and logical right shift.\n\n### 11.3.2 Random hashing\nChoose hash function randomly at runtime, independent of keys. Universal hashing is a special case.\nA family $H$ of hash functions is **universal** if for each pair of distinct keys $k_1, k_2 \\in U$, the number of hash functions $h \\in H$ for which $h(k_1) = h(k_2)$ is at most $|H|/m$. Collision probability $\\le 1/m$.\n**Corollary 11.3**: Using universal hashing and chaining, any sequence of $s$ INSERT, SEARCH, DELETE operations with $n=O(m)$ insertions takes $\\Theta(s)$ expected time.\n\n### 11.3.3 Achievable properties of random hashing\nLet $h$ be picked uniformly at random from family $H$.\n- $H$ is **uniform** if for any key $k$ and slot $q$, $P(h(k)=q) = 1/m$.\n- $H$ is **universal** if for distinct $k_1, k_2$, $P(h(k_1)=h(k_2)) \\le 1/m$.\n- $H$ is **$\\epsilon$-universal** if $P(h(k_1)=h(k_2)) \\le \\epsilon$. (Universal is $1/m$-universal).\n- $H$ is **d-independent** if for any distinct $k_1, ..., k_d$ and any slots $q_1, ..., q_d$, $P(h(k_i)=q_i \\text{ for all } i) = 1/m^d$.\n\n### 11.3.4 Designing a universal family of hash functions\n**Based on number theory**: Choose prime $p$ large enough for all keys ($0 \\le k < p$). Let $Z_p = \\{0, ..., p-1\\}$, $Z_p^* = \\{1, ..., p-1\\}$. For $a \\in Z_p^*$ and $b \\in Z_p$, define $h_{a,b}(k) = ((ak+b) \\pmod p) \\pmod m$. The family $H_{p,m} = \\{h_{a,b} : a \\in Z_p^*, b \\in Z_p\\}$ has $p(p-1)$ functions.\n**Theorem 11.4**: $H_{p,m}$ is universal. Proof involves showing distinct $k_1, k_2$ map to distinct $r_1, r_2 \\pmod p$, and then bounding probability $r_1 \\equiv r_2 \\pmod m$.\n**Based on multiply-shift**: Let $H = \\{h_a : a \\text{ is odd, } 1 \\le a < m\\}$, where $h_a$ is defined by eq (11.2) (multiply-shift $h_a(k) = (ka \\pmod{2^w}) \\gg (w-l)$ when $m=2^l$).\n**Theorem 11.5**: This family $H$ is $2/m$-universal.\n\n### 11.3.5 Hashing long inputs such as vectors or strings\n**Number-theoretic approaches**: Extend ideas from universal hash functions.\n**Cryptographic hashing**: Use cryptographic hash functions like SHA-256. $h(k) = \\text{SHA-256}(k) \\pmod m$. For a family, prepend a salt: $h_a(k) = \\text{SHA-256}(a || k) \\pmod m$. Message Authentication Codes (MACs) offer other methods. Useful for hierarchical memory systems (see Section 11.5).\n\n## 11.4 Open addressing\nAll elements stored in the hash table itself. No storage outside table. Load factor $\\alpha = n/m \\le 1$.\nCollision handling: probe table slots in a sequence until an empty one is found. Hash function $h: U \\times \\{0, ..., m-1\\} \\rightarrow \\{0, ..., m-1\\}$, where second arg is probe number.\nProbe sequence $\\langle h(k,0), h(k,1), ..., h(k,m-1) \\rangle$ must be a permutation of $\\langle 0, ..., m-1 \\rangle$.\n- `HASH-INSERT(T, k)`: Probes for an empty slot to insert $k$. [[PartIII Data Structures Algorithms.md#C11.4 HASH-INSERT]]\n- `HASH-SEARCH(T, k)`: Probes same sequence as HASH-INSERT. Stops if $k$ found or empty slot encountered. [[PartIII Data Structures Algorithms.md#C11.4 HASH-SEARCH]]\nDeletion is tricky: cannot simply mark slot NIL. Use special value DELETED. Search passes over DELETED. Insert treats DELETED as empty. Complicates analysis. Chaining often preferred if deletions are frequent.\nAssume **independent uniform permutation hashing**: each key's probe sequence is equally likely to be any of $m!$ permutations.\n**Double hashing**: $h(k,i) = (h_1(k) + i \\cdot h_2(k)) \\pmod m$. $h_1, h_2$ are auxiliary hash functions. $h_2(k)$ must be relatively prime to $m$. Good ways: $m$ is power of 2 and $h_2(k)$ always odd; or $m$ is prime and $h_2(k)$ is positive and $< m$. Generates $\\Theta(m^2)$ probe sequences.\n**Linear probing**: $h(k,i) = (h_1(k) + i) \\pmod m$. (Special case of double hashing with $h_2(k)=1$). Only $m$ distinct probe sequences. Suffers from primary clustering.\n\n**Analysis of open-address hashing** (Assuming independent uniform permutation hashing, $\\alpha < 1$, no deletions)\n- **Theorem 11.6**: Expected number of probes in an unsuccessful search is at most $1/(1-\\alpha)$.\n- **Corollary 11.7**: Expected probes for insertion is at most $1/(1-\\alpha)$.\n- **Theorem 11.8**: Expected number of probes in a successful search is at most $(1/\\alpha) \\ln(1/(1-\\alpha))$.\nIf $\\alpha$ is constant, searches $O(1)$. If $\\alpha \\rightarrow 1$, performance degrades.\n\n## 11.5 Practical considerations\n**Memory hierarchies**: Cache blocks benefit locality. Standard RAM model (counting probes) is an approximation.\n**Advanced instruction sets**: Can implement cryptographic functions efficiently.\n\n### 11.5.1 Linear probing\nOften disparaged in RAM model but excels for hierarchical memory (sequential probes likely in same cache block).\n**Deletion with linear probing**: Possible without DELETED marker. Procedure `LINEAR-PROBING-HASH-DELETE(T,q)` uses an inverse hash function $g(k,q) = (q - h_1(k)) \\pmod m$ to determine if a key $k'$ at $q'$ needs to move to the now-empty slot $q$. It iteratively moves keys to fill the vacated slot if their original probe sequence would have placed them in $q$ had $q$ been empty earlier. [[PartIII Data Structures Algorithms.md#C11.5 LINEAR-PROBING-HASH-DELETE]]\n**Primary clustering**: Long runs of occupied slots tend to get longer.\n**Theorem 11.9**: If $h_1$ is 5-independent and $\\alpha \\le 2/3$, linear probing takes expected constant time per operation.\n\n### 11.5.2 Hash functions for hierarchical memory models\nLinear probing is good for memory hierarchy. Complex (e.g., 5-independent) hash functions needed for good performance, can be efficient if implemented in registers.\n**The wee hash function**:\nFor word size $w$ (even), $a$ (odd), $b$ $w$-bit unsigned integers.\n`swap(x)` swaps $w/2$-bit halves of $x$.\n$f_a(k) = \\text{swap}((2k^2 + ak) \\pmod{2^w})$.\n$f_a^{(r)}(k)$ is $f_a$ iterated $r$ times on $k$.\nFor $t$-bit input $k$, parameters $a,b,r$: $h_{a,b,t,r}(k) = (f_{a+2t}^{(r)}(k+b)) \\pmod m$.\nFor variable-length inputs: break input $k$ into $w$-bit words $k_1, ..., k_u$. `chop(k)` does this.\n`WEE(k, a, b, t, r, m)` procedure: initializes hash $q=b$, then for $i=1$ to $u$, $q = f^{(r)}_{a+2t}(k_i+q)$, finally $q \\pmod m$. [[PartIII Data Structures Algorithms.md#C11.5 WEE]]\nThis function is fast, uses minimal memory (registers), and provides good randomness. Recommended $r=4$. Approximate 5-independence can be argued."
            },
            {
                "file": "PartIII Data Structures/C12 Binary Search Trees.md",
                "contents": "# 12 Binary Search Trees\n\nA search tree data structure supports dynamic-set operations like SEARCH, MINIMUM, MAXIMUM, PREDECESSOR, SUCCESSOR, INSERT, and DELETE. Operations on a binary search tree take time proportional to the height $h$ of the tree.\n\n## 12.1 What is a binary search tree?\nA binary search tree is organized as a binary tree. Each node $x$ has attributes `key`, satellite data, `left` (left child), `right` (right child), and `p` (parent). If a child or parent is missing, the attribute is NIL. The tree $T$ has an attribute `T.root`.\n\n**Binary-search-tree property**:\nFor any node $x$:\n- If $y$ is a node in the left subtree of $x$, then $y.key \\le x.key$.\n- If $y$ is a node in the right subtree of $x$, then $y.key \\ge x.key$.\n\nThis property allows printing all keys in sorted order using an **inorder tree walk**.\n`INORDER-TREE-WALK(x)`: [[PartIII Data Structures Algorithms.md#C12.1 INORDER-TREE-WALK]]\nRecursively walks the left subtree, prints $x.key$, then recursively walks the right subtree.\n**Theorem 12.1**: An inorder tree walk on an $n$-node subtree takes $\\Theta(n)$ time. Proof by substitution: $T(n) \\le T(k) + T(n-k-1) + d$, shows $T(n) = O(n)$. Since all nodes are visited, $T(n) = \\Omega(n)$.\n\n## 12.2 Querying a binary search tree\nOperations take $O(h)$ time on a tree of height $h$.\n\n**Searching**\n`TREE-SEARCH(x, k)`: Compares $k$ with $x.key$. If equal, found. If $k < x.key$, search left. Else, search right. [[PartIII Data Structures Algorithms.md#C12.2 TREE-SEARCH]]\n`ITERATIVE-TREE-SEARCH(x, k)`: Iterative version, traces a path downward. [[PartIII Data Structures Algorithms.md#C12.2 ITERATIVE-TREE-SEARCH]]\nBoth take $O(h)$ time.\n\n**Minimum and maximum**\n- `TREE-MINIMUM(x)`: Follows `left` pointers from $x$ until NIL. [[PartIII Data Structures Algorithms.md#C12.2 TREE-MINIMUM]]\n- `TREE-MAXIMUM(x)`: Follows `right` pointers from $x$ until NIL. [[PartIII Data Structures Algorithms.md#C12.2 TREE-MAXIMUM]]\nBoth take $O(h)$ time.\n\n**Successor and predecessor**\nSuccessor of node $x$: node with smallest key greater than $x.key$ (if keys distinct), or next node in inorder walk.\n`TREE-SUCCESSOR(x)`: [[PartIII Data Structures Algorithms.md#C12.2 TREE-SUCCESSOR]]\nTwo cases:\n1. If $x.right$ is not NIL, successor is `TREE-MINIMUM(x.right)`.\n2. If $x.right$ is NIL, successor $y$ is the lowest ancestor of $x$ whose left child is also an ancestor of $x$. Go up from $x$ using parent pointers until current node $x$ is a left child of its parent $y$, or root is reached.\n`TREE-PREDECESSOR(x)` is symmetric.\nBoth take $O(h)$ time.\n\n**Theorem 12.2**: SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, PREDECESSOR run in $O(h)$ time on a binary search tree of height $h$.\n\n## 12.3 Insertion and deletion\nThese operations modify the tree while maintaining the binary-search-tree property.\n\n**Insertion**\n`TREE-INSERT(T, z)`: Node $z$ (with $z.key$ set, $z.left=NIL, z.right=NIL$) is inserted. [[PartIII Data Structures Algorithms.md#C12.3 TREE-INSERT]]\nStarts at $T.root$. Traces a path downward, maintaining trailing pointer $y$ (parent of current $x$). When $x$ becomes NIL, $z$ is inserted as child of $y$.\nTakes $O(h)$ time.\n\n**Deletion**\nStrategy involves three basic cases for deleting node $z$:\n1. If $z$ has no children: remove $z$ by modifying $z.parent$.\n2. If $z$ has one child: elevate child to $z$'s position.\n3. If $z$ has two children: find $z$'s successor $y$ (in $z$'s right subtree, $y$ has no left child). Replace $z$ with $y$. $z$'s original left subtree becomes $y$'s new left subtree. $y$'s original right subtree moves into $y$'s original position.\n\nSubroutine `TRANSPLANT(T, u, v)` replaces subtree rooted at $u$ with subtree at $v$. $u.p$ becomes $v.p$. $v$ becomes appropriate child of $u.p$. Does not update $v.left, v.right$. [[PartIII Data Structures Algorithms.md#C12.3 TRANSPLANT]]\n\n`TREE-DELETE(T, z)`: Deletes node $z$. [[PartIII Data Structures Algorithms.md#C12.3 TREE-DELETE]]\nHandles cases based on $z.left$ and $z.right$:\n- If $z.left == NIL$: transplant $z$ with $z.right$.\n- Else if $z.right == NIL$: transplant $z$ with $z.left$.\n- Else ($z$ has two children):\n    - Let $y = \\text{TREE-MINIMUM}(z.right)$ ($y$ is $z$'s successor).\n    - If $y \\ne z.right$ (i.e., $y$ is not $z$'s immediate right child):\n        - Transplant $y$ with $y.right$.\n        - $y.right = z.right$.\n        - $y.right.p = y$.\n    - Transplant $z$ with $y$.\n    - $y.left = z.left$.\n    - $y.left.p = y$.\nTakes $O(h)$ time.\n\n**Theorem 12.3**: INSERT and DELETE run in $O(h)$ time on a binary search tree of height $h$."
            },
            {
                "file": "PartIII Data Structures/C13 Red-Black Trees.md",
                "contents": "# 13 Red-Black Trees\n\nRed-black trees are a type of balanced binary search tree that guarantee $O(\\lg n)$ time for basic dynamic-set operations in the worst case. They use an extra bit of storage per node for color (RED or BLACK).\n\n## 13.1 Properties of red-black trees\nA red-black tree is a binary search tree satisfying these properties:\n1. Every node is either red or black.\n2. The root is black.\n3. Every leaf (NIL) is black.\n4. If a node is red, then both its children are black.\n5. For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.\n\nA single sentinel node `T.nil` represents all NIL leaves and the root's parent. `T.nil` is black. Its other attributes are immaterial.\n**Black-height** `bh(x)` of a node $x$ is the number of black nodes on any simple path from, but not including, $x$ down to a leaf. This is well-defined by property 5. The black-height of a red-black tree is `bh(T.root)`.\n\n**Lemma 13.1**: A red-black tree with $n$ internal nodes has height at most $2 \\lg(n+1)$.\nProof idea: Show any subtree rooted at $x$ contains at least $2^{bh(x)}-1$ internal nodes by induction on height of $x$. Let $h$ be tree height. Root's black-height $bh(T.root) \\ge h/2$ (by property 4, at least half nodes on root-to-leaf path are black, excluding root). So $n \\ge 2^{h/2}-1$, which gives $h \\le 2 \\lg(n+1)$.\nThis lemma ensures that SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, PREDECESSOR run in $O(\\lg n)$ time. INSERT and DELETE also run in $O(\\lg n)$ but require modifications to maintain red-black properties.\n\n## 13.2 Rotations\nRotations are local pointer-restructuring operations that preserve the binary-search-tree property and change tree structure. They take $O(1)$ time.\n- `LEFT-ROTATE(T, x)`: Given node $x$ whose right child $y$ is not `T.nil`. $y$ becomes new root of subtree. $x$ becomes $y$'s left child. $y$'s original left child becomes $x$'s right child. [[PartIII Data Structures Algorithms.md#C13.2 LEFT-ROTATE]]\n- `RIGHT-ROTATE(T, y)` is symmetric to `LEFT-ROTATE(T, x)`.\n\n## 13.3 Insertion\n`RB-INSERT(T, z)` inserts node $z$ (key filled, children `T.nil`) into tree $T$. [[PartIII Data Structures Algorithms.md#C13.3 RB-INSERT]]\n1. Uses a modified TREE-INSERT logic with `T.nil` to find position for $z$.\n2. Sets $z.left = T.nil$, $z.right = T.nil$.\n3. Colors $z$ RED.\n4. Calls `RB-INSERT-FIXUP(T, z)` to restore red-black properties.\n\n`RB-INSERT-FIXUP(T, z)` restores properties. [[PartIII Data Structures Algorithms.md#C13.3 RB-INSERT-FIXUP]]\nViolations caused by inserting red $z$: Property 2 (if $z$ is root) or Property 4 (if $z.p$ is red).\n**Loop Invariant** (at start of `while z.p.color == RED` loop):\n  a. Node $z$ is red.\n  b. If $z.p$ is the root, then $z.p$ is black.\n  c. If the tree violates any red-black properties, it's at most one: either property 2 ( $z$ is red root) or property 4 ($z$ and $z.p$ are red).\nInitialization: Invariant holds when FIXUP is called.\nMaintenance (focus on $z.p == z.p.p.left$; other side symmetric): Let $y = z.p.p.right$ (uncle).\n- **Case 1: $z$'s uncle $y$ is RED.**\n    - Color $z.p$ BLACK.\n    - Color $y$ BLACK.\n    - Color $z.p.p$ RED.\n    - Set $z = z.p.p$. (Moves $z$ up two levels. Loop continues).\n- **Case 2: $z$'s uncle $y$ is BLACK and $z$ is a right child.**\n    - Set $z = z.p$.\n    - `LEFT-ROTATE(T, z)`.\n    - (Transforms into Case 3).\n- **Case 3: $z$'s uncle $y$ is BLACK and $z$ is a left child.**\n    - Color $z.p$ BLACK.\n    - Color $z.p.p$ RED.\n    - `RIGHT-ROTATE(T, z.p.p)`.\n    - (Loop terminates as $z.p$ is now BLACK).\nTermination: Loop terminates because either $z.p$ becomes black or $z$ moves up the tree. Finally, $T.root.color$ is set to BLACK to ensure property 2.\n**Analysis**: RB-INSERT takes $O(\\lg n)$ time. $O(\\lg n)$ for initial insertion path. FIXUP loop runs $O(\\lg n)$ times (Case 1 moves $z$ up). At most 2 rotations performed.\n\n## 13.4 Deletion\nDeletion also takes $O(\\lg n)$ time. Based on TREE-DELETE logic.\n`RB-TRANSPLANT(T, u, v)` is a customized version of TRANSPLANT using `T.nil`. It always sets $v.p = u.p$. [[PartIII Data Structures Algorithms.md#C13.4 RB-TRANSPLANT]]\n\n`RB-DELETE(T, z)` deletes node $z$. [[PartIII Data Structures Algorithms.md#C13.4 RB-DELETE]]\n- $y$ is the node either removed or moved within the tree ($y=z$ if $z$ has $<2$ children, else $y$ is $z$'s successor).\n- $y\\text{-original-color} = y.color$.\n- $x$ is the node that moves into $y$'s original position (can be `T.nil`).\n- If $y$ has two children, $y$ (successor of $z$) is moved into $z$'s position and given $z.color$.\n- If $y\\text{-original-color}$ was RED: Red-black properties hold after $y$ is removed/moved because no black-heights change, no red nodes become adjacent, and root (if $y$ wasn't it) remains black.\n- If $y\\text{-original-color}$ was BLACK: Problems may arise.\n    1. Property 1: $x$ now has an \"extra black\". If $x$ was RED, it becomes (singly) BLACK. If $x$ was BLACK, it becomes \"doubly black\".\n    2. Property 2: If $y$ was root and $x$ (red child) becomes new root, violation.\n    3. Property 4: If $x$ and $x.p$ are red, violation.\n    4. Property 5: Paths that contained $y$ now have one less black node. The \"extra black\" on $x$ conceptually fixes this.\n- Call `RB-DELETE-FIXUP(T, x)` if $y\\text{-original-color}$ was BLACK to resolve issues with $x$'s extra black.\n\n`RB-DELETE-FIXUP(T, x)` restores properties. [[PartIII Data Structures Algorithms.md#C13.4 RB-DELETE-FIXUP]]\nGoal: Move extra black on $x$ up the tree until $x$ is red-and-black (color $x$ black), $x$ is root (extra black vanishes), or rotations/recoloring fix it.\n`while x != T.root and x.color == BLACK` (meaning $x$ is doubly black).\nMaintenance (focus on $x == x.p.left$; $w = x.p.right$ is $x$'s sibling; other side symmetric):\n- **Case 1: $x$'s sibling $w$ is RED.**\n    - Color $w$ BLACK.\n    - Color $x.p$ RED.\n    - `LEFT-ROTATE(T, x.p)`.\n    - Update $w$ to $x$'s new sibling (which must be BLACK).\n    - (Transforms into Case 2, 3, or 4).\n- **Case 2: $x$'s sibling $w$ is BLACK, and both of $w$'s children are BLACK.**\n    - Color $w$ RED.\n    - Set $x = x.p$. (Moves extra black up. Loop continues).\n- **Case 3: $x$'s sibling $w$ is BLACK, $w.left$ is RED, and $w.right$ is BLACK.**\n    - Color $w.left$ BLACK.\n    - Color $w$ RED.\n    - `RIGHT-ROTATE(T, w)`.\n    - Update $w$ to $x$'s new sibling.\n    - (Transforms into Case 4).\n- **Case 4: $x$'s sibling $w$ is BLACK, and $w.right$ is RED.**\n    - Color $w$ with $x.p.color$.\n    - Color $x.p$ BLACK.\n    - Color $w.right$ BLACK.\n    - `LEFT-ROTATE(T, x.p)`.\n    - Set $x = T.root$. (Extra black removed. Loop terminates).\nFinally, $x.color$ (if $x$ is not root from loop termination) is set to BLACK to discharge any remaining extra black (if $x$ was red-and-black).\n**Analysis**: RB-DELETE takes $O(\\lg n)$ time. $O(\\lg n)$ for initial TREE-MINIMUM and transplantations. FIXUP loop runs $O(\\lg n)$ times (Case 2 moves $x$ up). At most 3 rotations performed."
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartIV Advanced Design and Analysis Techniques Algorithms.md",
                "contents": "# C14 Dynamic Programming Algorithms\n\n## 14.1 CUT-ROD\n```\nCUT-ROD(p, n)\n1 if n == 0\n2   return 0\n3 q = -\u221e\n4 for i = 1 to n\n5   q = max(q, p[i] + CUT-ROD(p, n - i))\n6 return q\n```\n\n## 14.1 MEMOIZED-CUT-ROD\n```\nMEMOIZED-CUT-ROD(p, n)\n1 let r[0..n] be a new array // will remember solution values in r\n2 for i = 0 to n\n3   r[i] = -\u221e\n4 return MEMOIZED-CUT-ROD-AUX(p, n, r)\n```\n\n## 14.1 MEMOIZED-CUT-ROD-AUX\n```\nMEMOIZED-CUT-ROD-AUX(p, n, r)\n1 if r[n] \u2265 0 // already have a solution for length n?\n2   return r[n]\n3 if n == 0\n4   q = 0\n5 else q = -\u221e\n6   for i = 1 to n // i is the position of the first cut\n7     q = max(q, p[i] + MEMOIZED-CUT-ROD-AUX(p, n - i, r))\n8 r[n] = q // remember the solution value for length n\n9 return q\n```\n\n## 14.1 BOTTOM-UP-CUT-ROD\n```\nBOTTOM-UP-CUT-ROD(p, n)\n1 let r[0..n] be a new array // will remember solution values in r\n2 r[0] = 0\n3 for j = 1 to n // for increasing rod length j\n4   q = -\u221e\n5   for i = 1 to j // i is the position of the first cut\n6     q = max(q, p[i] + r[j - i])\n7   r[j] = q // remember the solution value for length j\n8 return r[n]\n```\n\n## 14.1 EXTENDED-BOTTOM-UP-CUT-ROD\n```\nEXTENDED-BOTTOM-UP-CUT-ROD(p, n)\n1 let r[0..n] and s[1..n] be new arrays\n2 r[0] = 0\n3 for j = 1 to n // for increasing rod length j\n4   q = -\u221e\n5   for i = 1 to j // i is the position of the first cut\n6     if q < p[i] + r[j - i]\n7       q = p[i] + r[j - i]\n8       s[j] = i // best cut location so far for length j\n9   r[j] = q // remember the solution value for length j\n10 return r and s\n```\n\n## 14.1 PRINT-CUT-ROD-SOLUTION\n```\nPRINT-CUT-ROD-SOLUTION(p, n)\n1 (r, s) = EXTENDED-BOTTOM-UP-CUT-ROD(p, n)\n2 while n > 0\n3   print s[n] // cut location for length n\n4   n = n - s[n] // length of the remainder of the rod\n```\n\n## 14.2 RECTANGULAR-MATRIX-MULTIPLY\n```\nRECTANGULAR-MATRIX-MULTIPLY(A, B, C, p, q, r)\n1 for i = 1 to p\n2   for j = 1 to r\n3     for k = 1 to q\n4       c_ij = c_ij + a_ik * b_kj\n```\n\n## 14.2 MATRIX-CHAIN-ORDER\n```\nMATRIX-CHAIN-ORDER(p, n)\n1 let m[1..n, 1..n] and s[1..n-1, 2..n] be new tables\n2 for i = 1 to n // chain length 1\n3   m[i, i] = 0\n4 for l = 2 to n // l is the chain length\n5   for i = 1 to n - l + 1 // chain begins at A_i\n6     j = i + l - 1 // chain ends at A_j\n7     m[i, j] = \u221e\n8     for k = i to j - 1 // try A_i..k A_k+1..j\n9       q = m[i, k] + m[k + 1, j] + p_{i-1}p_k p_j\n10      if q < m[i, j]\n11        m[i, j] = q // remember this cost\n12        s[i, j] = k // remember this index\n13 return m and s\n```\n\n## 14.2 PRINT-OPTIMAL-PARENS\n```\nPRINT-OPTIMAL-PARENS(s, i, j)\n1 if i == j\n2   print \"A\"_i\n3 else print \"(\"\n4   PRINT-OPTIMAL-PARENS(s, i, s[i, j])\n5   PRINT-OPTIMAL-PARENS(s, s[i, j] + 1, j)\n6   print \")\"\n```\n\n## 14.3 RECURSIVE-MATRIX-CHAIN\n```\nRECURSIVE-MATRIX-CHAIN(p, i, j)\n1 if i == j\n2   return 0\n3 m[i, j] = \u221e\n4 for k = i to j - 1\n5   q = RECURSIVE-MATRIX-CHAIN(p, i, k)\n        + RECURSIVE-MATRIX-CHAIN(p, k + 1, j)\n        + p_{i-1}p_k p_j\n6   if q < m[i, j]\n7     m[i, j] = q\n8 return m[i, j]\n```\n\n## 14.3 MEMOIZED-MATRIX-CHAIN\n```\nMEMOIZED-MATRIX-CHAIN(p, n)\n1 let m[1..n, 1..n] be a new table\n2 for i = 1 to n\n3   for j = i to n\n4     m[i, j] = \u221e\n5 return LOOKUP-CHAIN(m, p, 1, n)\n```\n\n## 14.3 LOOKUP-CHAIN\n```\nLOOKUP-CHAIN(m, p, i, j)\n1 if m[i, j] < \u221e\n2   return m[i, j]\n3 if i == j\n4   m[i, j] = 0\n5 else for k = i to j - 1\n6   q = LOOKUP-CHAIN(m, p, i, k)\n        + LOOKUP-CHAIN(m, p, k + 1, j) + p_{i-1}p_k p_j\n7   if q < m[i, j]\n8     m[i, j] = q\n9 return m[i, j]\n```\n\n## 14.4 LCS-LENGTH\n```\nLCS-LENGTH(X, Y, m, n)\n1 let b[1..m, 1..n] and c[0..m, 0..n] be new tables\n2 for i = 1 to m\n3   c[i, 0] = 0\n4 for j = 0 to n\n5   c[0, j] = 0\n6 for i = 1 to m // compute table entries in row-major order\n7   for j = 1 to n\n8     if x_i == y_j\n9       c[i, j] = c[i - 1, j - 1] + 1\n10      b[i, j] = \"\u2196\"\n11    elseif c[i - 1, j] \u2265 c[i, j - 1]\n12      c[i, j] = c[i - 1, j]\n13      b[i, j] = \"\u2191\"\n14    else c[i, j] = c[i, j - 1]\n15      b[i, j] = \"\u2190\"\n16 return c and b\n```\n\n## 14.4 PRINT-LCS\n```\nPRINT-LCS(b, X, i, j)\n1 if i == 0 or j == 0\n2   return // the LCS has length 0\n3 if b[i, j] == \"\u2196\"\n4   PRINT-LCS(b, X, i - 1, j - 1)\n5   print x_i // same as y_j\n6 elseif b[i, j] == \"\u2191\"\n7   PRINT-LCS(b, X, i - 1, j)\n8 else PRINT-LCS(b, X, i, j - 1)\n```\n\n## 14.5 OPTIMAL-BST\n```\nOPTIMAL-BST(p, q, n)\n1 let e[1..n+1, 0..n], w[1..n+1, 0..n],\n    and root[1..n, 1..n] be new tables\n2 for i = 1 to n + 1 // base cases\n3   e[i, i - 1] = q_{i-1} // equation (14.14)\n4   w[i, i - 1] = q_{i-1}\n5 for l = 1 to n\n6   for i = 1 to n - l + 1\n7     j = i + l - 1\n8     e[i, j] = \u221e\n9     w[i, j] = w[i, j - 1] + p_j + q_j // equation (14.15)\n10    for r = i to j // try all possible roots r\n11      t = e[i, r - 1] + e[r + 1, j] + w[i, j] // equation (14.14)\n12      if t < e[i, j] // new minimum?\n13        e[i, j] = t\n14        root[i, j] = r\n15 return e and root\n```\n\n# C15 Greedy Algorithms Algorithms\n\n## 15.1 RECURSIVE-ACTIVITY-SELECTOR\n```\nRECURSIVE-ACTIVITY-SELECTOR(s, f, k, n)\n1 m = k + 1\n2 while m \u2264 n and s[m] < f[k] // find the first activity in S_k to finish\n3   m = m + 1\n4 if m \u2264 n\n5   return {a_m} \u222a RECURSIVE-ACTIVITY-SELECTOR(s, f, m, n)\n6 else return \u2205\n```\n\n## 15.1 GREEDY-ACTIVITY-SELECTOR\n```\nGREEDY-ACTIVITY-SELECTOR(s, f, n)\n1 A = {a_1}\n2 k = 1\n3 for m = 2 to n\n4   if s[m] \u2265 f[k] // is a_m in S_k?\n5     A = A \u222a {a_m} // yes, so choose it\n6     k = m // and continue from there\n7 return A\n```\n\n## 15.3 HUFFMAN\n```\nHUFFMAN(C)\n1 n = |C|\n2 Q = C\n3 for i = 1 to n - 1\n4   allocate a new node z\n5   x = EXTRACT-MIN(Q)\n6   y = EXTRACT-MIN(Q)\n7   z.left = x\n8   z.right = y\n9   z.freq = x.freq + y.freq\n10  INSERT(Q, z)\n11 return EXTRACT-MIN(Q) // the root of the tree is the only node left\n```\n\n# C16 Amortized Analysis Algorithms\n\n## 16.1 MULTIPOP\n```\nMULTIPOP(S, k)\n1 while not STACK-EMPTY(S) and k > 0\n2   POP(S)\n3   k = k - 1\n```\n\n## 16.1 INCREMENT\n```\nINCREMENT(A, k)\n1 i = 0\n2 while i < k and A[i] == 1\n3   A[i] = 0\n4   i = i + 1\n5 if i < k\n6   A[i] = 1\n```\n\n## 16.4 TABLE-INSERT\n```\nTABLE-INSERT(T, x)\n1 if T.size == 0\n2   allocate T.table with 1 slot\n3   T.size = 1\n4 if T.num == T.size\n5   allocate new-table with 2 * T.size slots\n6   insert all items in T.table into new-table\n7   free T.table\n8   T.table = new-table\n9   T.size = 2 * T.size\n10 insert x into T.table\n11 T.num = T.num + 1\n```\n"
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C14 Dynamic Programming.md",
                "contents": "This part covers three important techniques used in designing and analyzing efficient algorithms: dynamic programming (Chapter 14), greedy algorithms (Chapter 15), and amortized analysis (Chapter 16). Earlier parts have presented other widely applicable techniques, such as divide-and-conquer, randomization, and how to solve recurrences. The techniques in this part are somewhat more sophisticated, but you will be able to use them to solve many computational problems. The themes introduced in this part will recur later in this book.\n\nDynamic programming typically applies to optimization problems in which you make a set of choices in order to arrive at an optimal solution. Each choice generates subproblems of the same form as the original problem, and the same subproblems arise repeatedly. The key strategy is to store the solution to each such subproblem rather than recompute it. Chapter 14 shows how this simple idea can sometimes transform exponential-time algorithms into polynomial-time algorithms.\n\nLike dynamic-programming algorithms, greedy algorithms typically apply to optimization problems in which you make a set of choices in order to arrive at an optimal solution. The idea of a greedy algorithm is to make each choice in a locally optimal manner, resulting in a faster algorithm than you get with dynamic programming. Chapter 15 will help you determine when the greedy approach works.\n\nThe technique of amortized analysis applies to certain algorithms that perform a sequence of similar operations. Instead of bounding the cost of the sequence of operations by bounding the actual cost of each operation separately, an amortized analysis provides a worst-case bound on the actual cost of the entire sequence. One advantage of this approach is that although some operations might be expensive, many others might be cheap. You can use amortized analysis when designing algorithms, since the design of an algorithm and the analysis of its running time are often closely intertwined. Chapter 16 introduces three ways to perform an amortized analysis of an algorithm.\n\n# 14 Dynamic Programming\n\n## Introduction\nDynamic programming, like the divide-and-conquer method, solves problems by combining the solutions to subproblems. (\"Programming\" in this context refers to a tabular method, not to writing computer code.) Divide-and-conquer algorithms partition the problem into disjoint subproblems, solve the subproblems recursively, and then combine their solutions. In contrast, dynamic programming applies when the subproblems overlap\u2014that is, when subproblems share subsubproblems. In this context, a divide-and-conquer algorithm does more work than necessary, repeatedly solving common subsubproblems. A dynamic-programming algorithm solves each subsubproblem just once and then saves its answer in a table, avoiding recomputation.\n\nDynamic programming typically applies to **optimization problems**. Such problems can have many possible solutions, each with a value, and we wish to find a solution with an optimal (minimum or maximum) value. We call such a solution *an* optimal solution, as there may be several.\n\nTo develop a dynamic-programming algorithm, follow a sequence of four steps:\n1. Characterize the structure of an optimal solution.\n2. Recursively define the value of an optimal solution.\n3. Compute the value of an optimal solution, typically in a bottom-up fashion.\n4. Construct an optimal solution from computed information (if needed).\n\n## 14.1 Rod cutting\nThe rod-cutting problem is: Given a rod of length $n$ inches and a table of prices $p_i$ for $i = 1, 2, ..., n$, determine the maximum revenue $r_n$ obtainable by cutting up the rod and selling the pieces.\n\nAn optimal solution might require no cutting at all if the price $p_n$ is large enough. A rod of length $n$ can be cut in $2^{n-1}$ different ways, since there are $n-1$ independent options of cutting or not cutting at distances $1, 2, ..., n-1$ from the left end.\n\nIf an optimal solution cuts the rod into $k$ pieces of lengths $i_1, i_2, ..., i_k$, then $n = i_1 + i_2 + ... + i_k$, and the revenue is $r_n = p_{i_1} + p_{i_2} + ... + p_{i_k}$.\nOptimal solutions to a problem incorporate optimal solutions to related subproblems. We can express the optimal revenue $r_n$ for $n \nge 1$ in terms of optimal revenues from shorter rods: $r_n = \\max (p_n, r_1+r_{n-1}, r_2+r_{n-2}, ..., r_{n-1}+r_1)$. The first argument $p_n$ corresponds to making no cuts.\n\nA simpler way to define the recursive structure is to view a decomposition as a first piece of length $i$ cut off the left end, and then a right-hand remainder of length $n-i$. Only the remainder may be further divided. This leads to the recurrence: $r_n = \\max_{1 \\le i \\le n} (p_i + r_{n-i})$, where $r_0 = 0$.\n\n### Recursive top-down implementation\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 CUT-ROD]]` implements the computation based on recurrence (14.2) in a straightforward, top-down, recursive manner. This algorithm is inefficient because it solves the same subproblems repeatedly. The running time $T(n)$ satisfies $T(n) = 1 + \\sum_{j=0}^{n-1} T(j)$, which has the solution $T(n) = 2^n$.\n\n### Using dynamic programming for optimal rod cutting\nDynamic programming solves each subproblem only once. The first time a subproblem solution is computed, it is saved; subsequent needs for this solution are met by looking it up. This is a time-memory trade-off.\n\n**Top-down with memoization:** The recursive procedure is modified to save the result of each subproblem. `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 MEMOIZED-CUT-ROD]]` initializes an auxiliary array $r$ for storing results, and `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 MEMOIZED-CUT-ROD-AUX]]` performs the memoized computation. It checks if a value is known before computing.\n\n**Bottom-up method:** This approach solves subproblems in order of size, smallest first. `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 BOTTOM-UP-CUT-ROD]]` iteratively computes solutions for lengths $j = 0, 1, ..., n$. For a problem of size $j$, it directly references array entries for smaller subproblems instead of making recursive calls.\n\nBoth dynamic programming approaches run in $\\Theta(n^2)$ time for the rod-cutting problem. The bottom-up version often has better constant factors due to lower overhead for procedure calls.\n\n### Subproblem graphs\nA **subproblem graph** for a problem contains one vertex for each distinct subproblem. It has a directed edge from the vertex for subproblem $x$ to subproblem $y$ if an optimal solution for $x$ directly involves an optimal solution for $y$. The size of the subproblem graph $G=(V, E)$ can help determine the running time. Typically, it's $|V| + |E|$. For rod cutting, the graph has $n+1$ vertices and $O(n^2)$ edges.\n\n### Reconstructing a solution\nTo reconstruct the actual solution (list of piece sizes), we can extend the dynamic programming approach to store not only the optimal value but also the choice that led to it. `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 EXTENDED-BOTTOM-UP-CUT-ROD]]` computes, for each rod size $j$, the maximum revenue $r_j$ and $s_j$, the optimal size of the first piece to cut off. `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 PRINT-CUT-ROD-SOLUTION]]` then uses $s$ to print the cuts.\n\n## 14.2 Matrix-chain multiplication\nThe matrix-chain multiplication problem is: given a chain $\\langle A_1, A_2, ..., A_n \\rangle$ of $n$ matrices, where matrix $A_i$ has dimension $p_{i-1} \\times p_i$, fully parenthesize the product $A_1 A_2 ... A_n$ in a way that minimizes the number of scalar multiplications. The input is the sequence of dimensions $\\langle p_0, p_1, ..., p_n \\rangle$.\nMultiplying two matrices of size $p \\times q$ and $q \\times r$ using the standard algorithm `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 RECTANGULAR-MATRIX-MULTIPLY]]` takes $pqr$ scalar multiplications.\n\n### Counting the number of parenthesizations\nThe number of alternative parenthesizations $P(n)$ is given by the recurrence $P(n) = 1$ if $n=1$, and $P(n) = \\sum_{k=1}^{n-1} P(k)P(n-k)$ if $n \\ge 2$. This is related to Catalan numbers and grows as $\\Omega(2^n)$. Brute force is too slow.\n\n### Applying dynamic programming\n#### Step 1: The structure of an optimal parenthesization\nLet $A_{i..j}$ denote the matrix resulting from evaluating $A_i A_{i+1} ... A_j$. If $i < j$, any parenthesization must split the product between $A_k$ and $A_{k+1}$ for some $i \\le k < j$. The cost is the cost of computing $A_{i..k}$ plus $A_{k+1..j}$ plus the cost of multiplying them. An optimal solution exhibits optimal substructure: if the split is at $k$, then the parenthesizations of $A_{i..k}$ and $A_{k+1..j}$ must themselves be optimal.\n\n#### Step 2: A recursive solution\nLet $m[i, j]$ be the minimum number of scalar multiplications to compute $A_{i..j}$.\n$m[i, i] = 0$ for $i=1, ..., n$.\nFor $i < j$, $m[i, j] = \\min_{i \\le k < j} \\{ m[i, k] + m[k+1, j] + p_{i-1}p_k p_j \\}$.\nLet $s[i, j]$ be the value of $k$ that achieves this minimum.\n\n#### Step 3: Computing the optimal costs\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 MATRIX-CHAIN-ORDER]]` computes $m$ and $s$ tables bottom-up. It iterates over chain length $l=2, ..., n$. For each length $l$, it computes $m[i, j]$ for all $i, j$ such that $j-i+1=l$. The running time is $O(n^3)$, and it uses $\\Theta(n^2)$ space.\n\n#### Step 4: Constructing an optimal solution\nThe table $s$ stores the optimal split points. The procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 PRINT-OPTIMAL-PARENS]]` recursively constructs the parenthesization using the $s$ table in $O(n)$ time.\n\n## 14.3 Elements of dynamic programming\nDynamic programming applies when a problem exhibits optimal substructure and overlapping subproblems.\n\n### Optimal substructure\nA problem exhibits **optimal substructure** if an optimal solution to the problem contains within it optimal solutions to subproblems.\nDiscovering optimal substructure typically involves:\n1. Showing a solution involves making a choice, leaving one or more subproblems.\n2. Assuming a choice leading to an optimal solution is given.\n3. Determining the resulting subproblems.\n4. Using a \"cut-and-paste\" argument to show that subproblem solutions within the optimal solution must themselves be optimal.\n\nOptimal substructure varies across problems in:\n1. How many subproblems are used in an optimal solution.\n2. How many choices in determining which subproblem(s) to use.\n\n**Subtleties:**\n- Unweighted shortest path: exhibits optimal substructure. If $u \\leadsto w \\leadsto v$ is a shortest path, then $u \\leadsto w$ and $w \\leadsto v$ must be shortest paths. Subproblems are independent (share no resources beyond the common vertex $w$).\n- Unweighted longest simple path: does NOT exhibit optimal substructure. If $u \\leadsto w \\leadsto v$ is a longest simple path, $u \\leadsto w$ is not necessarily a longest simple path from $u$ to $w$, because the subproblems might not be independent (e.g., path $u \\leadsto w$ might use vertices needed for $w \\leadsto v$ to be simple).\n\nProblems with independent subproblems are suitable for dynamic programming if those subproblems overlap.\n\n### Overlapping subproblems\nThe space of subproblems must be \"small\" in that a recursive algorithm solves the same subproblems repeatedly. Dynamic programming solves each subproblem once and stores the solution. This contrasts with divide-and-conquer, where subproblems are typically new at each step of recursion.\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 RECURSIVE-MATRIX-CHAIN]]` illustrates this; without memoization, it takes exponential time.\n\n### Reconstructing an optimal solution\nOften, a separate table stores the choices made at each subproblem to facilitate reconstructing the optimal solution without re-deriving choices from the cost table.\n\n### Memoization\nMemoization is a top-down dynamic programming approach. It maintains a table of subproblem solutions, initially marked as uncomputed. When a subproblem is encountered, the algorithm checks the table. If the solution is there, it's used; otherwise, it's computed, stored, and then used. `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 MEMOIZED-MATRIX-CHAIN]]` with helper `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 LOOKUP-CHAIN]]` illustrates this for matrix-chain multiplication, achieving $O(n^3)$ time.\nBottom-up typically has better constant factors. Memoization can be more efficient if some subproblems in the space are never solved.\n\n## 14.4 Longest common subsequence\nThe longest-common-subsequence (LCS) problem is to find a maximum-length common subsequence of two given sequences $X = \\langle x_1, ..., x_m \\rangle$ and $Y = \\langle y_1, ..., y_n \\rangle$.\nA **subsequence** of $X$ is $X$ with 0 or more elements left out. A **common subsequence** of $X$ and $Y$ is a subsequence of both.\n\n### Step 1: Characterizing a longest common subsequence\nLet $X_i = \\langle x_1, ..., x_i \\rangle$ be the $i$-th prefix of $X$.\nTheorem 14.1 (Optimal substructure of an LCS): Let $Z = \\langle z_1, ..., z_k \\rangle$ be any LCS of $X$ and $Y$.\n1. If $x_m = y_n$, then $z_k = x_m = y_n$, and $Z_{k-1}$ is an LCS of $X_{m-1}$ and $Y_{n-1}$.\n2. If $x_m \\ne y_n$ and $z_k \\ne x_m$, then $Z$ is an LCS of $X_{m-1}$ and $Y$.\n3. If $x_m \\ne y_n$ and $z_k \\ne y_n$, then $Z$ is an LCS of $X$ and $Y_{n-1}$.\n\n### Step 2: A recursive solution\nLet $c[i, j]$ be the length of an LCS of $X_i$ and $Y_j$.\n$c[i, j] = 0$ if $i=0$ or $j=0$.\n$c[i, j] = c[i-1, j-1] + 1$ if $i,j > 0$ and $x_i = y_j$.\n$c[i, j] = \\max(c[i, j-1], c[i-1, j])$ if $i,j > 0$ and $x_i \\ne y_j$.\n\n### Step 3: Computing the length of an LCS\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 LCS-LENGTH]]` computes the $c$ table and an auxiliary table $b$ (to reconstruct the LCS) in $\\Theta(mn)$ time.\n\n### Step 4: Constructing an LCS\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 PRINT-LCS]]` uses table $b$ to trace back and print an LCS in $O(m+n)$ time.\n\n### Improving the code\nTable $b$ can be eliminated by re-deriving choices from table $c$ in $O(1)$ time per step. The space for table $c$ can be reduced to $O(\\min(m,n))$ if only the length is needed, by observing that computing $c[i,j]$ only requires values from row $i-1$ and row $i$. To reconstruct the LCS with reduced space is more complex.\n\n## 14.5 Optimal binary search trees\nGiven a sequence $K = \\langle k_1, ..., k_n \\rangle$ of $n$ distinct keys in sorted order, and probabilities $p_i$ that a search is for key $k_i$, and probabilities $q_i$ that a search is for a value between $k_i$ and $k_{i+1}$ (dummy keys $d_0, ..., d_n$), construct an optimal binary search tree that minimizes the expected search cost.\nThe expected cost of a search in tree $T$ is $E[\\text{search cost in } T] = \\sum_{i=1}^n (\text{depth}_T(k_i)+1)p_i + \\sum_{i=0}^n (\text{depth}_T(d_i)+1)q_i = 1 + \\sum_{i=1}^n \text{depth}_T(k_i)p_i + \\sum_{i=0}^n \text{depth}_T(d_i)q_i$. An optimal BST is not necessarily the one with smallest height.\n\n### Step 1: The structure of an optimal binary search tree\nAny subtree of a BST must contain keys in a contiguous range $k_i, ..., k_j$, along with dummy keys $d_{i-1}, ..., d_j$. If an optimal BST $T$ has a subtree $T'$ containing keys $k_i, ..., k_j$, then $T'$ must be optimal for the subproblem with these keys and dummy keys. If $k_r$ is the root of an optimal subtree for $k_i, ..., k_j$, its left child is an optimal tree for $k_i, ..., k_{r-1}$ and its right child is an optimal tree for $k_{r+1}, ..., k_j$.\n\n### Step 2: A recursive solution\nLet $e[i, j]$ be the expected cost of an optimal BST for keys $k_i, ..., k_j$. If $j=i-1$, it's just dummy key $d_{i-1}$, so $e[i, i-1] = q_{i-1}$.\nLet $w(i, j) = \\sum_{l=i}^j p_l + \\sum_{l=i-1}^j q_l$ be the sum of probabilities in the subtree for $k_i, ..., k_j$.\nIf $k_r$ is chosen as root for $k_i, ..., k_j$: $e[i,j] = p_r + (e[i, r-1] + w(i, r-1)) + (e[r+1, j] + w(r+1, j))$.\nSince $w(i,j) = w(i, r-1) + p_r + w(r+1, j)$, this simplifies to $e[i, j] = e[i, r-1] + e[r+1, j] + w(i, j)$.\nSo, $e[i, j] = \\min_{i \\le r \\le j} \\{ e[i, r-1] + e[r+1, j] + w(i, j) \\}$ for $i \\le j$.\nLet $root[i, j]$ store the index $r$ for which $k_r$ is the root of an optimal BST for $k_i, ..., k_j$.\n\n### Step 3: Computing the expected search cost of an optimal binary search tree\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14 OPTIMAL-BST]]` computes tables $e$, $w$, and $root$. The table $w$ can be computed efficiently: $w[i, i-1] = q_{i-1}$, and $w[i, j] = w[i, j-1] + p_j + q_j$. The algorithm runs in $\\Theta(n^3)$ time and uses $\\Theta(n^2)$ space. (Knuth showed an improvement to $\\Theta(n^2)$ time using an observation about $root[i,j]$ values, mentioned in Exercise 14.5-4.)\n"
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C15 Greedy Algorithms.md",
                "contents": "# 15 Greedy Algorithms\n\n## Introduction\nAlgorithms for optimization problems typically go through a sequence of steps, with a set of choices at each step. For many optimization problems, dynamic programming is overkill. A **greedy algorithm** always makes the choice that looks best at the moment, i.e., a locally optimal choice, in the hope that this choice leads to a globally optimal solution. Greedy algorithms do not always yield optimal solutions, but for many problems they do.\n\n## 15.1 An activity-selection problem\nThe problem is to select a maximum-size subset of mutually compatible activities from a set $S = \\{a_1, ..., a_n\\}$ that require exclusive use of a common resource. Each activity $a_i$ has a start time $s_i$ and a finish time $f_i$, with $0 \\le s_i < f_i < \\infty$. Activities $a_i$ and $a_j$ are compatible if $[s_i, f_i)$ and $[s_j, f_j)$ do not overlap (i.e., $s_i \\ge f_j$ or $s_j \\ge f_i$). Assume activities are sorted by monotonically increasing finish times: $f_1 \\le f_2 \\le ... \\le f_n$.\n\n### The optimal substructure of the activity-selection problem\nLet $S_{ij}$ be the set of activities that start after $a_i$ finishes and finish before $a_j$ starts. If $A_{ij}$ is a maximum set of compatible activities in $S_{ij}$ and $a_k \\in A_{ij}$, then $A_{ij} = A_{ik} \\cup \\{a_k\\} \\cup A_{kj}$, where $A_{ik}$ and $A_{kj}$ are maximum sets for $S_{ik}$ and $S_{kj}$ respectively. The size $c[i,j]$ of an optimal solution for $S_{ij}$ is $c[i,j] = c[i,k] + c[k,j] + 1$. If $S_{ij} = \\emptyset$, $c[i,j]=0$. Otherwise, $c[i,j] = \\max_{a_k \\in S_{ij}} \\{c[i,k] + c[k,j] + 1\\}$.\n\n### Making the greedy choice\nIntuition suggests choosing an activity that leaves the resource available for as many other activities as possible. Consider choosing the activity in $S$ with the earliest finish time. This is $a_1$ since activities are sorted by finish times. This choice leaves the resource available for all activities that start after $a_1$ finishes.\n\nTheorem 15.1: Consider any nonempty subproblem $S_k$ (activities that start after $a_k$ finishes), and let $a_m$ be an activity in $S_k$ with the earliest finish time. Then $a_m$ is included in some maximum-size subset of mutually compatible activities of $S_k$.\nProof: Let $A_k$ be a maximum-size subset of $S_k$, and let $a_j$ be the activity in $A_k$ with the earliest finish time. If $a_j = a_m$, we are done. If $a_j \\ne a_m$, let $A'_k = (A_k - \\{a_j\\}) \\cup \\{a_m\\}$. Since $f_m \\le f_j$ and activities in $A_k - \\{a_j\\}$ are compatible with $a_j$ (and thus start after $f_j$), they must also be compatible with $a_m$. Thus $A'_k$ is a set of mutually compatible activities, and $|A'_k| = |A_k|$. So $A'_k$ is a maximum-size subset and includes $a_m$.\n\nThis theorem implies that we can make the greedy choice (activity with earliest finish time) and then solve the subproblem of activities starting after this chosen activity finishes. Only one subproblem remains.\n\n### A recursive greedy algorithm\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15 RECURSIVE-ACTIVITY-SELECTOR]]` implements this. Given activities sorted by finish times, $s$ and $f$ arrays, an index $k$ (defining subproblem $S_k$), and total $n$. It finds the first activity $a_m$ in $S_k$ that finishes earliest (i.e., $s_m \\ge f_k$) and recursively calls for $S_m$. The initial call is RECURSIVE-ACTIVITY-SELECTOR(s, f, 0, n) with a fictitious $a_0$ where $f_0=0$. Running time is $\\Theta(n)$ if sorted.\n\n### An iterative greedy algorithm\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15 GREEDY-ACTIVITY-SELECTOR]]` is an iterative version. It selects $a_1$, then iterates through remaining activities, selecting $a_m$ if $s_m \\ge f_k$, where $a_k$ was the last activity selected. $f_k$ is always the maximum finish time of any activity in the selected set $A$ (equation 15.3). Runs in $\\Theta(n)$ if sorted.\n\n## 15.2 Elements of the greedy strategy\nA greedy algorithm makes a sequence of choices that seem best at the moment.\nGeneral steps to develop a greedy algorithm:\n1. Determine the optimal substructure.\n2. Develop a recursive solution (often bypassed).\n3. Show that if you make the greedy choice, only one subproblem remains.\n4. Prove it is always safe to make the greedy choice.\n5. Develop a recursive algorithm implementing the greedy strategy.\n6. Convert to an iterative algorithm.\n\nA more direct approach:\n1. Cast the problem as making a choice leaving one subproblem.\n2. Prove an optimal solution to the original problem includes the greedy choice.\n3. Demonstrate optimal substructure: an optimal solution to the subproblem, combined with the greedy choice, yields an optimal solution to the original problem.\n\n### Greedy-choice property\nThis property means a globally optimal solution can be arrived at by making locally optimal (greedy) choices. The choice is made without regard to solutions of subproblems. This differs from dynamic programming, where the choice may depend on subproblem solutions.\n\n### Optimal substructure\nA problem exhibits optimal substructure if an optimal solution to the problem contains optimal solutions to its subproblems. This is key for both greedy and DP.\n\n### Greedy versus dynamic programming\nBoth strategies exploit optimal substructure.\n- **0-1 knapsack problem**: Given $n$ items, each with weight $w_i$ and value $v_i$, and a knapsack of capacity $W$. Choose items to maximize total value without exceeding $W$. Items are taken entirely or not at all. Dynamic programming works. Greedy (e.g., by highest value per pound) does not necessarily yield an optimal solution because the choice might prevent filling the knapsack optimally.\n- **Fractional knapsack problem**: Same setup, but fractions of items can be taken. Greedy strategy works: compute $v_i/w_i$ for each item, take as much as possible of the item with highest $v_i/w_i$, then next highest, etc., until knapsack is full or items run out. Runs in $O(n \text{ lg } n)$ if sorting is needed.\n\n## 15.3 Huffman codes\nHuffman codes are used for data compression, achieving savings of 20-90%. They use a greedy algorithm to build an optimal way of representing characters as binary strings (codewords).\n\n### Prefix-free codes\nCodes where no codeword is a prefix of another are **prefix-free codes** (or prefix codes). This simplifies decoding: the codeword beginning an encoded file is unambiguous. Prefix-free codes can always achieve optimal data compression among character codes. They can be represented by full binary trees, where leaves are characters and paths from root to leaf define codewords (0 for left child, 1 for right child). A full binary tree for an alphabet $C$ has $|C|$ leaves and $|C|-1$ internal nodes.\nThe cost of a tree $T$ (number of bits to encode a file) is $B(T) = \\sum_{c \\in C} c.\\text{freq} \\cdot d_T(c)$, where $d_T(c)$ is depth of character $c$'s leaf (length of its codeword).\n\n### Constructing a Huffman code\nThe procedure `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15 HUFFMAN]]` builds an optimal prefix-free code. It takes a set $C$ of $n$ characters, each with $c.\\text{freq}$.\n1. Initialize a min-priority queue $Q$ with characters in $C$.\n2. For $i=1$ to $n-1$:\n   a. Allocate a new node $z$.\n   b. $x = \\text{EXTRACT-MIN}(Q)$. $z.\\text{left} = x$.\n   c. $y = \\text{EXTRACT-MIN}(Q)$. $z.\\text{right} = y$.\n   d. $z.\\text{freq} = x.\\text{freq} + y.\\text{freq}$.\n   e. $\\text{INSERT}(Q, z)$.\n3. Return $\\text{EXTRACT-MIN}(Q)$ (the root of the tree).\nThis algorithm runs in $O(n \\text{ lg } n)$ time using a binary min-heap for $Q$.\n\n### Correctness of Huffman's algorithm\nLemma 15.2 (Greedy-choice property): Let $C$ be an alphabet, $x, y \\in C$ be characters with lowest frequencies. Then there exists an optimal prefix-free code for $C$ in which codewords for $x$ and $y$ have the same length and differ only in the last bit (i.e., $x, y$ are siblings in the tree).\nProof idea: Take any optimal tree $T$. Let $a, b$ be siblings at maximum depth. If $x, y$ are $a, b$, done. Otherwise, swap $x$ with $a$ and $y$ with $b$. Since $x.\\text{freq} \\le a.\\text{freq}$ and $y.\\text{freq} \\le b.\\text{freq}$, the cost does not increase. So the new tree is also optimal and has $x, y$ as siblings at maximum depth.\n\nLemma 15.3 (Optimal-substructure property): Let $x, y$ be two characters in $C$ with minimum frequency. Let $C' = (C - \\{x,y\\}) \\cup \\{z\\}$, where $z$ is a new character with $z.\\text{freq} = x.\\text{freq} + y.\\text{freq}$. Let $T'$ be any tree representing an optimal prefix-free code for $C'$. Then the tree $T$, formed by replacing leaf $z$ in $T'$ with an internal node having $x$ and $y$ as children, represents an optimal prefix-free code for $C$.\nProof idea: $B(T) = B(T') + x.\\text{freq} + y.\\text{freq}$. If $T$ were not optimal for $C$, there'd be a $T''$ with $B(T'') < B(T)$. $T''$ can be made to have $x, y$ as siblings. Construct $T'''$ from $T''$ by replacing $x,y$ and parent with leaf $z$. Then $B(T''') < B(T')$, contradicting $T'$ optimality for $C'$.\n\nTheorem 15.4: Procedure HUFFMAN produces an optimal prefix-free code.\nProof uses Lemmas 15.2 and 15.3 by induction.\n\n## 15.4 Offline caching\nThe offline caching problem is to minimize cache misses given a sequence of $n$ memory requests $b_1, ..., b_n$ and a cache that can hold $k$ blocks. When a requested block $b_i$ is not in cache (a miss) and the cache is full, a block must be evicted.\n\nThe **furthest-in-future** strategy evicts the block in the cache whose next request in the sequence occurs furthest in the future. If a block in cache is never requested again, it's an ideal candidate.\n\n### Optimal substructure of offline caching\nLet subproblem $(C, i)$ be to process requests $b_i, ..., b_n$ given current cache contents $C$. An optimal solution $S$ to $(C,i)$ consists of a decision for $b_i$ and an optimal solution $S'$ for the resulting subproblem $(C', i+1)$.\nLet $miss(C,i)$ be the minimum misses for $(C,i)$.\n- If $i=n$: $miss(C,n)=0$ if $b_n \\in C$, $1$ if $b_n \\notin C$.\n- If $i<n$: $miss(C,i) = miss(C, i+1)$ if $b_i \\in C$.\n- If $i<n$ and $b_i \\notin C$: $miss(C,i) = 1 + \\min_{C' \\in R_{C,i}} \\{ miss(C', i+1) \\}$, where $R_{C,i}$ is the set of possible cache configurations after servicing $b_i$.\n\n### Greedy-choice property\nTheorem 15.5: Consider subproblem $(C, i)$ where cache $C$ is full ($|C|=k$) and $b_i \\notin C$. Let $z$ be the block in $C$ whose next access is furthest in the future. Evicting $z$ is part of some optimal solution.\nProof sketch (by exchange argument): Let $S$ be an optimal solution that evicts block $x \\ne z$ for request $b_i$. Construct $S'$ that evicts $z$ instead. Show $S'$ performs no more misses than $S$. If $S$ evicts $x$, its cache becomes $C_S = (C - \\{x\\}) \\cup \\{b_i\\}$. If $S'$ evicts $z$, its cache is $C_{S'} = (C - \\{z\\}) \\cup \\{b_i\\}$.\nConsider subsequent requests. If $S$ and $S'$ make same decisions, they have same misses. The critical point is when $x$ or $z$ is next requested. Since $z$ is requested after $x$ (or never), if $S'$ needs $x$ (which it has), $S$ might need $x$ (which it evicted). If $S$ needs $z$ (which it has), $S'$ would have had to evict something else to bring $z$ in (or $z$ was $b_i$). A careful case analysis shows $S'$ is no worse than $S$.\n\nThis establishes that the furthest-in-future greedy strategy is optimal."
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C16 Amortized Analysis.md",
                "contents": "# 16 Amortized Analysis\n\n## Introduction\nIn an **amortized analysis**, we average the time required to perform a sequence of data-structure operations over all the operations performed. While some operations in a sequence might be expensive, the average cost per operation can be small. Amortized analysis guarantees the average performance of each operation in the worst case, without involving probability (unlike average-case analysis).\n\nThree common techniques:\n1.  **Aggregate analysis**: Determine an upper bound $T(n)$ on the total cost of a sequence of $n$ operations. The amortized cost per operation is $T(n)/n$.\n2.  **Accounting method**: Assign different amortized costs to different operations. If an operation's amortized cost exceeds its actual cost, the difference is stored as \"credit\" with specific objects in the data structure. This credit can pay for later operations whose amortized cost is less than their actual cost.\n3.  **Potential method**: Similar to accounting, but credit is stored as \"potential energy\" of the data structure as a whole. The amortized cost of an operation is its actual cost plus the change in potential.\n\nCharges assigned are for analysis only and do not appear in code.\n\n## 16.1 Aggregate analysis\nShow that for all $n$, a sequence of $n$ operations takes $T(n)$ worst-case time in total. The amortized cost per operation is $T(n)/n$.\n\n### Stack operations\nStandard PUSH and POP operations cost $O(1)$. Consider a new operation MULTIPOP(S, k) that pops $\\min(s, k)$ objects from a stack $S$ of size $s$. `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16 MULTIPOP]]` makes $\\min(s,k)$ calls to POP, so its cost is $\\min(s,k)$.\nA sequence of $n$ PUSH, POP, and MULTIPOP operations on an initially empty stack: A single MULTIPOP can cost $O(n)$. Thus, $n$ operations could cost $O(n^2)$.\nHowever, an object can be popped (by POP or MULTIPOP) only if it was pushed. Each object is pushed at most once. So, the total number of POP operations (including those within MULTIPOP) is at most the total number of PUSH operations, which is at most $n$. The total cost for $n$ operations is $O(n)$. The amortized cost per operation is $O(n)/n = O(1)$.\n\n### Incrementing a binary counter\nConsider a $k$-bit binary counter implemented with an array $A[0..k-1]$ that counts upward from 0. `[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16 INCREMENT]]` adds 1 (modulo $2^k$). The cost of INCREMENT is the number of bits flipped.\nIn the worst case, INCREMENT flips all $k$ bits (e.g., from $011...1$ to $100...0$), costing $O(k)$. So $n$ operations could cost $O(nk)$.\nHowever, $A[0]$ flips on every call. $A[1]$ flips every other call. $A[i]$ flips every $2^i$-th call. The total number of flips for $n$ INCREMENT operations starting from 0 is $\\sum_{i=0}^{k-1} \\lfloor n/2^i \\rfloor < \\sum_{i=0}^{\\infty} n/2^i = n \\sum_{i=0}^{\\infty} (1/2)^i = 2n$. So, total cost is $O(n)$. Amortized cost per operation is $O(n)/n = O(1)$.\n\n## 16.2 The accounting method\nAssign an **amortized cost** $\\hat{c}_i$ to the $i$-th operation. If $\\hat{c}_i > c_i$ (actual cost), the difference $ \\hat{c}_i - c_i$ is stored as credit. If $\\hat{c}_i < c_i$, credit is used to pay $c_i - \\hat{c}_i$. The total credit $\\sum_{i=1}^k \\hat{c}_i - \\sum_{i=1}^k c_i$ must always be non-negative for any $k$.\n\n### Stack operations\nActual costs: PUSH=1, POP=1, MULTIPOP=min(s,k).\nAmortized costs: $\\hat{c}_{PUSH}=2$, $\\hat{c}_{POP}=0$, $\\hat{c}_{MULTIPOP}=0$.\nWhen PUSHing an object, $1 pays actual cost, $1 is stored as credit on the pushed object. When POPping, the $1 credit on the object pays for the actual cost. MULTIPOP is a sequence of POPs, each paid by credit. Total credit is non-negative (number of items on stack). Total amortized cost for $n$ operations is $O(n)$, bounding total actual cost.\n\n### Incrementing a binary counter\nActual cost = number of bits flipped. $1 per flip.\nAmortized cost to set a bit from 0 to 1: $2. ($1 pays for setting, $1 stored as credit on that bit).\nAmortized cost to set a bit from 1 to 0: $0 (paid by the credit on the bit).\nINCREMENT sets at most one bit to 1 (from 0). Other bits flipped are 1 to 0. So, amortized cost of INCREMENT $\\le 2$. Total credit $\\ge 0$ (sum of credits on 1-bits). For $n$ operations, total amortized cost $O(n)$.\n\n## 16.3 The potential method\nPrepaid work is represented as potential $\\Phi$, associated with the data structure $D_i$ after $i$-th operation. $\\Phi(D_i)$ is a real number.\nAmortized cost $\\hat{c}_i = c_i + \\Phi(D_i) - \\Phi(D_{i-1}) = c_i + \\Delta\\Phi_i$.\nTotal amortized cost $\\sum_{i=1}^n \\hat{c}_i = \\sum_{i=1}^n c_i + \\Phi(D_n) - \\Phi(D_0)$.\nIf $\\Phi(D_n) \\ge \\Phi(D_0)$, then $\\sum \\hat{c}_i$ is an upper bound on $\\sum c_i$. Often, $\\Phi(D_0)=0$ and $\\Phi(D_i) \\ge 0$ for all $i$.\n\n### Stack operations\nDefine $\\Phi(stack) = $ number of objects on the stack. $\\Phi(D_0) = 0$. Always $\\Phi(D_i) \\ge 0$.\n- PUSH: $c_i=1$. Stack size increases by 1, so $\\Delta\\Phi_i = 1$. $\\hat{c}_i = 1+1=2$.\n- POP: $c_i=1$. Stack size decreases by 1, so $\\Delta\\Phi_i = -1$. $\\hat{c}_i = 1+(-1)=0$.\n- MULTIPOP(S, k'): Actual cost $c_i=k'$. Stack size decreases by $k'$, so $\\Delta\\Phi_i = -k'$. $\\hat{c}_i = k' + (-k')=0$.\nAll operations have $O(1)$ amortized cost. Total actual cost for $n$ operations is $O(n)$.\n\n### Incrementing a binary counter\nDefine $\\Phi(counter) = b_i = $ number of 1s in the counter after $i$-th operation. Assume counter starts at 0, so $\\Phi(D_0)=0$. $\\Phi(D_i) \\ge 0$.\nFor $i$-th INCREMENT, let $t_i$ be number of bits reset from 1 to 0. At most one bit is set from 0 to 1.\nActual cost $c_i \\le t_i+1$.\nNumber of 1s after $i$-th op: $b_i \\le b_{i-1} - t_i + 1$.\nPotential change $\\Delta\\Phi_i = b_i - b_{i-1} \\le (b_{i-1} - t_i + 1) - b_{i-1} = 1 - t_i$.\nAmortized cost $\\hat{c}_i = c_i + \\Delta\\Phi_i \\le (t_i+1) + (1-t_i) = 2$.\nIf counter starts with $b_0$ ones and ends with $b_n$ ones after $n$ ops: $\\sum c_i = \\sum \\hat{c}_i - \\Phi(D_n) + \\Phi(D_0) \\le 2n - b_n + b_0$. If $k=O(n)$, total actual cost $O(n)$.\n\n## 16.4 Dynamic tables\nTables that expand or contract. Goal: $O(1)$ amortized cost for insertion/deletion.\n**Load factor** $\\alpha(T) = T.num / T.size$ (items/slots). Empty table $\\alpha=1$.\n\n### 16.4.1 Table expansion\n`[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16 TABLE-INSERT]]` inserts an item. If table is full ($T.num = T.size$), allocate new table twice the size, copy items, free old table.\nActual cost $c_i$: 1 if no expansion. $T.num$ if expansion occurs (cost $T.num_{i-1}$ to copy, 1 to insert new item; $T.num_i = T.num_{i-1}+1$. Cost is $T.num_i$). An expansion occurs when $T.num_{i-1}$ is a power of 2 (if $T.size_0=0, T.size_1=1$). So $c_i=i$ if $i-1$ is power of 2, $1$ otherwise.\n- **Aggregate analysis**: $\\sum_{i=1}^n c_i \\le n + \\sum_{j=0}^{\\lfloor \\lg n \\rfloor} 2^j < n + 2n = 3n$. Amortized cost $\\le 3$.\n- **Accounting method**: Amortized cost of TABLE-INSERT is $3. $1 pays for current elementary insertion. $1 is credit on new item (pays for moving it when table doubles). $1 is credit on an existing item (pays for moving that item). When table of size $m$ with $m/2$ items expands to $2m$, all $m/2$ old items have $1 credit, $m/2$ new items will get credit.\n- **Potential method**: $\\Phi(T) = 2 \\cdot T.num - T.size$. Assume $T.size=0$ initially, $\\Phi(D_0)=0$. Always $\\Phi(T) \\ge 0$ because $T.num \\ge T.size/2$ (load factor $\\ge 1/2$ after expansion).\n  - No expansion: $c_i=1$. $T.num_i = T.num_{i-1}+1$, $T.size_i = T.size_{i-1}$.\n    $\\Delta\\Phi_i = (2 T.num_i - T.size_i) - (2 T.num_{i-1} - T.size_{i-1}) = 2(T.num_{i-1}+1) - T.size_{i-1} - (2 T.num_{i-1} - T.size_{i-1}) = 2$.\n    $\\hat{c}_i = 1+2=3$.\n  - Expansion: $c_i = T.num_i$. $T.num_{i-1}=T.size_{i-1}$. $T.num_i = T.num_{i-1}+1$. $T.size_i = 2 T.size_{i-1}$.\n    $\\Phi(D_{i-1}) = 2 T.num_{i-1} - T.size_{i-1} = 2 T.size_{i-1} - T.size_{i-1} = T.size_{i-1}$.\n    $\\Phi(D_i) = 2 T.num_i - T.size_i = 2(T.num_{i-1}+1) - 2 T.size_{i-1} = 2(T.size_{i-1}+1) - 2 T.size_{i-1} = 2$.\n    $\\Delta\\Phi_i = \\Phi(D_i) - \\Phi(D_{i-1}) = 2 - T.size_{i-1}$.\n    $\\hat{c}_i = c_i + \\Delta\\Phi_i = T.num_i + (2 - T.size_{i-1}) = (T.size_{i-1}+1) + 2 - T.size_{i-1} = 3$.\n\n### 16.4.2 Table expansion and contraction\nTo save space, contract table if load factor too low. Strategy: double size on insert if full, halve size on delete if $\\alpha < 1/4$. Load factor $\\alpha$ is always $\\ge 1/4$. $\\alpha=1/2$ immediately after expansion/contraction.\nPotential function:\n$\\Phi(T) = 2 \\cdot T.num - T.size$ if $\\alpha(T) \\ge 1/2$.\n$\\Phi(T) = T.size/2 - T.num$ if $\\alpha(T) < 1/2$.\nThis ensures $\\Phi(T) \\ge 0$, and $\\Phi(T)=0$ when $\\alpha(T)=1/2$.\n- Analysis shows amortized costs for TABLE-INSERT and TABLE-DELETE are constant.\n  - Insertion, no expansion:\n    - If $\\alpha_{i-1} \\ge 1/2$: $\\hat{c}_i=3$. (If $\\alpha_i$ drops below $1/2$, needs special handling)\n    - If $\\alpha_{i-1} < 1/2$: $\\hat{c}_i=0$. (If $\\alpha_i$ goes to $1/2$, needs special handling)\n  - Deletion, no contraction:\n    - If $\\alpha_{i-1} \\ge 1/2$: $\\hat{c}_i=-1$. (If $\\alpha_i$ drops below $1/2$, needs special handling)\n    - If $\\alpha_{i-1} < 1/2$: $\\hat{c}_i=2$.\n  - Expansion (due to insertion): $\\hat{c}_i=3$ (as before, $\\alpha_i=1/2$ after, $\\Phi_i=0$).\n  - Contraction (due to deletion): $T.num_{i-1} = T.size_{i-1}/4$. Actual cost $c_i = T.num_{i-1}$ (copy items $T.num_i = T.num_{i-1}-1$, then delete). The actual cost for delete includes copying $num_i$ items. $c_i = num_i + 1$. If $num_{i-1}$ items were in table of $size_{i-1}$, $num_i = num_{i-1}-1$ are copied. Cost $ (num_{i-1}-1) + 1 = num_{i-1}$.\n    $\\Phi_{i-1} = T.size_{i-1}/2 - T.num_{i-1} = T.size_{i-1}/2 - T.size_{i-1}/4 = T.size_{i-1}/4$.\n    After delete and contract, $T.num_i = T.num_{i-1}-1$, $T.size_i = T.size_{i-1}/2$. $\\alpha_i = (T.num_{i-1}-1)/(T.size_{i-1}/2)$, which is just under $1/2$. $\\Phi_i = T.size_i/2 - T.num_i = (T.size_{i-1}/4) - (T.num_{i-1}-1)$.\n    This is becoming too detailed for a summary. The book states (page 470) for contraction: $c_i = \text{size}_{i-1}/4$. $\\Phi_{i-1} = \text{size}_{i-1}/4$. After op, $\text{num}_i = \text{size}_i/2 - 1$, $\\Phi_i = 1$. $\\Delta\\Phi_i = 1 - \text{size}_{i-1}/4$. $\\hat{c}_i = (\text{size}_{i-1}/4) + (1 - \text{size}_{i-1}/4) = 1$.\n    The key is that amortized cost is $O(1)$ for all operations.\n"
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartV Advanced Data Structures/C00 Introduction to Part V.md",
                "contents": "# Part V Advanced Data Structures\n\n## Introduction\n\nThis part returns to studying data structures that support operations on dynamic sets, but at a more advanced level than Part III. One of the chapters, for example, makes extensive use of the amortized analysis techniques from Chapter 16.\n\nChapter 17 shows how to augment red-black trees\u2014adding additional information in each node\u2014to support dynamic-set operations in addition to those covered in Chapters 12 and 13. The first example augments red-black trees to dynamically maintain order statistics for a set of keys. Another example augments them in a different way to maintain intervals of real numbers. Chapter 17 includes a theorem giving sufficient conditions for when a red-black tree can be augmented while maintaining the $O(\\lg n)$ running times for insertion and deletion.\n\nChapter 18 presents B-trees, which are balanced search trees specifically designed to be stored on disks. Since disks operate much more slowly than random-access memory, B-tree performance depends not only on how much computing time the dynamic-set operations consume but also on how many disk accesses they perform. For each B-tree operation, the number of disk accesses increases with the height of the B-tree, but B-tree operations keep the height low.\n\nChapter 19 examines data structures for disjoint sets. Starting with a universe of $n$ elements, each initially in its own singleton set, the operation UNION unites two sets. At all times, the $n$ elements are partitioned into disjoint sets, even as calls to the UNION operation change the members of a set dynamically. The query FIND-SET identifies the unique set that contains a given element at the moment. Representing each set as a simple rooted tree yields surprisingly fast operations: a sequence of $m$ operations runs in $O(m \\alpha(n))$ time, where $\\alpha(n)$ is an incredibly slowly growing function\u2014$\\alpha(n)$ is at most 4 in any conceivable application. The amortized analysis that proves this time bound is as complex as the data structure is simple.\n\n## Other Advanced Data Structures\n\nThe topics covered in this part are by no means the only examples of \u201cadvanced\" data structures. Other advanced data structures include the following:\n\n-   **Fibonacci heaps** [156] implement mergeable heaps (see Problem 10-2 on page 268) with the operations INSERT, MINIMUM, and UNION taking only $O(1)$ actual and amortized time, and the operations EXTRACT-MIN and DELETE taking $O(\\lg n)$ amortized time. The most significant advantage of these data structures, however, is that DECREASE-KEY takes only $O(1)$ amortized time. **Strict Fibonacci heaps** [73], developed later, made all of these time bounds actual. Because the DECREASE-KEY operation takes constant amortized time, (strict) Fibonacci heaps constitute key components of some of the asymptotically fastest algorithms to date for graph problems.\n\n-   **Dynamic trees** [415, 429] maintain a forest of disjoint rooted trees. Each edge in each tree has a real-valued cost. Dynamic trees support queries to find parents, roots, edge costs, and the minimum edge cost on a simple path from a node up to a root. Trees may be manipulated by cutting edges, updating all edge costs on a simple path from a node up to a root, linking a root into another tree, and making a node the root of the tree it appears in. One implementation of dynamic trees gives an $O(\\lg n)$ amortized time bound for each operation, while a more complicated implementation yields $O(\\lg n)$ worst-case time bounds. Dynamic trees are used in some of the asymptotically fastest network-flow algorithms.\n\n-   **Splay trees** [418, 429] are a form of binary search tree on which the standard search-tree operations run in $O(\\lg n)$ amortized time. One application of splay trees simplifies dynamic trees.\n\n-   **Persistent data structures** allow queries, and sometimes updates as well, on past versions of a data structure. For example, linked data structures can be made persistent with only a small time and space cost [126]. Problem 13-1 gives a simple example of a persistent dynamic set.\n\n-   Several data structures allow a faster implementation of dictionary operations (INSERT, DELETE, and SEARCH) for a restricted universe of keys. By taking advantage of these restrictions, they are able to achieve better worst-case asymptotic running times than comparison-based data structures. If the keys are unique integers drawn from the set $\\{0, 1, 2, \\dots, u-1\\}$, where $u$ is an exact power of 2, then a recursive data structure known as a **van Emde Boas tree** [440, 441] supports each of the operations SEARCH, INSERT, DELETE, MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR in $O(\\lg \\lg u)$ time. **Fusion trees** [157] were the first data structure to allow faster dictionary operations when the universe is restricted to integers, implementing these operations in $O(\\lg n / \\lg \\lg n)$ time. Several subsequent data structures, including **exponential search trees** [17], have also given improved bounds on some or all of the dictionary operations and are mentioned in the chapter notes throughout this book.\n\n-   **Dynamic graph data structures** support various queries while allowing the structure of a graph to change through operations that insert or delete vertices or edges. Examples of the queries that they support include vertex connectivity [214], edge connectivity, minimum spanning trees [213], biconnectivity, and transitive closure [212].\n\nChapter notes throughout this book mention additional data structures."
            },
            {
                "file": "PartV Advanced Data Structures/C17 Augmenting Data Structures.md",
                "contents": "# 17 Augmenting Data Structures\n\n## 17.0 Introduction\nSome solutions require no more than a \u201ctextbook\u201d data structure\u2014such as a doubly linked list, a hash table, or a binary search tree\u2014but many others require a dash of creativity. Rarely will you need to create an entirely new type of data structure, though. More often, you can augment a textbook data structure by storing additional information in it. You can then program new operations for the data structure to support your application. Augmenting a data structure is not always straightforward, however, since the added information must be updated and maintained by the ordinary operations on the data structure.\nThis chapter discusses two data structures based on red-black trees that are augmented with additional information. Section 17.1 describes a data structure that supports general order-statistic operations on a dynamic set: quickly finding the $i$th smallest number or the rank of a given element. Section 17.2 abstracts the process of augmenting a data structure and provides a theorem that you can use when augmenting red-black trees. Section 17.3 uses this theorem to help design a data structure for maintaining a dynamic set of intervals, such as time intervals. You can use this data structure to quickly find an interval that overlaps a given query interval.\n\n## 17.1 Dynamic order statistics\nChapter 9 introduced the notion of an order statistic. Specifically, the $i$th order statistic of a set of $n$ elements, where $i \\in \\{1, 2, \\dots, n\\}$, is simply the element in the set with the $i$th smallest key. This section shows how to modify red-black trees so that you can determine any order statistic for a dynamic set in $O(\\lg n)$ time and also compute the rank of an element\u2014its position in the linear order of the set\u2014in $O(\\lg n)$ time.\nAn **order-statistic tree** $T$ is a red-black tree with additional information stored in each node. Each node $x$ contains the usual red-black tree attributes $x.key, x.color, x.p, x.left,$ and $x.right$, along with a new attribute, $x.size$. This attribute contains the number of internal nodes in the subtree rooted at $x$ (including $x$ itself, but not including any sentinels). If we define the sentinel\u2019s size to be 0\u2014that is, we set $T.nil.size$ to be 0\u2014then we have the identity $x.size = x.left.size + x.right.size + 1$.\nKeys need not be distinct. When equal keys are present, the rank of an element is defined as the position at which it would be printed in an inorder walk of the tree.\n\n### 17.1.1 Retrieving the element with a given rank\nThe procedure `OS-SELECT(x, i)` returns a pointer to the node containing the $i$th smallest key in the subtree rooted at $x$. To find the $i$th smallest key in an order-statistic tree $T$, call `OS-SELECT(T.root, i)`. See [[PartV Advanced Data Structures Algorithms.md#17.1 OS-SELECT]].\n`OS-SELECT` works by first computing $r = x.left.size + 1$, which is the rank of node $x$ within the subtree rooted at $x$. If $i=r$, then $x$ is the $i$th smallest element. If $i < r$, the $i$th smallest element is in $x$'s left subtree, so it recurses on $x.left$. If $i > r$, the $i$th smallest element is in $x$'s right subtree, and it is the $(i-r)$th smallest element in that subtree, so it recurses on $x.right$ with rank $i-r$.\nSince each recursive call goes down one level in the $O(\\lg n)$-height tree, the total time for `OS-SELECT` is $O(\\lg n)$.\n\n### 17.1.2 Determining the rank of an element\nThe procedure `OS-RANK(T, x)` returns the position of $x$ in the linear order determined by an inorder tree walk of $T$. See [[PartV Advanced Data Structures Algorithms.md#17.1 OS-RANK]].\n`OS-RANK` works by starting with $r = x.left.size + 1$ (rank of $x$ in its own subtree) and iterating upwards. In each step, if current node $y$ was a right child of its parent $y.p$, then all nodes in $y.p$'s left subtree plus $y.p$ itself precede $x$. So, $r$ is updated by adding $y.p.left.size + 1$. The process continues until $y$ is the root.\nThe loop invariant is: At the start of each iteration of the `while` loop, $r$ is the rank of $x.key$ in the subtree rooted at node $y$.\n-   Initialization: $r = x.left.size + 1$, $y=x$. The invariant holds.\n-   Maintenance: If $y$ is a right child of $y.p$, then $x.key$ is preceded by all nodes in $y.p.left$ and $y.p$ itself. Adding $y.p.left.size + 1$ to $r$ correctly updates the rank relative to the subtree rooted at $y.p$. If $y$ is a left child, $r$ does not change as no new preceding elements from $y.p$'s scope are added relative to $y.p$.\n-   Termination: The loop terminates when $y = T.root$. $r$ is the rank of $x.key$ in the entire tree.\nThe running time is $O(\\lg n)$ since it traverses a path up to the root.\n\n### 17.1.3 Maintaining subtree sizes\nTo maintain subtree sizes during insertion and deletion:\nDuring insertion:\n1.  First phase (going down): Increment $x.size$ for each node $x$ on the path from the root. The new node gets size 1. Cost: $O(\\lg n)$.\n2.  Second phase (going up, rotations): Only rotations change structure. A rotation invalidates `size` attributes for at most two nodes. These can be updated in $O(1)$ time. For `LEFT-ROTATE(T, x)` (where $y=x.right$ becomes new root of subtree): $y.size = x.size$; then $x.size = x.left.size + x.right.size + 1$. `RIGHT-ROTATE` is symmetric. At most two rotations, so $O(1)$ additional cost. See [[PartV Advanced Data Structures Algorithms.md#17.1 LEFT-ROTATE (Order-Statistic Tree Update)]].\nTotal insertion time: $O(\\lg n)$.\nDuring deletion:\n1.  First phase: Traverse path from a moved node up to the root, decrementing `size` attributes. Cost: $O(\\lg n)$.\n2.  Second phase (rotations): At most three rotations. Updates are $O(1)$ per rotation. Cost: $O(1)$.\nTotal deletion time: $O(\\lg n)$.\n\n## 17.2 How to augment a data structure\nThe process of augmenting a data structure involves four steps:\n1.  Choose an underlying data structure.\n2.  Determine additional information to maintain in the underlying data structure.\n3.  Verify that the additional information can be maintained for the basic modifying operations on the underlying data structure.\n4.  Develop new operations.\nFor order-statistic trees: 1. Red-black trees. 2. `size` attribute. 3. Ensured updates in $O(\\lg n)$. 4. `OS-SELECT` and `OS-RANK`.\n\n### 17.2.1 Augmenting red-black trees\nTheorem 17.1 (Augmenting a red-black tree): Let $f$ be an attribute that augments a red-black tree $T$ of $n$ nodes. Suppose that the value of $f$ for each node $x$ depends only on the information in nodes $x, x.left,$ and $x.right$ (possibly including $x.left.f$ and $x.right.f$), and that $x.f$ can be computed in $O(1)$ time. Then, insertion and deletion can maintain the values of $f$ in all nodes of $T$ without asymptotically affecting the $O(\\lg n)$ running times of these operations.\nProof: A change to $x.f$ propagates only to ancestors of $x$. Updating path to root costs $O(\\lg n)$.\n-   Insertion: First phase adds node, computes its $f$, propagates up in $O(\\lg n)$. Second phase has at most two rotations. Each rotation changes $f$ for two nodes. If computing $f$ is $O(1)$ and depends only on children, updates after rotation are $O(1)$ locally. If it needs propagation, it's $O(\\lg n)$ per rotation, but CLRS implies here the total for rotations can be kept low. For attributes like `size`, update after rotation is $O(1)$. For attributes requiring full propagation, the cost is $O(\\lg n)$ if $f$ must be recomputed from affected node up to root. The theorem states the total time remains $O(\\lg n)$.\n-   Deletion: Similar logic, at most three rotations. Total time $O(\\lg n)$.\nIt is important that red-black trees require a constant number of rotations. Other balanced trees might not, making augmentation more costly.\n\n## 17.3 Interval trees\nThis section shows how to augment red-black trees to support operations on dynamic sets of intervals. Intervals are assumed to be closed $[t_1, t_2]$. An interval $i$ has attributes $i.low = t_1$ and $i.high = t_2$. Intervals $i$ and $i'$ overlap if $i.low \\le i'.high$ and $i'.low \\le i.high$.\nThe interval trichotomy states that for any two intervals $i, i'$, exactly one of these holds: a) $i$ and $i'$ overlap, b) $i$ is to the left of $i'$, c) $i$ is to the right of $i'$.\nAn **interval tree** supports:\n-   `INTERVAL-INSERT(T, x)`: adds interval $x.int$.\n-   `INTERVAL-DELETE(T, x)`: removes $x$.\n-   `INTERVAL-SEARCH(T, i)`: returns $x$ in $T$ such that $x.int$ overlaps $i$, or $T.nil$.\n\n### 17.3.1 Step 1: Underlying data structure\nA red-black tree where each node $x$ stores an interval $x.int$. Nodes are keyed by $x.int.low$.\n\n### 17.3.2 Step 2: Additional information\nEach node $x$ stores $x.max$, the maximum value of any interval endpoint stored in the subtree rooted at $x$.\n\n### 17.3.3 Step 3: Maintaining the information\n$x.max = \\max \\{ x.int.high, x.left.max, x.right.max \\}$ (assuming sentinels have $max = -\\infty$). This can be computed in $O(1)$ time. By Theorem 17.1, insertion and deletion take $O(\\lg n)$ time.\n\n### 17.3.4 Step 4: Developing new operations\nThe main new operation is `INTERVAL-SEARCH(T, i)`. See [[PartV Advanced Data Structures Algorithms.md#17.3 INTERVAL-SEARCH]].\nIt searches for an interval overlapping $i$. It starts at the root and proceeds downward. If current $x.int$ overlaps $i$, it's found. Otherwise, if $x.left \\ne T.nil$ and $x.left.max \\ge i.low$, it searches $x.left$. Else, it searches $x.right$. The running time is $O(\\lg n)$.\nCorrectness: Theorem 17.2 states that `INTERVAL-SEARCH(T, i)` either returns a node whose interval overlaps $i$, or it returns $T.nil$ and no interval in $T$ overlaps $i$.\nProof idea: If $x.int$ does not overlap $i$: \n1.  If search goes left (because $x.left.max \\ge i.low$): If an overlap exists in the right subtree, then $i.high < x.int.low$. Also, any interval $i''$ in right subtree has $i''.low \\ge x.int.low$. So $i.high < i''.low$, no overlap. So it is safe to go left. If $x.left.max < i.low$, any interval $i'$ in left subtree has $i'.high \\le x.left.max < i.low$, so no overlap in left subtree. Then search must go right.\n2.  If search goes right (because $x.left = T.nil$ or $x.left.max < i.low$): Any interval $i'$ in $x.left$ has $i'.high \\le x.left.max < i.low$. Thus $i'$ does not overlap $i$. It is safe to go right.\n\n## Chapter notes\nPreparata and Shamos [364] describe interval trees, citing Edelsbrunner (1980) and McCreight (1981). Their book details an interval tree that can enumerate all $k$ overlapping intervals in $O(k + \\lg n)$ time for a static database."
            },
            {
                "file": "PartV Advanced Data Structures/C18 B-Trees.md",
                "contents": "# 18 B-Trees\n\n## 18.0 Introduction\nB-trees are balanced search trees designed to work well on disk drives or other direct-access secondary storage devices. They are similar to red-black trees (Chapter 13) but are better at minimizing disk accesses.\nB-trees differ from red-black trees in that B-tree nodes may have many children, from a few to thousands. This \u201cbranching factor\u201d can be quite large, usually depending on disk characteristics. While an $n$-node B-tree has height $O(\\lg n)$, the base of the logarithm is much larger than for red-black trees, leading to considerably lower height.\nIf an internal B-tree node $x$ contains $x.n$ keys, then $x$ has $x.n+1$ children. The keys in node $x$ act as dividing points for the ranges of keys handled by its children. A search in a B-tree makes an $(x.n+1)$-way decision based on comparisons with the $x.n$ keys at node $x$.\n\n### 18.0.1 Data structures on secondary storage\nComputer systems use main memory (silicon chips, expensive, fast) and secondary storage (magnetic disks, SSDs; cheaper, slower, higher capacity).\nDisk drives consist of platters rotating around a spindle. Arms with read/write heads move across platters. A track is the surface passing under a stationary head. Disk access is slow due to mechanical parts: platter rotation and arm movement (latency). Typical disk rotation speeds (e.g., 7200 RPM) result in millisecond-scale latencies, orders of magnitude slower than nanosecond-scale main memory access.\nTo amortize latency, disks access data in blocks (e.g., 512 to 4096 bytes).\nPerformance analysis for disk-based structures considers:\n1.  The number of disk accesses.\n2.  CPU (computing) time.\nDisk accesses are often dominant. A B-tree application typically handles data too large for main memory. Algorithms copy selected blocks (nodes) from disk to main memory (DISK-READ) and write changed blocks back (DISK-WRITE). B-tree nodes are usually sized to be one disk block. A large branching factor (many keys/children per node) reduces tree height and thus disk accesses.\n\n## 18.1 Definition of B-trees\nSatellite information associated with a key either resides in the same node or is pointed to by the node (in which case the node stores only keys and child pointers, common in B+-trees, maximizing branching factor).\nA B-tree $T$ is a rooted tree with root $T.root$ having the following properties:\n1.  Every node $x$ has attributes:\n    a.  $x.n$: the number of keys currently stored in node $x$.\n    b.  The $x.n$ keys themselves, $x.key_1, x.key_2, \\dots, x.key_{x.n}$, stored in nondecreasing order: $x.key_1 \\le x.key_2 \\le \\dots \\le x.key_{x.n}$.\n    c.  $x.leaf$: a boolean value that is TRUE if $x$ is a leaf and FALSE if $x$ is an internal node.\n2.  Each internal node $x$ also contains $x.n+1$ pointers $x.c_1, x.c_2, \\dots, x.c_{x.n+1}$ to its children. Leaf nodes have no children, so their $c_i$ attributes are undefined.\n3.  The keys $x.key_i$ separate the ranges of keys stored in each subtree. If $k_i$ is any key stored in the subtree with root $x.c_i$, then $k_1 \\le x.key_1 \\le k_2 \\le x.key_2 \\le \\dots \\le x.key_{x.n} \\le k_{x.n+1}$.\n4.  All leaves have the same depth, which is the tree\u2019s height $h$.\n5.  Nodes have lower and upper bounds on the number of keys they can contain, expressed in terms of a fixed integer $t \\ge 2$ called the **minimum degree** of the B-tree:\n    a.  Every node other than the root must have at least $t-1$ keys. Every internal node other than the root thus has at least $t$ children. If the tree is nonempty, the root must have at least one key.\n    b.  Every node may contain at most $2t-1$ keys. Therefore, an internal node may have at most $2t$ children. A node is **full** if it contains exactly $2t-1$ keys.\nThe simplest B-tree has $t=2$, where every internal node has 2, 3, or 4 children (a 2-3-4 tree). Larger $t$ values yield shorter trees.\n\n### 18.1.1 The height of a B-tree\nTheorem 18.1: If $n \\ge 1$, then for any $n$-key B-tree $T$ of height $h$ and minimum degree $t \\ge 2$, $h \\le \\log_t \\frac{n+1}{2}$.\nProof: The root has at least 1 key. All other nodes have at least $t-1$ keys. The root has at least 2 children (if $h \\ge 1$). Each internal node (except possibly the root) has at least $t$ children. So, at depth 1 there are $\\ge 2$ nodes, depth 2 has $\\ge 2t$ nodes, depth $i$ has $\\ge 2t^{i-1}$ nodes. At depth $h$ (leaves), there are $\\ge 2t^{h-1}$ nodes. Each leaf contains $\\ge t-1$ keys. The number of keys $n$ is bounded by summing keys at each level, or simpler, by number of leaves: $n \\ge 1 + (t-1) \\sum_{i=1}^{h} 2t^{i-1}$ (number of keys in root + keys in other nodes). More simply, $n \\ge 1$ (root) $+ (t-1)$(sum of keys in nodes at level 1 to $h-1$) $+ (t-1) \\times (\text{number of leaves})$. The number of nodes at depth $h$ is at least $2t^{h-1}$. Thus $n \\ge 1 + (t-1) \times (2+2t+\\dots+2t^{h-1}) = 1 + 2(t-1) \frac{t^h-1}{t-1} = 2t^h-1$. (This is wrong, total nodes is $1 + t + t^2 + ... + t^h$ is not right. The number of keys $n$ satisfies $n \\ge 1 + (t-1) \\sum_{i=0}^{h-1} (\text{nodes at depth } i+1 \text{ from root children count}) \times (\text{min children})$. The book proof relates $n$ to the number of nodes. Number of nodes: root (1), level 1 ($\\ge 2$), level 2 ($\\ge 2t$), ..., level $h$ ($\\ge 2t^{h-1}$). Each node except root has $\\ge t-1$ keys. The root has $\\ge 1$ key. So $n \\ge 1 + (2+2t+\\dots+2t^{h-1}-1)(t-1)$. The text uses $n \\ge 1 + (t-1) \\sum_{i=1}^{h} (\text{nodes at depth } i \text{ from children of root})$. A more direct proof from the book: The root has at least 2 children. Every other internal node has at least $t$ children. Number of nodes at depth $d$: $N_0=1, N_1 \\ge 2, N_2 \\ge 2t, \text{etc., } N_h \\ge 2t^{h-1}$. Since leaves contain at least one key (actually $t-1$), $n \\ge (\text{number of leaves}) \times 1$. The text states: $n \\ge 1 + (t-1) \\sum_{i=1}^{h} (\text{number of nodes at depth } i)$. It simplifies to $n \\ge 1 + (t-1) (2 \\frac{t^h-1}{t-1}) = 2t^h - 1$. No, the book says $n \\ge 1 + (t-1) (\text{nodes at level 1} + \text{nodes at level 2} + \text{...})$. The inequality derived is $n \\ge 1 + (t-1) \times 2(1+t+t^2+...+t^{h-1}) \times \text{incorrect factor of 2 here}$.\nThe book's path is: Number of nodes at depth $0$ is $1$. Depth $1$ is $\\ge 2$. Depth $2$ is $\\ge 2t$. Depth $d$ is $\\ge 2t^{d-1}$. Depth $h$ (leaves) is $\\ge 2t^{h-1}$. Each node has $\\ge (t-1)$ keys (except root $\\ge 1$). $n \\ge 1 + (t-1)(\text{number of non-root nodes})$. Number of non-root nodes is $\\ge (2-1) + (2t-1) + ... + (2t^{h-1}-1)$. This is not what CLRS does. CLRS directly states: root has $\\ge 1$ key. All other nodes have $\\ge t-1$ keys. At depth $h$, there are $\\ge 2t^{h-1}$ nodes. The number of keys $n$ therefore satisfies the inequality: $n \\ge 1 + (t-1) \\sum_{i=1}^{h} 2t^{i-1}$. This sum is incorrect. The summation should be over nodes. $n \\ge (\text{min keys in root}) + (\text{min nodes at depth 1}) \\times (t-1) + \text{...} + (\text{min nodes at depth h}) \\times (t-1)$. Number of nodes: root (1), depth 1 ($\\ge 2$), depth 2 ($\\ge 2t$), ..., depth $h$ (leaves) $\\ge 2t^{h-1}$.  So $n \\ge 1 \\cdot (\text{keys in root}) + (N_1 + N_2 + \text{...} + N_h)(t-1)$. No, this is simpler. The total number of keys $n$ is at least $1$ (for the root) plus $(t-1)$ for each of the other nodes. The number of nodes is at least $1 + 2 + 2t + \\dots + 2t^{h-1}$. So $n \\ge 1 + (2+2t+\\dots+2t^{h-1}-1)(t-1)$. The simplest way: $n+1$ (number of items if we think of keys splitting intervals, and $n$ keys split into $n+1$ intervals at leaves) compared to $2t^h$. The book says $n \\ge 1 + (t-1) \text{[sum of } 2t^{i-1} \text{ for } i=1 \text{ to } h\text{]}$. This sum is $2(t^h-1)/(t-1)$. So $n \\ge 1 + (t-1)2(t^h-1)/(t-1) = 1 + 2(t^h-1) = 2t^h-1$. So $n+1 \\ge 2t^h$. $t^h \\le (n+1)/2$. $h \\le \\log_t ((n+1)/2)$. This is correct.\nThis height means B-trees save a factor of about $\\lg t$ in node examinations compared to red-black trees.\n\n## 18.2 Basic operations on B-trees\nConventions: Root is always in main memory. Passed nodes are already read from disk.\n\n### 18.2.1 Searching a B-tree\n`B-TREE-SEARCH(x, k)` returns $(y, i)$ where $y.key_i = k$, or NIL. See [[PartV Advanced Data Structures Algorithms.md#18.2 B-TREE-SEARCH]].\nIt performs a multiway branch at each internal node. Finds smallest index $i$ such that $k \\le x.key_i$. If $k = x.key_i$, found. If $x$ is leaf, not found. Else, DISK-READ $x.c_i$ and recurse.\nAccesses $O(h) = O(\\log_t n)$ disk blocks. CPU time $O(th) = O(t \\log_t n)$ due to linear search within nodes (can be $O(\\lg t \\cdot h)$ with binary search within nodes).\n\n### 18.2.2 Creating an empty B-tree\n`B-TREE-CREATE(T)` allocates a new root node. See [[PartV Advanced Data Structures Algorithms.md#18.2 B-TREE-CREATE]]. Uses `ALLOCATE-NODE`. $O(1)$ disk ops, $O(1)$ CPU time.\n\n### 18.2.3 Inserting a key into a B-tree\nSearch for leaf position. Cannot simply add a new leaf node. Insert key into existing leaf. If leaf is full ($2t-1$ keys), it must be split. A split involves moving the median key of the full node up to its parent. If parent is full, it must also split. This can propagate up to the root.\nTo avoid backing up, split full nodes encountered on the way down. Guarantees parent is not full when a child needs to split.\n\n#### Splitting a node in a B-tree\n`B-TREE-SPLIT-CHILD(x, i)` splits the full child $x.c_i$ of a nonfull internal node $x$. See [[PartV Advanced Data Structures Algorithms.md#18.2 B-TREE-SPLIT-CHILD]].\nNode $y = x.c_i$ has $2t-1$ keys. A new node $z$ is allocated. $z$ takes the $t-1$ largest keys from $y$. $y$ keeps the $t-1$ smallest keys. The median key of $y$ (original $y.key_t$) moves up into $x$. $z$ becomes a new child of $x$, immediately after $y$. $x$ is adjusted to accommodate the new key and child pointer. CPU time $\\Theta(t)$. $3$ DISK-WRITE operations ($x, y, z$).\n\n#### Inserting a key into a B-tree in a single pass down the tree\n`B-TREE-INSERT(T, k)`. See [[PartV Advanced Data Structures Algorithms.md#18.2 B-TREE-INSERT]].\nIf root $r$ is full ($r.n = 2t-1$), it's split using `B-TREE-SPLIT-ROOT(T)`. See [[PartV Advanced Data Structures Algorithms.md#18.2 B-TREE-SPLIT-ROOT]]. This creates a new root $s$ with 0 keys and $r$ as its child, then calls `B-TREE-SPLIT-CHILD(s,1)` to split $r$. This is the only way tree height increases.\nThen, `B-TREE-INSERT-NONFULL(s_or_r, k)` is called. See [[PartV Advanced Data Structures Algorithms.md#18.2 B-TREE-INSERT-NONFULL]].\n`B-TREE-INSERT-NONFULL(x, k)` inserts key $k$ into nonfull node $x$.\n-   If $x$ is a leaf: insert $k$ into $x$ (shifting keys if needed). DISK-WRITE($x$).\n-   If $x$ is internal: find child $x.c_i$ where $k$ should go. DISK-READ($x.c_i$). If $x.c_i$ is full, call `B-TREE-SPLIT-CHILD(x, i)`, then determine correct child (might be new $z$). Recursively call `B-TREE-INSERT-NONFULL` on the nonfull child.\nDisk accesses: $O(h)$. CPU time: $O(th)$.\n\n## 18.3 Deleting a key from a B-tree\nDeletion is analogous to insertion but more complex. Key can be deleted from any node. If deleting from an internal node, children must be rearranged.\nTo prevent a node from becoming underfull (fewer than $t-1$ keys), ensure that whenever recursion descends to a node $x$, $x$ has at least $t$ keys (one more than minimum). This may require moving a key from $x$ to a child or merging children.\nProcedure `B-TREE-DELETE(T, k)` (conceptual description):\n1.  If $k$ is in leaf node $x$: remove $k$ from $x$. (Assumes $x$ has at least $t$ keys initially, so after deletion has at least $t-1$).\n2.  If $k$ is in internal node $x$ ($k = x.key_j$): Let $y = x.c_j$ (child preceding $k$) and $z = x.c_{j+1}$ (child following $k$).\n    a.  If $y$ has at least $t$ keys: find predecessor $k'$ of $k$ in subtree rooted at $y$. Recursively delete $k'$ (from $y$'s subtree). Replace $k$ in $x$ with $k'$.\n    b.  Symmetrically, if $z$ has at least $t$ keys: find successor $k'$ of $k$ in subtree rooted at $z$. Recursively delete $k'$. Replace $k$ in $x$ with $k'$.\n    c.  If both $y$ and $z$ have $t-1$ keys: Merge $k$ and all of $z$ into $y$. Node $x$ loses $k$ and pointer to $z$. Node $y$ now contains $2t-1$ keys. Free $z$. Recursively delete $k$ from $y$.\n3.  If $k$ is not in internal node $x$: Determine child $x.c_j$ that must contain $k$. If $x.c_j$ has only $t-1$ keys, take action to ensure it has $t$ keys before recursing:\n    a.  If $x.c_j$ has an immediate sibling (say $x.c_{j+1}$) with at least $t$ keys: Give $x.c_j$ an extra key by moving $x.key_j$ down into $x.c_j$, moving $x.c_{j+1}.key_1$ (or last key of left sibling) up into $x.key_j$, and moving the appropriate child pointer from sibling to $x.c_j$.\n    b.  If $x.c_j$ and its immediate sibling(s) all have $t-1$ keys: Merge $x.c_j$ with one sibling (e.g., $x.c_{j+1}$). This involves moving $x.key_j$ down into the new merged node (which becomes median of merged node). Free one of the children. The merged node has $2t-1$ keys.\n    After ensuring $x.c_j$ has at least $t$ keys, recursively delete $k$ from subtree rooted at $x.c_j$.\nIf the root node $x$ becomes keyless (due to 2c or 3b when $x$ is root and its child is merged from/into another), $x$ is deleted and its only remaining child becomes the new root. Tree height decreases by one. This is the only way height decreases.\nDisk operations: $O(h)$. CPU time: $O(th)$.\n\n## Chapter notes\nSources: Knuth [261], Aho, Hopcroft, and Ullman [5], Sedgewick and Wayne [402]. Comer [99] provides a survey. Guibas and Sedgewick [202] discuss relations among balanced trees.\nHistory: J. E. Hopcroft invented 2-3 trees (1970). Bayer and McCreight [39] introduced B-trees (1972).\nBender, Demaine, and Farach-Colton [47] studied cache-oblivious B-trees for memory hierarchy performance."
            },
            {
                "file": "PartV Advanced Data Structures/C19 Data Structures for Disjoint Sets.md",
                "contents": "# 19 Data Structures for Disjoint Sets\n\n## 19.0 Introduction\nSome applications involve grouping $n$ distinct elements into a collection of disjoint sets (sets with no elements in common). Key operations are finding the unique set containing a given element and uniting two sets.\nThis chapter explores methods for maintaining data structures that support these operations efficiently. Section 19.1 describes operations and an application. Section 19.2 covers a linked-list implementation. Section 19.3 presents a more efficient tree-based representation. Section 19.4 analyzes the tree-based version, which achieves nearly linear time.\n\n## 19.1 Disjoint-set operations\nA disjoint-set data structure maintains a collection $\\mathcal{S} = \\{S_1, S_2, \\dots, S_k\\}$ of disjoint dynamic sets. Each set is identified by a **representative**, which is some member of the set.\nThe operations are:\n-   `MAKE-SET(x)`: Creates a new set whose only member (and thus representative) is $x$. Assumes $x$ is not already in another set.\n-   `UNION(x, y)`: Unites the dynamic sets $S_x$ and $S_y$ (containing $x$ and $y$, respectively) into a new set $S_x \\cup S_y$. The original $S_x$ and $S_y$ are destroyed. The representative of the new set can be any member from $S_x \\cup S_y$.\n-   `FIND-SET(x)`: Returns a pointer to the representative of the unique set containing $x$.\nAnalysis is in terms of $n$ (number of `MAKE-SET` operations) and $m$ (total number of operations). $m \\ge n$. At most $n-1$ `UNION` operations can occur since sets are always disjoint.\n\n### 19.1.1 An application of disjoint-set data structures\nDisjoint-set data structures are used to determine connected components of an undirected graph $G=(V,E)$. See [[PartV Advanced Data Structures Algorithms.md#19.1 CONNECTED-COMPONENTS]] and [[PartV Advanced Data Structures Algorithms.md#19.1 SAME-COMPONENT]].\n`CONNECTED-COMPONENTS(G)`:\n1.  For each vertex $v \\in G.V$, call `MAKE-SET(v)`.\n2.  For each edge $(u,v) \\in G.E$, if `FIND-SET(u) != FIND-SET(v)`, call `UNION(u,v)`.\nAfter processing all edges, `SAME-COMPONENT(u,v)` returns TRUE if `FIND-SET(u) == FIND-SET(v)`, indicating $u$ and $v$ are in the same connected component.\nThis approach is efficient, especially if edges are added dynamically. For static graphs, DFS can be faster.\n\n## 19.2 Linked-list representation of disjoint sets\nEach set is represented by its own linked list. The set object has attributes `head` (pointing to first list object) and `tail` (pointing to last). Each list object stores a set member, a pointer to the next object, and a pointer back to the set object. The representative is the member in the `head` object.\n-   `MAKE-SET(x)`: Create a new list with $x$. $O(1)$ time.\n-   `FIND-SET(x)`: Follow pointer from $x$ to its set object, return member at `head`. $O(1)$ time.\n\n### 19.2.1 A simple implementation of union\n`UNION(x,y)`: Appends $y$'s list to $x$'s list. Representative of $x$'s list becomes new representative. Update set object pointers for all members originally in $y$'s list. Time is linear in length of $y$'s list.\nA sequence of $m=2n-1$ operations can take $\\Theta(n^2)$ time (e.g., `UNION(x_i, x_1)` for $i=2, \text{...}, n$). Amortized time per operation is $\\Theta(n)$.\n\n### 19.2.2 A weighted-union heuristic\nAlways append the shorter list onto the longer list. Each list stores its length. Ties are broken arbitrarily.\nA single `UNION` can still take $\\Omega(n)$ time.\nTheorem 19.1: Using linked-list representation and weighted-union heuristic, a sequence of $m$ operations ($n$ of which are `MAKE-SET`) takes $O(m + n \\lg n)$ time.\nProof: Each `MAKE-SET` and `FIND-SET` is $O(1)$, total $O(m)$. For `UNION` operations: an object $x$'s pointer to its set object is updated only when $x$ is in the shorter list. Each time $x$'s pointer is updated, the size of its new set is at least double the size of its old set. An object's pointer is updated at most $\\lfloor \\lg n \\rfloor$ times. Total time for updating object pointers over all `UNION`s is $O(n \\lg n)$. Updating tail pointers and list lengths is $O(1)$ per `UNION`. Total time for $n-1$ `UNION`s is $O(n \\lg n)$. Total for $m$ operations is $O(m + n \\lg n)$.\n\n## 19.3 Disjoint-set forests\nSets are represented by rooted trees. Each node contains one member and points to its parent. Root of tree is representative and its own parent.\n-   `MAKE-SET(x)`: Creates a tree with single node $x$.\n-   `FIND-SET(x)`: Follows parent pointers from $x$ to root. Path traversed is the **find path**.\n-   `UNION(x,y)`: Makes root of one tree point to root of other tree.\n\n### 19.3.1 Heuristics to improve the running time\n1.  **Union by rank**: Each node $x$ stores $x.rank$, an upper bound on height of $x$. When uniting, make root with smaller rank point to root with larger rank. If ranks are equal, choose one as parent and increment its rank.\n2.  **Path compression**: During `FIND-SET(x)`, make every node on find path (from $x$ to root) point directly to root. Does not change ranks.\n\n### 19.3.2 Pseudocode for disjoint-set forests\nEach node $x$ has $x.p$ (parent) and $x.rank$.\n-   `MAKE-SET(x)`: $x.p = x, x.rank = 0$. See [[PartV Advanced Data Structures Algorithms.md#19.3 MAKE-SET]].\n-   `UNION(x,y)`: Calls `LINK(FIND-SET(x), FIND-SET(y))`. See [[PartV Advanced Data Structures Algorithms.md#19.3 UNION]].\n-   `LINK(x,y)`: (Assumes $x, y$ are roots) Implements union by rank. If $x.rank > y.rank$, then $y.p = x$. Else $x.p = y$; if $x.rank == y.rank$, then $y.rank = y.rank + 1$. See [[PartV Advanced Data Structures Algorithms.md#19.3 LINK]].\n-   `FIND-SET(x)`: Implements path compression recursively. If $x \\ne x.p$, then $x.p = \\text{FIND-SET}(x.p)$. Return $x.p$. See [[PartV Advanced Data Structures Algorithms.md#19.3 FIND-SET]]. This is a two-pass method (pass up to find root, pass down to update pointers).\n\n### 19.3.3 Effect of the heuristics on the running time\n-   Union by rank alone: $O(m \\lg n)$. This bound is tight.\n-   Path compression alone (for $n$ `MAKE-SET`s and $f$ `FIND-SET`s): $\\Theta(n + f \\cdot (1 + \\log_{2+f/n} n))$.\n-   Combined heuristics: $O(m \\alpha(n))$, where $\\alpha(n)$ is the very slowly growing inverse of Ackermann's function. For practical purposes, $\\alpha(n) \\le 4$.\n\n## 19.4 Analysis of union by rank with path compression\n\n### 19.4.1 A very quickly growing function and its very slowly growing inverse\nDefine $A_k(j)$ for integers $j,k \\ge 0$:\n$A_k(j) = j+1$ if $k=0$.\n$A_k(j) = A_{k-1}^{(j+1)}(j)$ if $k \\ge 1$, where $A_{k-1}^{(i)}(j)$ is $A_{k-1}$ applied $i$ times to $j$.\n(CLRS page 532 actually has $A_k(j) = A_{k-1}^{(j)}(j)$ for $k \\ge 1$ and $A_0(j) = j+1$. The OCR on page 57 implies $A_k(j) = A_{k-1}^{(j+1)}(j)$ if $k=1$ and $A_k(j)=A_{k-1}^{(A_k(j-1))}(1)$ for $k \\ge 2, j \\ge 1$. The definition in text (19.1) on page 532 is $A_k(j) = A_{k-1}^{(j+1)}(j)$ for $k \\ge 1$, and $A_k(j)$ refers to $A_{k-1}$ applied $A_k(j-1)$ times when $j>1$. Simpler CLRS text (edition 3): $A_0(j) = j+1$. $A_k(j) = A_{k-1}^{(j+1)}(1)$ for $k \\ge 1$. Let's use (19.1) from OCR (p.57): $A_k(j) = j+1$ if $k=0$; $A_k(j) = A_{k-1}^{(j+1)}(j)$ if $k \\ge 1$. This is incorrect per CLRS 3rd/4th ed, which defines $A_k(j)$ where $j$ refers to number of applications. The definition on page 532 of this OCR is: $A_k(j) = j+1$ if $k=0$, $A_k(j) = A_{k-1}^{(j+1)}(j)$ if $k \\ge 1$. Wait, equation (19.1) on OCR page 57 shows $A_k(j) = A_{k-1}^{(A_k(j-1))}(j)$ which is standard Ackermann. The text following says $A_k(j)$ for $A_{k-1}^{(j+1)}(j)$ uses functional iteration $A_{k-1}(A_{k-1}(...))$, not $A_{k-1}^{(A_k(j-1))}$. This is confusing in OCR. Standard CLRS definition (e.g. 3rd ed, p. 573): $A_k(j) = j+1$ for $k=0$. $A_k(j) = A_{k-1}^{(j+1)}(1)$ for $k \\ge 1$.  Let's use the simpler $A_k(1)$ growth from text:\n$A_0(1)=2$.\n$A_1(1)=A_0^{(1+1)}(1) = A_0(A_0(1)) = A_0(2) = 3$.\n$A_2(1)=A_1^{(1+1)}(1) = A_1(A_1(1)) = A_1(3) = 2(3)+1=7$ (using Lemma 19.2: $A_1(j)=2j+1$).\n$A_3(1)=A_2^{(1+1)}(1) = A_2(A_2(1)) = A_2(7) = 2^{7+1}(7+1)-1 = 2^8 \\cdot 8 - 1 = 2048-1 = 2047$.\n$A_4(1)=A_3^{(1+1)}(1) = A_3(A_3(1)) = A_3(2047)$. This is $A_2^{(2047+1)}(1) = A_2^{(2048)}(1)$ which is huge, $ > 10^{80}$.\nInverse: $\\alpha(n) = \\min\\{k : A_k(1) \\ge n\\}$. $\\alpha(n) \\le 4$ for practical $n$.\n\n### 19.4.2 Properties of ranks\nLemma 19.4: For any node $x$, $x.rank \\le x.p.rank$, with strict inequality if $x \\ne x.p$. $x.rank$ is initially 0, increases until $x \\ne x.p$, then constant. $x.p.rank$ monotonically increases.\nCorollary 19.5: Ranks strictly increase on any simple path up toward a root.\nLemma 19.6: Every node has rank at most $n-1$. (Tighter bound is $\\lfloor \\lg n \\rfloor$.)\n\n### 19.4.3 Proving the time bound\nUses potential method. Convert `UNION` to two `FIND-SET`s and one `LINK`.\nLemma 19.7: If converted sequence $S$ takes $O(m \\alpha(n))$ time, original $S'$ (with $m'$ ops) takes $O(m' \\alpha(n))$ time ($m = \\Theta(m')$).\nPotential function $\\Phi_q = \\sum_x \\phi_q(x)$.\nFor a node $x$ after $q$ operations:\nIf $x$ is a root or $x.rank=0$: $\\phi_q(x) = \\alpha(n) \\cdot x.rank$.\nIf $x$ is not a root and $x.rank \\ge 1$: $\\phi_q(x) = (\\alpha(n) - \\text{level}(x)) \\cdot x.rank - \\text{iter}(x)$.\nWhere $\\text{level}(x) = \\max\\{k : x.p.rank \\ge A_k(x.rank)\\}$. $0 \\le \\text{level}(x) < \\alpha(n)$.\nAnd $\\text{iter}(x) = \\max\\{i : x.p.rank \\ge A_{\\text{level}(x)}^{(i)}(x.rank)\\}$. $1 \\le \\text{iter}(x) \\le x.rank$.\nLemma 19.8: $0 \\le \\phi_q(x) \\le \\alpha(n) \\cdot x.rank$.\nCorollary 19.9: If $x$ not root and $x.rank > 0$, then $\\phi_q(x) < \\alpha(n) \\cdot x.rank$.\n\n### 19.4.4 Potential changes and amortized costs of operations\nLemma 19.10: For non-root $x$, after a LINK or FIND-SET, $\\phi_q(x) \\le \\phi_{q-1}(x)$. If $x.rank > 0$ and $\\text{level}(x)$ or $\\text{iter}(x)$ changes, then $\\phi_q(x) \\le \\phi_{q-1}(x) - 1$.\nLemma 19.11: Amortized cost of `MAKE-SET` is $O(1)$. (Actual $O(1)$, $\\Delta\\Phi = 0$).\nLemma 19.12: Amortized cost of `LINK` is $O(\\alpha(n))$. (Actual $O(1)$. Max potential increase for parent $y$ is $\\alpha(n)$ if its rank increments. Other nodes' potentials do not increase or decrease).\nLemma 19.13: Amortized cost of `FIND-SET` is $O(\\alpha(n))$.\nProof: Actual cost is $O(s)$ for path of length $s$. Potential does not increase for any node. At least $\\max\\{0, s - (\\alpha(n)+2)\\}$ nodes on path decrease potential by at least 1. Amortized cost $\\le O(s) - (s - (\\alpha(n)+2)) \\cdot 1 = O(\\alpha(n))$. (Constant factor in $O(s)$ absorbed by scaling potential units).\nTheorem 19.14: Sequence of $m$ operations takes $O(m \\alpha(n))$ time.\n\n## Chapter notes\nKey results by Tarjan [427, 429] (inverse Ackermann bound). Hopcroft and Ullman [5, 227] (earlier $O(m \\lg^* n)$ bound). Analysis in Section 19.4 adapted from Tarjan [431], based on Kozen [270]. Harfst and Reingold [209] (potential-based version of Tarjan's earlier bound).\nTarjan and van Leeuwen [432] (one-pass variants). Goel et al. [182] (random linking). Gabow and Tarjan [166] ($O(m)$ time for certain applications). Lower bound $\\Omega(m \\hat{\\alpha}(m,n))$ by Tarjan [428], generalized by Fredman and Saks [155]."
            },
            {
                "file": "PartV Advanced Data Structures Algorithms.md",
                "contents": "# Part V Advanced Data Structures Algorithms\n\n# C17 Augmenting Data Structures Algorithms\n\n## 17.1 OS-SELECT\n```\nOS-SELECT(x, i)\n1  r = x.left.size + 1  // rank of x within the subtree rooted at x\n2  if i == r\n3      return x\n4  elseif i < r\n5      return OS-SELECT(x.left, i)\n6  else return OS-SELECT(x.right, i - r)\n```\n\n## 17.1 OS-RANK\n```\nOS-RANK(T, x)\n1  r = x.left.size + 1  // rank of x within the subtree rooted at x\n2  y = x\n3  while y != T.root\n4      if y == y.p.right\n5          r = r + y.p.left.size + 1 // add in parent and its left subtree\n6      y = y.p                     // move y toward the root\n7  return r\n```\n\n## 17.1 LEFT-ROTATE (Order-Statistic Tree Update)\nLines to add to `LEFT-ROTATE(T, x)` (where $y = x.right$):\n```\n// ... original LEFT-ROTATE lines ...\n// After structural changes and y becomes root of subtree previously rooted by x:\n13  y.size = x.size\n14  x.size = x.left.size + x.right.size + 1\n// ... potentially other updates if x.p was involved ...\n```\n(Note: `RIGHT-ROTATE` is symmetric.)\n\n## 17.3 INTERVAL-SEARCH\n```\nINTERVAL-SEARCH(T, i)\n1  x = T.root\n2  while x != T.nil and i does not overlap x.int\n3      if x.left != T.nil and x.left.max >= i.low\n4          x = x.left  // overlap in left subtree or no overlap in right subtree\n5      else x = x.right // no overlap in left subtree\n6  return x\n```\n\n# C18 B-Trees Algorithms\n\n## 18.2 B-TREE-SEARCH\n```\nB-TREE-SEARCH(x, k)\n1  i = 1\n2  while i <= x.n and k > x.key[i]  // CLRS uses 1-based indexing for keys array x.key\n3      i = i + 1\n4  if i <= x.n and k == x.key[i]\n5      return (x, i)\n6  elseif x.leaf\n7      return NIL\n8  else DISK-READ(x.c[i]) // CLRS uses 1-based indexing for children array x.c\n9      return B-TREE-SEARCH(x.c[i], k)\n```\n\n## 18.2 B-TREE-CREATE\n```\nB-TREE-CREATE(T)\n1  x = ALLOCATE-NODE()\n2  x.leaf = TRUE\n3  x.n = 0\n4  DISK-WRITE(x)\n5  T.root = x\n```\n\n## 18.2 B-TREE-SPLIT-CHILD\n```\nB-TREE-SPLIT-CHILD(x, i)\n1  y = x.c[i]                        // y is x's ith child, assumed full\n2  z = ALLOCATE-NODE()\n3  z.leaf = y.leaf\n4  z.n = t - 1\n5  for j = 1 to t - 1                // Copy y's t-1 largest keys to z\n6      z.key[j] = y.key[j+t]\n7  if not y.leaf                     // If y is internal, copy its t largest children to z\n8      for j = 1 to t\n9          z.c[j] = y.c[j+t]\n10 y.n = t - 1                       // y now has t-1 keys\n11 for j = x.n + 1 downto i + 1    // Shift children of x to make room for new child z\n12     x.c[j+1] = x.c[j]\n13 x.c[i+1] = z                      // Link z as child of x\n14 for j = x.n downto i              // Shift keys of x to make room for y's median key\n15     x.key[j+1] = x.key[j]\n16 x.key[i] = y.key[t]               // Move y's median key to x\n17 x.n = x.n + 1\n18 DISK-WRITE(y)\n19 DISK-WRITE(z)\n20 DISK-WRITE(x)\n```\n\n## 18.2 B-TREE-INSERT\n```\nB-TREE-INSERT(T, k)\n1  r = T.root\n2  if r.n == 2*t - 1\n3      s = ALLOCATE-NODE()\n4      T.root = s\n5      s.leaf = FALSE\n6      s.n = 0\n7      s.c[1] = r\n8      B-TREE-SPLIT-CHILD(s, 1)\n9      B-TREE-INSERT-NONFULL(s, k)\n10 else B-TREE-INSERT-NONFULL(r, k)\n```\n(Note: `B-TREE-SPLIT-ROOT` is implicitly lines 3-8 of `B-TREE-INSERT`)\n\n## 18.2 B-TREE-SPLIT-ROOT\n(This is effectively lines 3-8 within `B-TREE-INSERT` when the root is full. As a standalone procedure based on CLRS page 509 text (Fig 18.6 based call structure):\n```\nB-TREE-SPLIT-ROOT(T) // T is the tree, T.root is the full root\n1  s = ALLOCATE-NODE()\n2  s.leaf = FALSE\n3  s.n = 0\n4  s.c[1] = T.root // Old root becomes child of new root s\n5  T.root = s      // New root s is now T.root\n6  B-TREE-SPLIT-CHILD(s, 1) // Split the old root (which is s.c[1])\n7  return s // Return the new root\n```\n\n## 18.2 B-TREE-INSERT-NONFULL\n```\nB-TREE-INSERT-NONFULL(x, k)\n1  i = x.n\n2  if x.leaf\n3      while i >= 1 and k < x.key[i]\n4          x.key[i+1] = x.key[i]\n5          i = i - 1\n6      x.key[i+1] = k\n7      x.n = x.n + 1\n8      DISK-WRITE(x)\n9  else \n10     while i >= 1 and k < x.key[i]\n11         i = i - 1\n12     i = i + 1\n13     DISK-READ(x.c[i])\n14     if x.c[i].n == 2*t - 1\n15         B-TREE-SPLIT-CHILD(x, i)\n16         if k > x.key[i]   // Key k might go to the new child created by split\n17             i = i + 1\n18     B-TREE-INSERT-NONFULL(x.c[i], k)\n```\n\n# C19 Data Structures for Disjoint Sets Algorithms\n\n## 19.1 CONNECTED-COMPONENTS\n```\nCONNECTED-COMPONENTS(G)\n1  for each vertex v in G.V\n2      MAKE-SET(v)\n3  for each edge (u,v) in G.E\n4      if FIND-SET(u) != FIND-SET(v)\n5          UNION(u,v)\n```\n\n## 19.1 SAME-COMPONENT\n```\nSAME-COMPONENT(u,v)\n1  if FIND-SET(u) == FIND-SET(v)\n2      return TRUE\n3  else return FALSE\n```\n\n## 19.3 MAKE-SET (Forests)\n```\nMAKE-SET(x)\n1  x.p = x\n2  x.rank = 0\n```\n\n## 19.3 UNION (Forests)\n```\nUNION(x,y)\n1  LINK(FIND-SET(x), FIND-SET(y))\n```\n\n## 19.3 LINK (Forests)\n```\nLINK(x,y) // x and y are roots\n1  if x.rank > y.rank\n2      y.p = x\n3  else x.p = y\n4      if x.rank == y.rank\n5          y.rank = y.rank + 1\n```\n\n## 19.3 FIND-SET (Forests with Path Compression)\n```\nFIND-SET(x)\n1  if x != x.p\n2      x.p = FIND-SET(x.p)\n3  return x.p\n```\n"
            }
        ]
    }
]