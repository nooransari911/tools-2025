[
    {
        "items": [
            {
                "file": "foundations overview.md",
                "contents": "# Part I: Foundations Overview\n\nThis document provides a distilled overview of core concepts from Part I: Foundations, covering Chapters 1-3 based on the provided materials, with general mentions for Chapters 4-5.\n\n# 1 Chapter 1: The Role of Algorithms in Computing\n\nAn *algorithm* is any well-defined computational procedure that takes some value, or set of values, as *input* and produces some value, or set of values, as *output* in a finite amount of time. An algorithm is a sequence of computational steps that transform the input into the output.\nA *computational problem* specifies the desired input/output relationship. An *instance of a problem* consists of the input needed to compute a solution to the problem.\nA correct algorithm halts with the correct output for every input instance. Incorrect algorithms might not halt or halt with an incorrect answer but can sometimes be useful if their error rate is controllable.\n\n*Data structures* are ways to store and organize data to facilitate access and modifications. No single data structure works well for all purposes.\n\n*Hard problems*, such as NP-complete problems, are those for which no known efficient algorithm exists. If an efficient algorithm exists for one NP-complete problem, it exists for all. For such problems, approximation algorithms that find a good, but not necessarily optimal, solution are often used.\n\n# 2 Chapter 2: Getting Started\n\nThis chapter introduces algorithm analysis using insertion sort and merge sort.\n\nA *loop invariant* is a property of a loop that helps prove its correctness. It relies on three properties:\n-   **Initialization**: It is true prior to the first iteration of the loop.\n-   **Maintenance**: If it is true before an iteration of the loop, it remains true before the next iteration.\n-   **Termination**: When the loop terminates, the invariant (usually along with the reason for termination) gives a useful property that helps show the algorithm is correct.\n\nAlgorithm analysis involves predicting resources required, primarily computational time.\n-   *Input size* ($n$): Depends on the problem (e.g., number of items, total bits).\n-   *Running time*: Number of primitive operations or \"steps\" executed on a RAM model.\n-   *Worst-case analysis*: Longest running time for any input of size $n$. Provides an upper bound and is often the focus because it occurs frequently for some problems, and the average case can be as bad.\n-   *Average-case analysis*: Expected running time over all inputs of a given size, assuming a certain probability distribution of inputs.\n-   *Order of growth* (rate of growth): Simplifies analysis by focusing on the leading term of the running time formula and ignoring constant coefficients, especially for large inputs.\n\nThe *divide-and-conquer* method involves three steps:\n1.  **Divide** the problem into smaller subproblems.\n2.  **Conquer** the subproblems by solving them recursively. If subproblems are small enough (base case), solve them directly.\n3.  **Combine** the solutions to the subproblems into the solution for the original problem.\n\nMerge sort is an example: $T(n) = 2T(n/2) + \\Theta(n)$, which solves to $T(n) = \\Theta(n \\lg n)$.\nA *recurrence equation* or *recurrence* describes the overall running time on a problem of size $n$ in terms of the running time on smaller inputs.\n\n# 3 Chapter 3: Characterizing Running Times\n\nThis chapter formalizes asymptotic notation to describe algorithm efficiency.\n\n*Asymptotic notations* provide bounds on function growth:\n-   **$\\Theta(g(n))$ (Theta-notation)**: Asymptotically tight bound.\n    $\\Theta(g(n)) = \\{f(n) : \\exists c_1 > 0, c_2 > 0, n_0 > 0 \\text{ s.t. } 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) \\forall n \\ge n_0\\}$.\n-   **$O(g(n))$ (O-notation / Big-O)**: Asymptotic upper bound.\n    $O(g(n)) = \\{f(n) : \\exists c > 0, n_0 > 0 \\text{ s.t. } 0 \\le f(n) \\le cg(n) \\forall n \\ge n_0\\}$.\n-   **$\\Omega(g(n))$ (Omega-notation / Big-Omega)**: Asymptotic lower bound.\n    $\\Omega(g(n)) = \\{f(n) : \\exists c > 0, n_0 > 0 \\text{ s.t. } 0 \\le cg(n) \\le f(n) \\forall n \\ge n_0\\}$.\n-   **$o(g(n))$ (o-notation / little-o)**: Asymptotic upper bound that is not tight.\n    $o(g(n)) = \\{f(n) : \\forall c > 0, \\exists n_0 > 0 \\text{ s.t. } 0 \\le f(n) < cg(n) \\forall n \\ge n_0\\}$. This implies $\\lim_{n\\to\\infty} f(n)/g(n) = 0$.\n-   **$\\omega(g(n))$ (omega-notation / little-omega)**: Asymptotic lower bound that is not tight.\n    $\\omega(g(n)) = \\{f(n) : \\forall c > 0, \\exists n_0 > 0 \\text{ s.t. } 0 \\le cg(n) < f(n) \\forall n \\ge n_0\\}$. This implies $\\lim_{n\\to\\infty} f(n)/g(n) = \\infty$.\n\nTheorem 3.1: $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$.\n\n# 4 Chapter 4: Divide-and-Conquer (General Overview)\nThis chapter typically delves deeper into the divide-and-conquer paradigm, presenting more examples like Strassen's algorithm for matrix multiplication. It details methods for solving recurrences:\n-   *Substitution method*: Guess a bound and prove by induction.\n-   *Recursion-tree method*: Convert recurrence into a tree to sum costs at each level. Provides intuition and helps guess bounds.\n-   *Master method*: Provides a cookbook method for solving recurrences of the form $T(n) = aT(n/b) + f(n)$.\n\n# 5 Chapter 5: Probabilistic Analysis and Randomized Algorithms (General Overview)\nThis chapter introduces techniques to analyze algorithms whose behavior depends on randomness, either in the input or in the algorithm itself.\n-   *Indicator random variables*: A simple yet powerful tool to compute expected values. $E[X] = \\sum E[X_i]$.\n-   Common problems analyzed include:\n    -   *The hiring problem*: Expected cost $O(c_h \\ln n)$.\n    -   *The birthday paradox*: With $k$ people, expected pairs with same birthday. With $\\approx \\sqrt{2n}$ people, probability $> 1/2$ of shared birthday in a year of $n$ days.\n    -   *Balls and bins*: Expected tosses to fill $n$ bins is $\\Theta(n \\ln n)$.\nRandomized algorithms use random numbers to make choices, often leading to good expected performance regardless of input distribution."
            },
            {
                "file": "PartI Foundations Algorithms.md",
                "contents": "# Part I Foundations Algorithms\n\n# C2 Getting Started Algorithms\n\n## C2.1 INSERTION-SORT\n```\nINSERTION-SORT(A, n)\n1  for i = 2 to n\n2      key = A[i]\n3      // Insert A[i] into the sorted subarray A[1:i-1].\n4      j = i - 1\n5      while j > 0 and A[j] > key\n6          A[j+1] = A[j]\n7          j = j - 1\n8      A[j+1] = key\n```\n\n## C2.1 SUM-ARRAY\n(From Exercise 2.1-2 context, for adding numbers in an array)\n```\nSUM-ARRAY(A, n)\n1  sum = 0\n2  for i = 1 to n\n3      sum = sum + A[i]\n4  return sum\n```\n\n## C2.3 MERGE\n```\nMERGE(A, p, q, r)\n1  n_L = q - p + 1\n2  n_R = r - q\n3  let L[0:n_L-1] and R[0:n_R-1] be new arrays\n4  for i = 0 to n_L - 1  // copy A[p:q] into L[0:n_L-1]\n5      L[i] = A[p+i]\n6  for j = 0 to n_R - 1  // copy A[q+1:r] into R[0:n_R-1]\n7      R[j] = A[q+j+1]\n8  i = 0  // i indexes the smallest remaining element in L\n9  j = 0  // j indexes the smallest remaining element in R\n10 k = p  // k indexes the location in A to fill\n11 // As long as each of the arrays L and R contains an unmerged element,\n12 //   copy the smallest unmerged element back into A[p:r].\n13 while i < n_L and j < n_R\n14     if L[i] <= R[j]\n15         A[k] = L[i]\n16         i = i + 1\n17     else A[k] = R[j]\n18         j = j + 1\n19     k = k + 1\n20 // Having gone through one of L and R entirely, copy the\n21 //   remainder of the other to the end of A[p:r].\n22 while i < n_L\n23     A[k] = L[i]\n24     i = i + 1\n25     k = k + 1\n26 while j < n_R\n27     A[k] = R[j]\n28     j = j + 1\n29     k = k + 1\n```\n\n## C2.3 MERGE-SORT\n```\nMERGE-SORT(A, p, r)\n1  if p >= r\n2      return\n3  q = floor((p+r)/2)  // midpoint of A[p:r]\n4  MERGE-SORT(A, p, q)  // recursively sort A[p:q]\n5  MERGE-SORT(A, q+1, r) // recursively sort A[q+1:r]\n6  // Merge A[p:q] and A[q+1:r] into A[p:r].\n7  MERGE(A, p, q, r)\n```\n\n## C2P2-2 BUBBLESORT\n(From Problem 2-2)\n```\nBUBBLESORT(A, n)\n1  for i = 1 to n-1\n2      for j = n downto i+1\n3          if A[j] < A[j-1]\n4              exchange A[j] with A[j-1]\n```\n\n## C2P2-3 HORNER\n(From Problem 2-3)\n```\nHORNER(A, n, x) // A is array A[0:n] of coefficients\n1  p = 0\n2  for i = n downto 0\n3      p = A[i] + x * p\n4  return p\n```"
            },
            {
                "file": "PartI Foundations/C1 The Role of Algorithms in Computing.md",
                "contents": "# 1 The Role of Algorithms in Computing\n\nThis chapter answers: What are algorithms? Why is the study of algorithms worthwhile? What is the role of algorithms relative to other technologies used in computers?\n\n## 1.1 Algorithms\n\nInformally, an *algorithm* is any well-defined computational procedure that takes some value, or set of values, as *input* and produces some value, or set of values, as *output* in a finite amount of time. An algorithm is thus a sequence of computational steps that transform the input into the output.\nAn algorithm can also be viewed as a tool for solving a well-specified *computational problem*. The problem statement specifies in general terms the desired input/output relationship. The algorithm describes a specific computational procedure for achieving that relationship for all problem instances.\n\nExample: The sorting problem.\n*Input*: A sequence of $n$ numbers $(a_1, a_2, \\dots, a_n)$.\n*Output*: A permutation (reordering) $(a'_1, a'_2, \\dots, a'_n)$ of the input sequence such that $a'_1 \\le a'_2 \\le \\dots \\le a'_n$.\nAn *instance of a problem* consists of the input (satisfying constraints) needed to compute a solution. For sorting, an instance is a specific sequence like $(31, 41, 59, 26, 41, 58)$.\n\nAn algorithm is *correct* if, for every problem instance provided as input, it halts and outputs the correct solution. An incorrect algorithm might not halt or might halt with an incorrect answer. Incorrect algorithms can sometimes be useful if their error rate can be controlled.\nAn algorithm can be specified in English, as a computer program, or as a hardware design. The specification must provide a precise description of the computational procedure.\n\nWhat kinds of problems are solved by algorithms?\n-   The Human Genome Project: Identifying genes, determining DNA sequences, storing information, and developing analysis tools all require sophisticated algorithms (e.g., dynamic programming for sequence similarity).\n-   The Internet: Managing and manipulating large volumes of data, finding routes for data travel (Chapter 22), and search engines (Chapters 11, 32).\n-   Electronic Commerce: Relies on privacy of information (credit cards, passwords) using public-key cryptography and digital signatures (Chapter 31), based on numerical algorithms and number theory.\n-   Manufacturing and Commercial Enterprises: Allocating scarce resources beneficially, often modeled as linear programs (Chapter 29).\n\nSpecific problems include:\n-   Shortest path in a graph (Chapter 22).\n-   Topological sorting of parts in a mechanical design (Chapter 20).\n-   Clustering for medical diagnosis (Chapter 33).\n-   Text compression using Huffman coding (Chapter 15).\n\nTwo common characteristics of algorithmic problems:\n1.  They have many candidate solutions, most of which do not solve the problem. Finding a correct or best solution can be challenging without examining every possibility.\n2.  They have practical applications.\n\nNot all problems have an easily identified set of candidate solutions, e.g., discrete Fourier transform (Chapter 30).\n\nA *data structure* is a way to store and organize data to facilitate access and modifications. Appropriate data structures are crucial for algorithm design.\n\nThis book teaches techniques of algorithm design and analysis to develop algorithms, prove correctness, and analyze efficiency. Chapters address specific problems (e.g., medians, minimum spanning trees, max flow) and techniques (e.g., divide-and-conquer, dynamic programming, amortized analysis).\n\n*Hard problems*, such as NP-complete problems (Chapter 34), are those for which no known algorithm runs in a reasonable amount of time. Key points about NP-complete problems:\n1.  No efficient algorithm has been found, but none proven impossible.\n2.  If an efficient algorithm exists for one, it exists for all.\n3.  They are similar to problems with known efficient algorithms; small changes in problem statements can lead to large changes in efficiency.\nIf a problem is NP-complete, one might develop an *approximation algorithm* that gives a good, but not necessarily optimal, solution (Chapter 35). Example: traveling-salesperson problem.\n\nAlternative computing models:\n-   *Parallel computers* (multicore): Require algorithms designed for parallelism (Chapter 26, task-parallel algorithms).\n-   *Online algorithms*: Receive input over time and must make decisions without knowing future data (Chapter 27).\n\n## 1.2 Algorithms as a technology\n\nEfficiency matters. Even if computers were infinitely fast, correctness and termination would still be important. Since computers are not infinitely fast, computing time is a bounded, precious resource. Memory is also bounded.\n\nDifferent algorithms for the same problem can differ dramatically in efficiency, more so than differences due to hardware/software.\nExample: Insertion sort ($c_1 n^2$ time) vs. Merge sort ($c_2 n \\lg n$ time).\n-   Insertion sort is often faster for small $n$ due to smaller $c_1$.\n-   For large $n$, merge sort's $n \\lg n$ growth is far superior to $n^2$.\n    A concrete example: Computer A (10 billion instructions/sec, $2n^2$ instructions for insertion sort) vs. Computer B (10 million instructions/sec, $50n \\lg n$ instructions for merge sort) to sort 10 million numbers.\n    -   Computer A: $2 \\cdot (10^7)^2 / 10^{10} = 20,000$ seconds (approx 5.5 hours).\n    -   Computer B: $50 \\cdot 10^7 \\lg 10^7 / 10^7 \\approx 50 \\cdot 23.25 = 1163$ seconds (under 20 minutes).\n    Computer B runs over 17 times faster due to a better algorithm despite being 1000 times slower in raw speed.\n\nAlgorithms should be considered a technology, as crucial as fast hardware. Advances in algorithms are ongoing.\nAlgorithms are fundamental even in applications not explicitly algorithmic at the application level. Fast hardware, GUIs, networking, compilers, interpreters, and assemblers all rely on algorithms.\nMachine learning is a collection of algorithms for performing tasks by inferring patterns from data. It doesn't make learning about algorithms obsolete; efficient algorithms are often more successful for problems humans understand well.\nData science uses methods from statistics, computer science (including algorithms), and optimization to extract knowledge from data."
            },
            {
                "file": "PartI Foundations/C2 Getting Started.md",
                "contents": "# 2 Getting Started\n\nThis chapter familiarizes with the framework for designing and analyzing algorithms. It covers insertion sort, merge sort (using divide-and-conquer), and their analyses.\n\n## 2.1 Insertion sort\n\nInsertion sort solves the sorting problem:\n*Input*: A sequence of $n$ numbers $(a_1, a_2, \\dots, a_n)$.\n*Output*: A permutation (reordering) $(a'_1, a'_2, \\dots, a'_n)$ of the input sequence such that $a'_1 \\le a'_2 \\le \\dots \\le a'_n$.\nNumbers to be sorted are *keys*. Input is typically an array. Keys are often associated with *satellite data*, forming a *record*.\n\nInsertion sort works like sorting a hand of playing cards: take cards one by one from a pile and insert them into the correct position in the hand. The hand is always kept sorted. See [[PartI Foundations Algorithms.md#C2.1 INSERTION-SORT]] for pseudocode.\n\n**Loop invariants and the correctness of insertion sort**\nA *loop invariant* is used to prove correctness. For the `for` loop of lines 1-8 in INSERTION-SORT:\n*Loop Invariant*: At the start of each iteration of the `for` loop, the subarray $A[1 \\dots i-1]$ consists of the elements originally in $A[1 \\dots i-1]$, but in sorted order.\nTo show this loop invariant helps prove correctness, we show three properties:\n-   *Initialization*: True prior to the first loop iteration (when $i=2$). The subarray $A[1]$ contains the original element $A[1]$ and is trivially sorted.\n-   *Maintenance*: If true before an iteration for value $i$, it remains true before the next iteration (for $i+1$). The body of the `for` loop (lines 2-8) works by moving $A[i-1], A[i-2], \text{etc.}$, one position to the right until the proper position for $key$ (original $A[i]$) is found, and $key$ is inserted. Then $A[1 \riots i]$ consists of original elements from $A[1 \riots i]$ in sorted order. Incrementing $i$ for the next iteration reestablishes the invariant for $A[1 \riots (i+1)-1]$.\n-   *Termination*: The loop terminates when $i > n$ (i.e., $i=n+1$). Substituting $n+1$ for $i$ in the invariant, the subarray $A[1 \riots n]$ consists of the elements originally in $A[1 \riots n]$, but in sorted order. This means the entire array is sorted.\n\n**Pseudocode conventions**\n-   Indentation indicates block structure.\n-   Looping constructs (`while`, `for`, `repeat-until`) and conditional `if-else` are similar to C, Java, Python. Loop counters retain their value after exit.\n-   `//` denotes comments.\n-   Variables are local unless specified.\n-   Array elements are accessed like $A[i]$. Indexing is 1-origin unless specified. $A[i \riots j]$ denotes a subarray.\n-   Compound data are in *objects* with *attributes* ($x.f$). Variables for arrays/objects are pointers/references.\n-   `NIL` is a special pointer value.\n-   Parameters are passed *by value*. Objects/arrays are passed by pointer-to-data (value of pointer is copied).\n-   `return` statement transfers control and optionally values. Multiple values can be returned.\n-   Boolean operators `and`, `or` are short-circuiting.\n-   `error` keyword indicates an error and termination.\n\n## 2.2 Analyzing algorithms\n\nAnalyzing an algorithm means predicting its required resources. Usually, computational time.\nAssume a generic one-processor, *random-access machine (RAM)* model: instructions execute sequentially, no concurrency. Each instruction and data access takes constant time. Arithmetic (add, subtract, multiply, divide, remainder, floor, ceiling), data movement (load, store, copy), and control (branch, call, return) are supported. Data types are integer, floating-point, character. Word size is limited (e.g., $c \\lg n$ bits for inputs of size $n$). The model doesn't account for memory hierarchy (caches, virtual memory), but RAM model analyses are usually good predictors.\n\n**Analysis of insertion sort**\nRunning time depends on input size $n$. Also depends on how sorted the input already is.\n-   *Input size*: Number of items in the input ($n$ for sorting).\n-   *Running time*: Number of primitive operations executed.\nEach line $k$ of pseudocode takes constant time $c_k$.\nFor INSERTION-SORT:\n-   Line 1 (`for i = 2 to n`): $c_1 \textit{n}$ times.\n-   Line 2 (`key = A[i]`): $c_2 (n-1)$ times.\n-   Line 4 (`j = i-1`): $c_4 (n-1)$ times.\n-   Line 5 (`while j > 0 and A[j] > key`): $c_5 \\sum_{i=2}^{n} t_i$ times, where $t_i$ is number of times test for $A[i]$ is run.\n-   Line 6 (`A[j+1] = A[j]`): $c_6 \\sum_{i=2}^{n} (t_i-1)$ times.\n-   Line 7 (`j = j-1`): $c_7 \\sum_{i=2}^{n} (t_i-1)$ times.\n-   Line 8 (`A[j+1] = key`): $c_8 (n-1)$ times.\nTotal running time $T(n)$ is sum of (cost $\\times$ times) for all statements.\n\n*Best case*: Array is already sorted. $t_i=1$ for all $i$. $T(n) = (c_1+c_2+c_4+c_5+c_8)n - (c_2+c_4+c_5+c_8)$. This is a linear function $an+b$.\n*Worst case*: Array is reverse sorted. $t_i=i$. $\\sum_{i=2}^{n} i = n(n+1)/2 - 1$. $\\sum_{i=2}^{n} (i-1) = n(n-1)/2$.\n$T(n) = (c_5/2+c_6/2+c_7/2)n^2 + (c_1+c_2+c_4+c_5/2-c_6/2-c_7/2+c_8)n - (c_2+c_4+c_5+c_8)$. This is a quadratic function $an^2+bn+c$.\n\n*Worst-case and average-case analysis*: We usually focus on worst-case because:\n1.  It's an upper bound on running time for any input.\n2.  For some algorithms, worst case occurs often.\n3.  Average case is often roughly as bad as worst case (e.g., for insertion sort, average $t_i \\approx i/2$, still quadratic).\n\n*Order of growth*: We are interested in the rate of growth for large $n$. Consider only the leading term, ignore its constant coefficient. For insertion sort, worst-case is $\\Theta(n^2)$, best-case is $\\Theta(n)$. $\\Theta$-notation used informally here for \"roughly proportional when $n$ is large.\"\nAn algorithm with lower order of growth is generally more efficient for large inputs.\n\n## 2.3 Designing algorithms\n\n**2.3.1 The divide-and-conquer method**\nRecursive algorithms often follow divide-and-conquer:\n1.  *Divide*: Break problem into smaller, similar subproblems.\n2.  *Conquer*: Solve subproblems recursively. If small enough (base case), solve directly.\n3.  *Combine*: Combine subproblem solutions to solve original problem.\n\n*Merge sort* algorithm follows this method for sorting $A[p \\dots r]$:\n-   *Divide*: Find midpoint $q = \\lfloor(p+r)/2\\rfloor$. Subproblems are $A[p \\dots q]$ and $A[q+1 \\dots r]$.\n-   *Conquer*: Recursively sort the two subarrays using merge sort.\n-   *Combine*: Merge the two sorted subarrays $A[p \\dots q]$ and $A[q+1 \\dots r]$ into $A[p \\dots r]$.\nBase case: $p \\ge r$ (subarray has $\\le 1$ element, already sorted).\nKey operation is merging, done by auxiliary procedure `MERGE(A, p, q, r)`. See [[PartI Foundations Algorithms.md#C2.3 MERGE]].\n`MERGE` assumes $A[p \\dots q]$ and $A[q+1 \\dots r]$ are sorted. It copies these to temporary arrays $L$ and $R$. Then it repeatedly compares top elements of $L, R$, copies smaller to $A$, until one array is empty. Copies remainder of other array to $A$. Merging $n$ elements takes $\\Theta(n)$ time.\nPseudocode for MERGE-SORT: See [[PartI Foundations Algorithms.md#C2.3 MERGE-SORT]].\n\n**2.3.2 Analyzing divide-and-conquer algorithms**\nRunning time often described by a *recurrence equation*.\nFor $T(n)$ = worst-case time for problem of size $n$:\nIf $n$ is small (e.g., $n < n_0$), $T(n) = \\Theta(1)$.\nOtherwise (divide into $a$ subproblems of size $n/b$):\n$T(n) = D(n) + aT(n/b) + C(n)$, where $D(n)$ is divide time, $C(n)$ is combine time.\n\n*Analysis of merge sort*:\n-   *Divide*: Compute $q$. $D(n) = \\Theta(1)$.\n-   *Conquer*: Solve 2 subproblems of size $n/2$. $2T(n/2)$. (Ignoring floors/ceilings).\n-   *Combine*: Merge $n$ elements using `MERGE`. $C(n) = \\Theta(n)$.\nRecurrence: $T(n) = 2T(n/2) + \\Theta(n)$. (Base case $T(n) = \\Theta(1)$ for small $n$ is implicit).\nSolution (using master theorem from Ch 4, or recursion tree here): $T(n) = \\Theta(n \\lg n)$.\n\n*Recursion tree for $T(n) = 2T(n/2) + cn$ (assuming $n$ is power of 2)*:\n-   Root: cost $cn$.\n-   Level 1: 2 nodes, each $T(n/2)$, each contributes $cn/2$. Total $2(cn/2) = cn$.\n-   Level $i$: $2^i$ nodes, each problem size $n/2^i$, each contributes $c(n/2^i)$. Total $2^i (cn/2^i) = cn$.\n-   Tree has $\\lg n + 1$ levels. Top $\\lg n$ levels each cost $cn$. Bottom level (leaves) has $n$ nodes, each $T(1)=\\Theta(1)$, total cost $c_1 n$ (if $T(1)=c_1$).\nTotal cost: $cn \\lg n + c_1 n = \\Theta(n \\lg n)$."
            },
            {
                "file": "PartI Foundations/C3 Characterizing Running Times.md",
                "contents": "# 3 Characterizing Running Times\n\nThis chapter provides methods for simplifying asymptotic analysis of algorithms, focusing on the order of growth for large input sizes.\n\n## 3.1 O-notation, $\\Omega$-notation, and $\\Theta$-notation\n\nThese notations characterize function growth by discarding lower-order terms and constant coefficients of the leading term.\n-   **$O$-notation (Big-O)**: Describes an asymptotic upper bound. A function $f(n)$ is $O(g(n))$ if $f(n)$ grows no faster than $g(n)$ for large $n$. Example: $7n^3 + 100n^2 - 20n + 6$ is $O(n^3)$, and also $O(n^4)$, $O(n^5)$, etc.\n-   **$\\Omega$-notation (Big-Omega)**: Describes an asymptotic lower bound. A function $f(n)$ is $\\Omega(g(n))$ if $f(n)$ grows at least as fast as $g(n)$ for large $n$. Example: $7n^3 + 100n^2 - 20n + 6$ is $\\Omega(n^3)$, and also $\\Omega(n^2)$, $\\Omega(n)$.\n-   **$\\Theta$-notation (Theta)**: Describes an asymptotically tight bound. A function $f(n)$ is $\\Theta(g(n))$ if $f(n)$ grows at the same rate as $g(n)$ for large $n$. Example: $7n^3 + 100n^2 - 20n + 6$ is $\\Theta(n^3)$.\n\n**Example: Insertion sort revisited**\nWorst-case running time of INSERTION-SORT:\n-   The nested loops imply an $O(n^2)$ upper bound: outer loop $n-1$ times, inner loop at most $i-1 < n$ times, body constant time. Total iterations $< n^2$.\n-   A specific input (e.g., first $n/3$ positions hold the $n/3$ largest values, as in Figure 3.1) forces at least $(n/3)(n/3) = n^2/9$ operations, demonstrating an $\\Omega(n^2)$ lower bound for the worst case.\n-   Since worst-case is $O(n^2)$ and $\\Omega(n^2)$, it is $\\Theta(n^2)$.\nBest-case running time is $\\Theta(n)$.\nWe cannot say insertion sort's running time is $\\Theta(n^2)$ in general (all cases), only its worst-case running time is $\\Theta(n^2)$. Its running time is $O(n^2)$ (upper bound for all cases) and $\\Omega(n)$ (lower bound for all cases).\n\n## 3.2 Asymptotic notation: formal definitions\n\nNotations are defined for functions whose domains are typically $\\mathbb{N}$ (natural numbers) or $\\mathbb{R}$ (real numbers).\n\n-   **$O(g(n))$ (Big-O notation)**: $O(g(n)) = \\{f(n) : \\text{there exist positive constants } c \\text{ and } n_0 \\text{ such that } 0 \\le f(n) \\le cg(n) \\text{ for all } n \\ge n_0\\}$.\n    $f(n)$ must be asymptotically nonnegative. $g(n)$ must also be asymptotically nonnegative. We write $f(n) = O(g(n))$.\n    Example: $4n^2+100n+500 = O(n^2)$ (choose $c=5.05, n_0=100$, or $c=604, n_0=1$).\n    Example: $n^3 - 100n^2 \\neq O(n^2)$ because $n-100 \\le c$ cannot hold for all $n \\ge n_0$ for a fixed $c$.\n\n-   **$\\Omega(g(n))$ (Big-Omega notation)**: $\\Omega(g(n)) = \\{f(n) : \\text{there exist positive constants } c \\text{ and } n_0 \\text{ such that } 0 \\le cg(n) \\le f(n) \\text{ for all } n \\ge n_0\\}$.\n    Example: $4n^2+100n+500 = \\Omega(n^2)$ (choose $c=4, n_0=1$).\n\n-   **$\\Theta(g(n))$ (Theta notation)**: $\\Theta(g(n)) = \\{f(n) : \\text{there exist positive constants } c_1, c_2, \\text{ and } n_0 \\text{ such that } 0 \\le c_1g(n) \\le f(n) \\le c_2g(n) \\text{ for all } n \\ge n_0\\}$.\n\n-   **Theorem 3.1**: For any two functions $f(n)$ and $g(n)$, $f(n) = \\Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \\Omega(g(n))$.\n\n**Asymptotic notation in equations and inequalities**\n-   $f(n) = O(g(n))$ on RHS means $f(n) \\in O(g(n))$.\n-   In a formula like $2n^2 + 3n + 1 = 2n^2 + \\Theta(n)$, $\\Theta(n)$ stands for some anonymous function $f(n) \\in \\Theta(n)$. Here, $f(n) = 3n+1$.\n-   On LHS, e.g., $2n^2 + \\Theta(n) = \\Theta(n^2)$: For any function $f(n) \\in \\Theta(n)$, there is some function $h(n) \\in \\Theta(n^2)$ such that $2n^2+f(n)=h(n)$.\n\n**Proper abuses of asymptotic notation**\n-   Context implies the variable tending to $\\infty$ (e.g., $f(n)=O(1)$ implies $n \\to \\infty$).\n-   $T(n)=O(1)$ for $n < k$: Conventionally means $\\exists c > 0$ s.t. $T(n) \\le c$ for $n < k$.\n\n-   **$o(g(n))$ (little-o notation)**: Upper bound not asymptotically tight.\n    $o(g(n)) = \\{f(n) : \\text{for any constant } c > 0, \\text{ there exists a constant } n_0 > 0 \\text{ such that } 0 \\le f(n) < cg(n) \\text{ for all } n \\ge n_0\\}$.\n    Equivalently, $\\lim_{n\\to\\infty} f(n)/g(n) = 0$.\n    Example: $2n = o(n^2)$, but $2n^2 \\neq o(n^2)$.\n\n-   **$\\omega(g(n))$ (little-omega notation)**: Lower bound not asymptotically tight.\n    $\\omega(g(n)) = \\{f(n) : \\text{for any constant } c > 0, \\text{ there exists a constant } n_0 > 0 \\text{ such that } 0 \\le cg(n) < f(n) \\text{ for all } n \\ge n_0\\}$.\n    Equivalently, $\\lim_{n\\to\\infty} f(n)/g(n) = \\infty$. Also, $f(n) \\in \\omega(g(n)) \\iff g(n) \\in o(f(n))$.\n    Example: $n^2/2 = \\omega(n)$, but $n^2/2 \\neq \\omega(n^2)$.\n\n**Comparing functions (asymptotically positive $f,g$)**\n-   *Transitivity*: Holds for $\\Theta, O, \\Omega, o, \\omega$.\n-   *Reflexivity*: Holds for $\\Theta, O, \\Omega$.\n-   *Symmetry*: $f(n)=\\Theta(g(n)) \\iff g(n)=\\Theta(f(n))$.\n-   *Transpose symmetry*: $f(n)=O(g(n)) \\iff g(n)=\\Omega(f(n))$; $f(n)=o(g(n)) \\iff g(n)=\\omega(f(n))$.\n-   *Trichotomy* (for real numbers $a<b, a=b, \text{or } a>b$) does not hold for functions. E.g., $n$ and $n^{1+\\sin n}$ are not asymptotically comparable.\n\n## 3.3 Standard notations and common functions\n\n-   **Monotonicity**: $f(n)$ is monotonically increasing if $m \\le n \\implies f(m) \\le f(n)$. Strictly increasing if $m < n \\implies f(m) < f(n)$. Similar for decreasing.\n-   **Floors and ceilings**: $\\lfloor x \\rfloor$ = greatest integer $\\le x$. $\\lceil x \\rceil$ = least integer $\\ge x$.\n    Properties: $x-1 < \\lfloor x \\rfloor \\le x \\le \\lceil x \\rceil < x+1$. For integer $n$, $\\lfloor n \\rfloor = \\lceil n \\rceil = n$.\n-   **Modular arithmetic**: $a \\pmod n = a - n\\lfloor a/n \\rfloor$. $0 \\le a \\pmod n < n$. $a \\equiv b \\pmod n$ if $(a \\pmod n) = (b \\pmod n)$.\n-   **Polynomials**: $p(n) = \\sum_{i=0}^{d} a_i n^i$ where $a_d \\neq 0$ is a polynomial of degree $d$. If $a_d > 0$, $p(n) = \\Theta(n^d)$. A function $f(n)$ is polynomially bounded if $f(n)=O(n^k)$ for some constant $k$.\n-   **Exponentials**: $a^0=1, a^1=a, a^{-1}=1/a, (a^m)^n = a^{mn}, a^m a^n = a^{m+n}$. For $a>1$, $a^n$ is monotonically increasing. $n^b = o(a^n)$ for any real constants $a>1, b$. (Exponentials grow faster than polynomials).\n    $e^x = 1+x+x^2/2! + x^3/3! + \\dots = \\sum_{i=0}^{\\infty} x^i/i!$. $1+x \\le e^x$. For $|x| \\le 1, e^x \\approx 1+x$.\n-   **Logarithms**: $\\lg n = \\log_2 n$ (binary), $\\ln n = \\log_e n$ (natural), $\\lg^k n = (\\lg n)^k$, $\\lg\\lg n = \\lg(\\lg n)$. Convention: $\\lg n + 1 = (\\lg n) + 1$.\n    $a = b^{\\log_b a}$. $\\log_c (ab) = \\log_c a + \\log_c b$. $\\log_b a^n = n \\log_b a$. $\\log_b a = \\log_c a / \\log_c b$.\n    $\\lg^b n = o(n^a)$ for any $a>0$. (Polynomials grow faster than polylogarithms).\n-   **Factorials**: $n! = 1 \\cdot 2 \\cdots n$. $n!=0$ if $n=0$. Stirling's approx: $n! = \\sqrt{2\\pi n} (n/e)^n (1+\\Theta(1/n))$.\n    $n! = o(n^n)$, $n! = \\omega(2^n)$, $\\lg(n!) = \\Theta(n \\lg n)$.\n-   **Functional iteration**: $f^{(i)}(n)$ is $f$ applied $i$ times. $f^{(0)}(n)=n$.\n-   **Iterated logarithm**: $\\lg^* n = \\min \\{i \\ge 0 : \\lg^{(i)} n \\le 1\\}$. Very slowly growing. $\\lg^* (2^{65536}) = 5$.\n-   **Fibonacci numbers**: $F_0=0, F_1=1, F_i = F_{i-1}+F_{i-2}$ for $i \\ge 2$. Sequence: $0,1,1,2,3,5,\\dots$.\n    Related to golden ratio $\\phi = (1+\\sqrt{5})/2 \\approx 1.61803$ and $\\hat{\\phi} = (1-\\sqrt{5})/2 \\approx -0.61803$.\n    $F_i = (\\phi^i - \\hat{\\phi}^i)/\\sqrt{5}$. Since $|\\hat{\\phi}| < 1$, $F_i = \\lfloor \\phi^i/\\sqrt{5} + 1/2 \\rfloor$. Fibonacci numbers grow exponentially."
            }
        ]
    },
    {
        "items": [
            {
                "file": "foundations overview.md",
                "contents": "# 1 Chapter 1 The Role of Algorithms in Computing\n\nThis chapter introduces the concept of algorithms and their importance in computer science. It defines what constitutes an algorithm and discusses algorithms as a technology.\n\n- A *problem statement* defines a relationship between input and output, possibly with additional constraints on input and output.\n- An *instance of a problem* is any input that satisfies the constraints on input as defined in the problem.\n- An *algorithm* is a sequence of computational steps that transform input into output. It is a tool for solving a well-defined computational problem.\n- A *data structure* is a way to store and organize data in order to facilitate access and modifications.\n\n# 2 Chapter 2 Getting Started\n\nThis chapter typically covers basic algorithm design and analysis, often using sorting as an example. Key concepts include:\n\n- **Algorithmic Techniques**: One common technique is the **incremental approach**, where the solution is built step-by-step. Insertion sort is a classic example.\n- **Analyzing Algorithms**: This involves predicting the resources an algorithm requires, primarily computational time (running time) and memory space.\n  - *Input size*: Depends on the problem. For sorting, it's often the number of items. For graph algorithms, it might be vertices and edges.\n  - *Running time*: The number of primitive operations or \"steps\" executed. A step is assumed to take constant time.\n- **Correctness of Algorithms**: Often proven using **loop invariants**.\n  Properties of a loop invariant:\n  - *Initialization*: It is true prior to the first iteration of the loop.\n  - *Maintenance*: If it is true before an iteration of the loop, it remains true before the next iteration.\n  - *Termination*: The loop terminates, and when it terminates, the invariant\u2014usually along with the reason that the loop terminated\u2014gives a useful property that helps show that the algorithm is correct.\n- **Order of Growth**: The primary interest is usually in the rate of growth of the running time as the input size increases. This focuses on the leading term of the running time formula, ignoring constant factors and lower-order terms.\n- **Worst-case and Average-case Analysis**:\n  - *Worst-case running time*: The longest running time for any input of a given size. This provides an upper bound and is often the focus of analysis.\n  - *Average-case running time*: The expected running time over all inputs of a given size, assuming a certain probability distribution for inputs. Requires assumptions about input distribution.\n  Reasons for focusing on worst-case: Guarantees performance, often coincides with average-case or is not much worse, average-case can be hard to determine.\n\n# 3 Chapter 3 Characterizing Running Times\n\nThis chapter introduces asymptotic notation for characterizing the running times of algorithms.\n\n- **Asymptotic Notations** are used to describe the limiting behavior of a function when the argument tends towards a particular value or infinity. They simplify algorithm analysis by focusing on the order of growth.\n  - $\\Theta(g(n))$ (Theta-notation): Asymptotically tight bound.\n    $\\Theta(g(n)) = \\{f(n) : \\text{there exist positive constants } c_1, c_2, \\text{ and } n_0 \\text{ such that } 0 \\le c_1 g(n) \\le f(n) \\le c_2 g(n) \\text{ for all } n \\ge n_0\\}$.\n  - $O(g(n))$ (Big-O notation): Asymptotic upper bound.\n    $O(g(n)) = \\{f(n) : \\text{there exist positive constants } c \\text{ and } n_0 \\text{ such that } 0 \\le f(n) \\le c g(n) \\text{ for all } n \\ge n_0\\}$.\n  - $\\Omega(g(n))$ (Big-Omega notation): Asymptotic lower bound.\n    $\\Omega(g(n)) = \\{f(n) : \\text{there exist positive constants } c \\text{ and } n_0 \\text{ such that } 0 \\le c g(n) \\le f(n) \\text{ for all } n \\ge n_0\\}$.\n  - $o(g(n))$ (Little-o notation): Upper bound that is not asymptotically tight.\n    $o(g(n)) = \\{f(n) : \\text{for every positive constant } c, \\text{ there exists a constant } n_0 > 0 \\text{ such that } 0 \\le f(n) < c g(n) \\text{ for all } n \\ge n_0\\}$.\n  - $\\omega(g(n))$ (Little-omega notation): Lower bound that is not asymptotically tight.\n    $\\omega(g(n)) = \\{f(n) : \\text{for every positive constant } c, \\text{ there exists a constant } n_0 > 0 \\text{ such that } 0 \\le c g(n) < f(n) \\text{ for all } n \\ge n_0\\}$.\n\n# 4 Chapter 4 Divide-and-Conquer\n\nThis chapter introduces the divide-and-conquer paradigm and methods for analyzing recurrences that arise from such algorithms.\n\n- **Divide-and-Conquer Method**: This strategy involves three steps:\n  1.  **Divide** the problem into one or more subproblems that are smaller instances of the same problem.\n  2.  **Conquer** the subproblems by solving them recursively. If subproblem sizes are small enough (base case), solve them directly.\n  3.  **Combine** the solutions to the subproblems into the solution for the original problem.\n- **Recurrences**: An equation or inequality that describes a function in terms of its value on smaller inputs. Divide-and-conquer algorithms naturally lead to recurrences for their running times.\n  - Example: Merge sort leads to $T(n) = 2T(n/2) + \\Theta(n)$, which solves to $T(n) = \\Theta(n \\lg n)$.\n  - *Algorithmic recurrences* are those that describe running times of algorithms. They are assumed to have $T(n) = \\Theta(1)$ for small $n$.\n- **Methods for Solving Recurrences**:\n  - **Substitution Method**: Guess a solution form and use mathematical induction to prove it.\n  - **Recursion-Tree Method**: Visualize the recurrence as a tree, sum costs per level, then sum total costs. Useful for guessing solutions.\n  - **Master Method**: Provides a \"cookbook\" solution for recurrences of the form $T(n) = aT(n/b) + f(n)$. It has three cases based on comparing $f(n)$ with $n^{\\log_b a}$.\n  - **Akra-Bazzi Method**: Solves a more general class of divide-and-conquer recurrences, including those with unequal subproblem sizes, often requiring calculus.\n\n# 5 Chapter 5 Probabilistic Analysis and Randomized Algorithms\n\nThis chapter explores techniques for analyzing algorithms whose behavior depends on probability, either due to random inputs or randomization within the algorithm itself.\n\n- **Probabilistic Analysis**: Involves computing the average-case running time or cost by assuming a probability distribution on the inputs. The algorithm itself is deterministic.\n- **Randomized Algorithms**: The algorithm makes random choices during its execution. The running time or cost is an expected value over these random choices, independent of the input distribution.\n- **Indicator Random Variables**: A simple yet powerful tool. For an event $A$, the indicator random variable $I\\{A\\}$ is 1 if $A$ occurs and 0 otherwise. A key property is $E[I\\{A\\}] = \\text{Pr}\\{A\\}$.\n  - *Linearity of Expectation*: $E[\\sum X_i] = \\sum E[X_i]$, even if variables are dependent. This is crucial for many analyses.\n- **Key Problems and Analyses**:\n  - **The Hiring Problem**: Analyze the cost of hiring assistants when candidates arrive in random order.\n    - Expected number of hires: $\\Theta(\\ln n)$.\n    - Cost: $\\Theta(c_i n + c_h \\ln n)$.\n  - **Randomly Permuting Arrays**: Creating a uniform random permutation. The `RANDOMLY-PERMUTE` algorithm achieves this in $\\Theta(n)$ time.\n  - **The Birthday Paradox**: With $k$ people, the probability of two sharing a birthday is $\\ge 1/2$ when $k \\approx 23$ (for $n=365$ days). Expected number of pairs with same birthday is $k(k-1)/(2n)$.\n  - **Balls and Bins**: Tossing $n$ balls into $b$ bins.\n    - Expected balls in a given bin: $n/b$.\n    - Expected tosses until a given bin gets a ball: $b$.\n    - Expected tosses until all bins get at least one ball (Coupon Collector's): $\\Theta(b \\ln b)$.\n  - **Streaks**: Longest expected streak of heads in $n$ coin flips is $\\Theta(\\lg n)$.\n  - **Online Hiring Problem**: Hire exactly once. Optimal strategy involves interviewing and rejecting $k \\approx n/e$ candidates, then hiring the next best. Probability of hiring the best candidate is $\\approx 1/e$."
            },
            {
                "file": "PartI Foundations Algorithms.md",
                "contents": "# C4 Divide-and-Conquer Algorithms\n\n## C4 4.1 MATRIX-MULTIPLY\n```\nMATRIX-MULTIPLY(A, B, C, n)\n  // compute entries in each of n rows\n  for i = 1 to n\n    // compute n entries in row i\n    for j = 1 to n\n      for k = 1 to n\n        // add in another term of equation (4.1)\n        C[i,j] = C[i,j] + A[i,k] * B[k,j]\n```\n\n## C4 4.1 MATRIX-MULTIPLY-RECURSIVE\n```\nMATRIX-MULTIPLY-RECURSIVE(A, B, C, n)\n  if n == 1\n    // Base case.\n    C[1,1] = C[1,1] + A[1,1] * B[1,1]\n    return\n  // Divide.\n  partition A, B, and C into n/2 x n/2 submatrices\n    A11, A12, A21, A22; B11, B12, B21, B22;\n    and C11, C12, C21, C22; respectively\n  // Conquer.\n  MATRIX-MULTIPLY-RECURSIVE(A11, B11, C11, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A11, B12, C12, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A21, B11, C21, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A21, B12, C22, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A12, B21, C11, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A12, B22, C12, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A22, B21, C21, n/2)\n  MATRIX-MULTIPLY-RECURSIVE(A22, B22, C22, n/2)\n```\n\n# C5 Probabilistic Analysis and Randomized Algorithms Algorithms\n\n## C5 5.1 HIRE-ASSISTANT\n```\nHIRE-ASSISTANT(n)\n  1 best = 0  // candidate 0 is a least-qualified dummy candidate\n  2 for i = 1 to n\n  3   interview candidate i\n  4   if candidate i is better than candidate best\n  5     best = i\n  6     hire candidate i\n```\n\n## C5 5.3 RANDOMIZED-HIRE-ASSISTANT\n```\nRANDOMIZED-HIRE-ASSISTANT(n)\n  1 randomly permute the list of candidates\n  2 HIRE-ASSISTANT(n)\n```\n\n## C5 5.3 RANDOMLY-PERMUTE\n```\nRANDOMLY-PERMUTE(A, n)\n  1 for i = 1 to n\n  2   swap A[i] with A[RANDOM(i, n)]\n```\n\n## C5 Ex5.3-2 PERMUTE-WITHOUT-IDENTITY\n```\nPERMUTE-WITHOUT-IDENTITY(A, n)\n  1 for i = 1 to n - 1\n  2   swap A[i] with A[RANDOM(i + 1, n)]\n```\n\n## C5 Ex5.3-3 PERMUTE-WITH-ALL\n```\nPERMUTE-WITH-ALL(A, n)\n  1 for i = 1 to n\n  2   swap A[i] with A[RANDOM(1, n)]\n```\n\n## C5 Ex5.3-4 PERMUTE-BY-CYCLE\n```\nPERMUTE-BY-CYCLE(A, n)\n  1 let B[1:n] be a new array\n  2 offset = RANDOM(1, n)\n  3 for i = 1 to n\n  4   dest = i + offset\n  5   if dest > n\n  6     dest = dest - n\n  7   B[dest] = A[i]\n  8 return B\n```\n\n## C5 Ex5.3-5 RANDOM-SAMPLE\n```\nRANDOM-SAMPLE(m, n)\n  1 S = {}\n  2 // iterates m times\n  3 for k = n - m + 1 to n\n  4   i = RANDOM(1, k)\n  5   if i belongs to S\n  6     S = S U {k}\n  7   else S = S U {i}\n  8 return S\n```\n\n## C5 5.4 ONLINE-MAXIMUM\n```\nONLINE-MAXIMUM(k, n)\n  1 best-score = -infinity\n  2 for i = 1 to k\n  3   if score(i) > best-score\n  4     best-score = score(i)\n  5 for i = k + 1 to n\n  6   if score(i) > best-score\n  7     return i\n  8 return n\n```"
            },
            {
                "file": "PartI Foundations/C4 Divide-and-Conquer.md",
                "contents": "# 4 Divide-and-Conquer\n\nThe divide-and-conquer method is a powerful strategy for designing asymptotically efficient algorithms. It involves breaking a problem into smaller subproblems, solving them recursively, and then combining their solutions.\n\n## 4.1 Introduction to Divide-and-Conquer\n\nFor divide-and-conquer, you solve a given problem (instance) recursively. If the problem is small enough\u2014the **base case**\u2014you just solve it directly without recursing. Otherwise\u2014the **recursive case**\u2014you perform three characteristic steps:\n1.  **Divide** the problem into one or more subproblems that are smaller instances of the same problem.\n2.  **Conquer** the subproblems by solving them recursively.\n3.  **Combine** the subproblem solutions to form a solution to the original problem.\n\nThe recursion bottoms out when it reaches a base case, and the subproblem is small enough to solve directly.\n\n### Recurrences\nA **recurrence** is an equation or inequality that describes a function in terms of its value on other, typically smaller, arguments. Recurrences naturally characterize the running times of divide-and-conquer algorithms.\n\nThe general form of a recurrence is an equation or inequality describing a function over integers or reals using the function itself. It typically contains:\n-   A **base case**: does not involve recursive invocation.\n-   A **recursive case**: involves recursive invocation on smaller inputs.\nA recurrence is **well defined** if at least one function satisfies it, and **ill defined** otherwise.\n\n**Algorithmic recurrences** describe running times of divide-and-conquer algorithms. A recurrence $T(n)$ is algorithmic if, for every sufficiently large threshold constant $n_0 > 0$:\n1.  For all $n < n_0$, $T(n) = \\Theta(1)$.\n2.  For all $n \\ge n_0$, every path of recursion terminates in a defined base case within a finite number of recursive invocations.\n\n**Conventions for recurrences**: When a recurrence is stated without an explicit base case, it is assumed to be algorithmic, meaning $T(n) = \\Theta(1)$ for sufficiently small $n$. Asymptotic solutions often remain unchanged by dropping floors or ceilings in integer arguments, simplifying analysis.\n\nRecurrences can be inequalities, like $T(n) \\le 2T(n/2) + \\Theta(n)$, leading to solutions using $O$-notation. If reversed ($T(n) \\ge \text{...}$), it leads to $\\Omega$-notation.\n\nExamples of divide-and-conquer algorithms leading to recurrences:\n-   Simple matrix multiplication (recursive): $T(n) = 8T(n/2) + \\Theta(1)$, solution $T(n) = \\Theta(n^3)$.\n-   Strassen's algorithm: $T(n) = 7T(n/2) + \\Theta(n^2)$, solution $T(n) = O(n^{\\lg 7})$.\n-   Unequal subproblem sizes: e.g., $T(n) = T(n/3) + T(2n/3) + \\Theta(n)$, solution $T(n) = \\Theta(n \\lg n)$.\n-   Recursive linear search: $T(n) = T(n-1) + \\Theta(1)$, solution $T(n) = \\Theta(n)$.\nMost efficient divide-and-conquer algorithms solve subproblems that are a constant fraction of the original problem size.\n\n## 4.2 Multiplying square matrices\n\nLet $A = (a_{ik})$ and $B = (b_{jk})$ be square $n \\times n$ matrices. Their product $C = A \\cdot B$ is an $n \\times n$ matrix where $c_{ij} = \\sum_{k=1}^{n} a_{ik} \\cdot b_{kj}$.\n\n### 4.2.1 A simple divide-and-conquer algorithm\nThe straightforward `MATRIX-MULTIPLY` procedure, using three nested loops, takes $\\Theta(n^3)$ time. [[PartI Foundations Algorithms.md#C4 4.1 MATRIX-MULTIPLY]]\n\nA divide-and-conquer algorithm for $n \\times n$ matrix multiplication partitions $A, B, C$ into four $n/2 \\times n/2$ submatrices each. Assuming $n$ is a power of 2:\n$A = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{pmatrix}$, $B = \\begin{pmatrix} B_{11} & B_{12} \\\\ B_{21} & B_{22} \\end{pmatrix}$, $C = \\begin{pmatrix} C_{11} & C_{12} \\\\ C_{21} & C_{22} \\end{pmatrix}$.\nThis leads to eight $n/2 \\times n/2$ multiplications and four additions:\n$C_{11} = A_{11}B_{11} + A_{12}B_{21}$\n$C_{12} = A_{11}B_{12} + A_{12}B_{22}$\n$C_{21} = A_{21}B_{11} + A_{22}B_{21}$\n$C_{22} = A_{21}B_{12} + A_{22}B_{22}$\n\nPartitioning can be done by copying elements (takes $\\Theta(n^2)$ time) or by index calculation (takes $\\Theta(1)$ time). Assuming index calculation for partitioning, the `MATRIX-MULTIPLY-RECURSIVE` procedure performs eight recursive calls on $n/2 \\times n/2$ matrices. [[PartI Foundations Algorithms.md#C4 4.1 MATRIX-MULTIPLY-RECURSIVE]]\n\nThe recurrence for its running time is $T(n) = 8T(n/2) + \\Theta(1)$ (if additions of $n/2 \\times n/2$ matrices take $\\Theta(n^2)$ then it would be $8T(n/2) + \\Theta(n^2)$, but the book says if partitioning is $\\Theta(1)$ and updates happen in place, the recurrence is $T(n) = 8T(n/2) + \\Theta(1)$ which leads to $T(n)=\\Theta(n^3)$). The $\\Theta(1)$ term arises if we assume adding submatrices takes no time beyond the recursive calls or that the procedure computes $C = C+AB$ and matrix additions are implicit in recursive calls. The actual time to add the resulting $n/2 \\times n/2$ matrices would be $\\Theta((n/2)^2) = \\Theta(n^2)$. If this combination step is explicit, $T(n) = 8T(n/2) + \\Theta(n^2)$, which also yields $T(n) = \\Theta(n^3)$ by the master theorem. The book states on page 84 that the recursive matrix multiplication results in $T(n) = 8T(n/2) + \\Theta(1)$, with solution $\\Theta(n^3)$. This implies the cost of matrix additions is absorbed into the recursive calls or considered negligible relative to the number of scalar operations at the base case, which is not standard. Re-reading page 84: \"There is no combine step, because the matrix C is updated in place. The total time for the recursive case, therefore, is the sum of the partitioning time and the time for all the recursive calls, or $\\Theta(1) + 8T(n/2)$.\" This explanation suggests that the additions of matrix products are handled within the structure of recursive calls accumulating into C submatrices, rather than as a separate combination step after all 8 products are formed. This means the $\\Theta(n^2)$ work of additions is effectively spread or accounted for differently. However, the solution $\\Theta(n^3)$ is correct under both $T(n) = 8T(n/2) + \\Theta(1)$ and $T(n) = 8T(n/2) + \\Theta(n^2)$. The recursion tree for $8T(n/2)$ has $8^{\\log_2 n} = n^{\\log_2 8} = n^3$ leaves, each doing $\\Theta(1)$ work, so leaf cost is $\\Theta(n^3)$.\n\n## 4.3 Strassen's algorithm for matrix multiplication\n\nStrassen's algorithm improves upon the simple divide-and-conquer matrix multiplication by reducing the number of $n/2 \\times n/2$ multiplications from 8 to 7, at the expense of more matrix additions and subtractions.\nIt runs in $O(n^{\\lg 7}) \\approx O(n^{2.81})$ time.\n\nThe algorithm computes $C = A \\cdot B$ (or $C=C+A \\cdot B$) by:\n1.  If $n=1$, perform scalar multiplication. Otherwise, partition $A, B, C$ into $n/2 \\times n/2$ submatrices ($\\\theta(1)$ by index calculation).\n2.  Create 10 matrices $S_1, S_2, \\dots, S_{10}$, each being a sum or difference of two submatrices from step 1. Create 7 matrices $P_1, \\dots, P_7$ to hold $n/2 \\times n/2$ products. This step takes $\\Theta(n^2)$ time (10 additions/subtractions of $n/2 \\times n/2$ matrices, plus allocation/zeroing).\n    $S_1 = B_{12} - B_{22}$\n    $S_2 = A_{11} + A_{12}$\n    $S_3 = A_{21} + A_{22}$\n    $S_4 = B_{21} - B_{11}$\n    $S_5 = A_{11} + A_{22}$\n    $S_6 = B_{11} + B_{22}$\n    $S_7 = A_{12} - A_{22}$\n    $S_8 = B_{21} + B_{22}$\n    $S_9 = A_{11} - A_{21}$\n    $S_{10} = B_{11} + B_{12}$\n3.  Recursively compute 7 matrix products $P_1, \\dots, P_7$ using submatrices from step 1 and $S_i$ matrices. This takes $7T(n/2)$ time.\n    $P_1 = A_{11} \\cdot S_1$\n    $P_2 = S_2 \\cdot B_{22}$\n    $P_3 = S_3 \\cdot B_{11}$\n    $P_4 = A_{22} \\cdot S_4$\n    $P_5 = S_5 \\cdot S_6$\n    $P_6 = S_7 \\cdot S_8$\n    $P_7 = S_9 \\cdot S_{10}$\n4.  Update the submatrices of $C$ using sums and differences of $P_i$ matrices. This takes $\\Theta(n^2)$ time (8 additions/subtractions of $n/2 \\times n/2$ matrices).\n    $C_{11} = P_5 + P_4 - P_2 + P_6$\n    $C_{12} = P_1 + P_2$\n    $C_{21} = P_3 + P_4$\n    $C_{22} = P_5 + P_1 - P_3 - P_7$\n\nThe recurrence for Strassen's algorithm is $T(n) = 7T(n/2) + \\Theta(n^2)$. Applying the master theorem, $a=7, b=2$, $f(n) = \\Theta(n^2)$. $n^{\\log_b a} = n^{\\log_2 7} \\approx n^{2.81}$. Since $f(n) = O(n^{\\log_2 7 - \\epsilon})$ for $\\epsilon \\approx 0.81$, case 1 applies, and $T(n) = \\Theta(n^{\\log_2 7})$.\n\n## 4.4 The substitution method for solving recurrences\n\nThe substitution method involves two steps:\n1.  Guess the form of the solution (e.g., $T(n) = O(n \\lg n)$).\n2.  Use mathematical induction to find constants and show that the solution works.\n\nExample: $T(n) = 2T(\\lfloor n/2 \\rfloor) + \\Theta(n)$. Guess $T(n) \\le cn \\lg n$ for some $c > 0$.\nAssume $T(\\lfloor n/2 \\rfloor) \\le c \\lfloor n/2 \\rfloor \\lg(\\lfloor n/2 \\rfloor)$.\n$T(n) \\le 2(c \\lfloor n/2 \\rfloor \\lg(\\lfloor n/2 \\rfloor)) + dn$ (for some $d>0$ from $\\Theta(n)$)\n$\\le 2(c (n/2) \\lg(n/2)) + dn = cn \\lg(n/2) + dn = cn \\lg n - cn \\lg 2 + dn = cn \\lg n - cn + dn$.\nThis is $\\le cn \\lg n$ if $-cn + dn \\le 0$, i.e., $d \\le c$. This can be satisfied by choosing $c$ large enough. Base cases must also be handled.\n\n**Making a good guess**: Experience, creativity, or using recursion trees. Loose bounds can be found and then refined.\n\n**A trick of the trade: subtracting a lower-order term**: If the induction fails because the inductive assumption is not strong enough, try subtracting a lower-order term. Example: $T(n) = 2T(n/2) + \\Theta(1)$. Guess $T(n) \\le cn$. Substitution yields $T(n) \\le 2(c(n/2)) + d = cn + d$, which is not $\\le cn$. Instead, guess $T(n) \\le cn - d'$ for $d' \\ge 0$. Then $T(n) \\le 2(c(n/2) - d') + d = cn - 2d' + d$. This is $\\le cn - d'$ if $-2d' + d \\le -d'$, or $d' \\ge d$. This can work.\n\n**Avoiding pitfalls**: Do not use asymptotic notation ($O, \\Omega$) directly in the inductive hypothesis like $T(n) \\le O(n)$. Use specific constants, e.g., $T(n) \\le cn$.\n\n## 4.5 The recursion-tree method for solving recurrences\n\nA recursion tree visualizes the costs incurred at each level of recursion.\n-   Each node represents the cost of a single subproblem.\n-   Sum costs within each level to get per-level costs.\n-   Sum all per-level costs to get the total cost.\nBest used to generate a guess, then verify with substitution method. Can be a direct proof if done carefully.\n\nExample: $T(n) = 3T(n/4) + cn^2$.\n-   Root: cost $cn^2$. Children: 3 nodes for $T(n/4)$.\n-   Level 1: 3 nodes, each cost $c(n/4)^2$. Total cost $3c(n/4)^2 = (3/16)cn^2$.\n-   Level $i$: $3^i$ nodes, each cost $c(n/4^i)^2$. Total cost $(3/16)^i cn^2$.\n-   Height of tree: $\\log_4 n$. Number of leaves: $3^{\\log_4 n} = n^{\\log_4 3}$. Leaf cost: $\\Theta(n^{\\log_4 3})$.\nTotal cost: $T(n) = cn^2 \\sum_{i=0}^{\\log_4 n - 1} (3/16)^i + \\Theta(n^{\\log_4 3})$.\nThe sum is a geometric series $\\sum_{i=0}^{k} x^i < 1/(1-x)$ for $x < 1$. So $\\sum (3/16)^i < 1/(1-3/16) = 16/13$.\n$T(n) = \\Theta(cn^2) + \\Theta(n^{\\log_4 3}) = \\Theta(n^2)$ since $n^2$ dominates $n^{\\log_4 3}$.\nVerify with substitution: $T(n) \\le dn^2$. $3d(n/4)^2 + cn^2 = (3/16)dn^2 + cn^2 = ((3/16)d + c)n^2$. This is $\\le dn^2$ if $(3/16)d + c \\le d$, i.e., $c \\le (13/16)d$, or $d \\ge (16/13)c$.\n\nIrregular example: $T(n) = T(n/3) + T(2n/3) + cn$.\n-   Root: $cn$. Children: $T(n/3)$ and $T(2n/3)$.\n-   Costs at each level sum to $cn$. Height is $\\log_{3/2} n$ (longest path).\n-   Total cost $\\approx cn \\times \\text{height} = cn \\log_{3/2} n = \\Theta(n \\lg n)$.\n-   Number of leaves $L(n) = L(n/3) + L(2n/3)$ with $L(1)=1$. Solution $L(n) = \\Theta(n)$. Leaf cost $\\Theta(n)$.\n-   Overall solution $\\Theta(n \\lg n)$.\n\n## 4.6 The master method for solving recurrences\n\nThe master method solves recurrences of the form $T(n) = aT(n/b) + f(n)$, where $a \\ge 1, b > 1$ are constants, and $f(n)$ is an asymptotically positive function.\nCompare $f(n)$ with $n^{\\log_b a}$ (the watershed function).\n\n**Theorem 4.1 (Master Theorem)**\n1.  If $f(n) = O(n^{\\log_b a - \\epsilon})$ for some constant $\\epsilon > 0$, then $T(n) = \\Theta(n^{\\log_b a})$.\n    (The watershed function grows polynomially faster than $f(n)$. Solution dominated by leaf costs.)\n2.  If $f(n) = \\Theta(n^{\\log_b a} \\lg^k n)$ for some constant $k \\ge 0$, then $T(n) = \\Theta(n^{\\log_b a} \\lg^{k+1} n)$.\n    (If $k=0$, $f(n) = \\Theta(n^{\\log_b a})$, then $T(n) = \\Theta(n^{\\log_b a} \\lg n)$.)\n    (Costs are roughly equal across levels or grow slowly.)\n3.  If $f(n) = \\Omega(n^{\\log_b a + \\epsilon})$ for some constant $\\epsilon > 0$, and if $af(n/b) \\le cf(n)$ for some constant $c < 1$ and all sufficiently large $n$ (regularity condition), then $T(n) = \\Theta(f(n))$.\n    ($f(n)$ grows polynomially faster than the watershed function. Solution dominated by root cost.)\n\n**Using the master method**:\n-   $T(n) = 9T(n/3) + n$. $a=9, b=3$. $n^{\\log_3 9} = n^2$. $f(n)=n = O(n^{2-\\epsilon})$ for $\\epsilon=1$. Case 1: $T(n) = \\Theta(n^2)$.\n-   $T(n) = T(2n/3) + 1$. $a=1, b=3/2$. $n^{\\log_{3/2} 1} = n^0 = 1$. $f(n)=1 = \\Theta(n^0 \\lg^0 n)$. Case 2 ($k=0$): $T(n) = \\Theta(\\lg n)$.\n-   $T(n) = 3T(n/4) + n \\lg n$. $a=3, b=4$. $n^{\\log_4 3} \\approx n^{0.793}$. $f(n)=n \\lg n = \\Omega(n^{\\log_4 3 + \\epsilon})$ where $\\epsilon \\approx 0.2$. Regularity condition: $3(n/4)\\lg(n/4) \\le (3/4) n \\lg n = cf(n)$ for $c=3/4 < 1$. Case 3: $T(n) = \\Theta(n \\lg n)$.\n\n**When the master method doesn't apply**: If $f(n)$ falls in the gaps between cases (e.g., $f(n)$ is slower than $n^{\\log_b a}$ but not polynomially slower), or if the regularity condition fails. E.g., $T(n) = 2T(n/2) + n/\\lg n$. Here $n^{\\log_b a} = n$. $f(n) = n/\\lg n = o(n)$, but not $O(n^{1-\\epsilon})$.\n\n## 4.7 Proof of the continuous master theorem\n\nThis section proves a version of the master theorem for continuous variables, avoiding floors/ceilings.\n\n**Lemma 4.2**: For $T(n) = aT(n/b) + f(n)$ (with $T(n)=\\Theta(1)$ for $0 \\le n < 1$), the solution is\n$T(n) = \\Theta(n^{\\log_b a}) + \\sum_{j=0}^{\\lfloor \\log_b n \\rfloor} a^j f(n/b^j)$.\nThe first term is leaf costs, sum is costs of internal nodes.\n\n**Lemma 4.3**: Provides asymptotic bounds for the summation $g(n) = \\sum_{j=0}^{\\lfloor \\log_b n \\rfloor} a^j f(n/b^j)$ corresponding to the three cases of the master theorem.\n1.  If $f(n) = O(n^{\\log_b a - \\epsilon})$, then $g(n) = O(n^{\\log_b a})$.\n2.  If $f(n) = \\Theta(n^{\\log_b a} \\lg^k n)$, then $g(n) = \\Theta(n^{\\log_b a} \\lg^{k+1} n)$.\n3.  If $af(n/b) \\le cf(n)$ for $c<1$, and $f(n)=\\Omega(n^{\\log_b a + \\epsilon})$, then $g(n) = \\Theta(f(n))$.\n\n**Theorem 4.4 (Continuous Master Theorem)**: States the three cases using the results of Lemma 4.2 and 4.3. The proof involves transforming $T(n)$ to $T'(n') = T(n_0 n')$ to handle general base cases $n_0$.\n\n## 4.8 Akra-Bazzi recurrences\n\nSolves recurrences of the form $T(n) = f(n) + \\sum_{i=1}^{k} a_i T(n/b_i)$ for constants $a_i > 0$, $b_i > 1$. Generalizes master recurrences.\n\n**Polynomial-growth condition**: $f(n)$ satisfies this if for any $\\phi \\ge 1$, there is a $d > 1$ such that $f(n)/d \\le f(\\psi n) \\le df(n)$ for $1 \\le \\psi \\le \\phi$ and $n \\ge \\tilde{n}$. Roughly $f(O(n)) = \\Theta(f(n))$. Most polynomial-like functions satisfy this. Exponentials do not.\n\n**Theorem 4.5**: If $f(n)$ satisfies polynomial-growth, floors and ceilings in $n/b_i$ do not change the asymptotic solution of an Akra-Bazzi recurrence.\n\n**The Akra-Bazzi method**:\n1.  Find the unique real number $p$ such that $\\sum_{i=1}^{k} a_i / b_i^p = 1$.\n2.  The solution is $T(n) = \\Theta(n^p (1 + \\int_1^n \\frac{f(x)}{x^{p+1}} dx))$.\n\nExample: $T(n) = T(n/5) + T(7n/10) + n$.\nHere $a_1=1, b_1=5, a_2=1, b_2=10/7, f(n)=n$.\nFind $p$ such that $(1/5)^p + (1/(10/7))^p = (1/5)^p + (7/10)^p = 1$. For $p=1$, sum is $1/5 + 7/10 = 9/10 < 1$. For $p=0$, sum is $1+1=2 > 1$. So $0 < p < 1$.\nIntegral: $\\int_1^n \\frac{x}{x^{p+1}} dx = \\int_1^n x^{-p} dx = [\\frac{x^{-p+1}}{-p+1}]_1^n = \\frac{n^{1-p}-1}{1-p} = \\Theta(n^{1-p})$ since $1-p > 0$.\n$T(n) = \\Theta(n^p (1 + n^{1-p})) = \\Theta(n^p + n) = \\Theta(n)$ since $p<1$ implies $n$ dominates $n^p$."
            },
            {
                "file": "PartI Foundations/C5 Probabilistic Analysis and Randomized Algorithms.md",
                "contents": "# 5 Probabilistic Analysis and Randomized Algorithms\n\nThis chapter introduces techniques for analyzing algorithms whose performance depends on probability, either due to assumptions about input distribution (probabilistic analysis) or explicit use of random numbers in the algorithm (randomized algorithms).\n\n## 5.1 The hiring problem\n\nProblem: Hire a new office assistant. An employment agency sends one candidate per day. Interviewing has a small cost $c_i$. Hiring has a larger cost $c_h$ (includes firing current assistant and agency fee). Goal: always have the best person seen so far. Strategy: if the current candidate is better than the current assistant, fire the assistant and hire the new candidate.\nProcedure `HIRE-ASSISTANT(n)` implements this strategy. [[PartI Foundations Algorithms.md#C5 5.1 HIRE-ASSISTANT]]\nTotal cost: $O(c_i n + c_h m)$, where $n$ is total candidates, $m$ is number hired. We focus on $c_h m$.\n\n**Worst-case analysis**: If candidates arrive in strictly increasing order of quality, all $n$ candidates are hired. Cost $O(c_h n)$.\n\n**Probabilistic analysis**: Assume candidates arrive in a random order. This means each of the $n!$ permutations of candidate ranks is equally likely (uniform random permutation).\n\n**Randomized algorithms**: If we cannot assume random input order, we can enforce it by randomly permuting the input before processing. An algorithm is **randomized** if its behavior is determined by its input and values from a random-number generator like `RANDOM(a,b)`.\n- *Average-case running time*: Expectation over input distribution (for deterministic algorithms).\n- *Expected running time*: Expectation over random choices made by the algorithm (for randomized algorithms, applies to any input).\n\n## 5.2 Indicator random variables\n\nAn **indicator random variable** $I\\{A\\}$ for an event $A$ is defined as:\n$I\\{A\\} = 1$ if $A$ occurs,\n$I\\{A\\} = 0$ if $A$ does not occur.\n\n**Lemma 5.1**: For an event $A$, $E[I\\{A\\}] = \\text{Pr}\\{A\\}$.\nProof: $E[I\\{A\\}] = 1 \\cdot \\text{Pr}\\{A\\} + 0 \\cdot \\text{Pr}\\{\\bar{A}\\} = \\text{Pr}\\{A\\}$.\nIndicator random variables simplify analysis, especially when combined with linearity of expectation ($E[\\sum X_i] = \\sum E[X_i]$).\n\n**Analysis of the hiring problem using indicator random variables**:\nAssume candidates arrive in random order. Let $X$ be the random variable for the number of hires.\nLet $X_i = I\\{\\text{candidate } i \\text{ is hired}\\}$. Then $X = \\sum_{i=1}^n X_i$.\n$E[X] = E[\\sum_{i=1}^n X_i] = \\sum_{i=1}^n E[X_i]$.\n$E[X_i] = \\text{Pr}\\{\\text{candidate } i \\text{ is hired}\\}$.\nCandidate $i$ is hired if candidate $i$ is better than candidates $1, \\dots, i-1$. Since the first $i$ candidates arrive in a random order, any one of them is equally likely to be the best so far. Thus, $\\text{Pr}\\{\\text{candidate } i \\text{ is best among first } i\\} = 1/i$.\nSo, $E[X_i] = 1/i$.\n$E[X] = \\sum_{i=1}^n (1/i) = H_n$, the $n$-th harmonic number.\n$H_n = \\ln n + O(1)$.\n\n**Lemma 5.2**: Assuming candidates are presented in a random order, algorithm `HIRE-ASSISTANT` has an average-case total hiring cost of $O(c_h \\ln n)$.\nThis is a significant improvement over the worst-case $O(c_h n)$.\n\n## 5.3 Randomized algorithms\n\nIf input distribution is unknown, probabilistic analysis cannot be used. Instead, use a randomized algorithm.\nFor the hiring problem, `RANDOMIZED-HIRE-ASSISTANT` first randomly permutes the candidate list, then calls `HIRE-ASSISTANT`. [[PartI Foundations Algorithms.md#C5 5.3 RANDOMIZED-HIRE-ASSISTANT]]\n\n**Lemma 5.3**: The expected hiring cost of `RANDOMIZED-HIRE-ASSISTANT` is $O(c_h \\ln n)$.\nThe proof follows because random permutation enforces the condition needed for the probabilistic analysis.\n\n**Randomly permuting arrays**:\nGoal: produce a **uniform random permutation** (each of $n!$ permutations equally likely).\nProcedure `RANDOMLY-PERMUTE(A, n)`: [[PartI Foundations Algorithms.md#C5 5.3 RANDOMLY-PERMUTE]]\nIn its $i$-th iteration, it swaps $A[i]$ with a random element $A[j]$ where $j$ is chosen uniformly from $i, \\dots, n$.\n\n**Lemma 5.4**: Procedure `RANDOMLY-PERMUTE` computes a uniform random permutation.\nProof uses a loop invariant:\n- *Loop Invariant*: Just prior to the $i$-th iteration, for each possible $(i-1)$-permutation of the $n$ elements, the subarray $A[1 \\dots i-1]$ contains this $(i-1)$-permutation with probability $(n-i+1)!/n!$.\n- *Initialization* ($i=1$): The subarray $A[1 \\dots 0]$ is empty. A $0$-permutation has no elements. Probability is $n!/n! = 1$. Holds.\n- *Maintenance*: Assume invariant holds for $i$. Consider $(i)$-permutation $(x_1, \\dots, x_i)$. This occurs if $A[1 \\dots i-1]$ was $(x_1, \\dots, x_{i-1})$ (event $E_1$) and $A[i]$ becomes $x_i$ (event $E_2$). $\\text{Pr}\\{E_1\\} = (n-i+1)!/n!$. $\\text{Pr}\\{E_2 | E_1\\} = 1/(n-i+1)$ because $x_i$ is one of the $n-i+1$ elements in $A[i \\dots n]$ (if $x_i$ was already in $A[1 \\dots i-1]$ it cannot be chosen for $A[i]$ unless it's $A[i]$ itself, so $x_i$ must be one of the elements not in $A[1 \\dots i-1]$ or the one currently at $A[i]$ from $A[i..n]$). The element $x_i$ is chosen from $A[i \\dots n]$ (which has $n-i+1$ elements) with probability $1/(n-i+1)$ if it is in that range. $\\text{Pr}\\{E_1 \\cap E_2\\} = \\text{Pr}\\{E_2 | E_1\\} \\text{Pr}\\{E_1\\} = \\frac{1}{n-i+1} \\cdot \\frac{(n-i+1)!}{n!} = \\frac{(n-i)!}{n!}$. This matches the invariant for $i+1$.\n- *Termination* ($i=n+1$): $A[1 \\dots n]$ contains a given $n$-permutation with probability $(n-(n+1)+1)!/n! = 0!/n! = 1/n!$.\n\n## 5.4 Probabilistic analysis and further uses of indicator random variables\n\n### 5.4.1 The birthday paradox\nHow many people $k$ must be in a room for a $\\ge 50\\%$ chance that at least two share a birthday (assuming $n=365$ days, birthdays independent and uniform)?\nLet $B_k$ be event that all $k$ people have distinct birthdays.\n$\\text{Pr}\\{B_k\\} = \\text{Pr}\\{B_{k-1}\\} \\text{Pr}\\{A_k | B_{k-1}\\} = 1 \\cdot \\frac{n-1}{n} \\cdot \\frac{n-2}{n} \\cdots \\frac{n-k+1}{n}$.\nUsing $1+x \\le e^x$, $\\text{Pr}\\{B_k\\} \\le e^{-1/n} e^{-2/n} \nopagebreak \\cdots e^{-(k-1)/n} = e^{-k(k-1)/(2n)}$.\nWe want $\\text{Pr}\\{B_k\\} \\le 1/2$. So $e^{-k(k-1)/(2n)} \\le 1/2 \rmsg -k(k-1)/(2n) \\le \\ln(1/2) = -\\ln 2$.\n$k(k-1) \\ge 2n \\ln 2$. For $n=365$, $k \\ge 23$.\n\n**Using indicator random variables (approximate analysis)**:\nLet $X_{ij} = I\\{\\text{person } i \\text{ and person } j \\text{ share a birthday}\\}$ for $1 \\le i < j \\le k$.\n$\\text{Pr}\\{\\text{person } i, j \\text{ share birthday}\\} = 1/n$. So $E[X_{ij}] = 1/n$.\nLet $X = \\sum_{i<j} X_{ij}$ be number of pairs with same birthday.\n$E[X] = \\sum_{i<j} E[X_{ij}] = \\binom{k}{2} \\frac{1}{n} = \\frac{k(k-1)}{2n}$.\nIf $E[X] \\ge 1$, then $k(k-1) \\ge 2n$. This gives $k \\approx \\sqrt{2n}$. For $n=365, k \\approx 28$.\nBoth analyses yield $k = \\Theta(\\sqrt{n})$.\n\n### 5.4.2 Balls and bins\nProcess: Toss identical balls randomly into $b$ bins. Tosses are independent.\n- *How many balls in a given bin?* If $n$ balls tossed, expected number in a bin is $n/b$.\n- *How many balls until a given bin contains a ball?* This follows geometric distribution with success prob $1/b$. Expected tosses $b$.\n- *How many balls until every bin contains at least one ball?* (Coupon collector's problem)\n  Let $n_i$ be tosses to get $i$-th hit (ball in new empty bin) after $(i-1)$-th hit.\n  Prob of hit when $i-1$ bins have balls: $(b-(i-1))/b$.\n  $E[n_i] = b/(b-i+1)$.\n  Total expected tosses $E[N] = \\sum_{i=1}^b E[n_i] = \\sum_{i=1}^b \\frac{b}{b-i+1} = b \\sum_{j=1}^b \\frac{1}{j} = b H_b = b (\\ln b + O(1))$.\n  Expected $b \\ln b$ tosses.\n\n### 5.4.3 Streaks\nLongest streak of consecutive heads in $n$ fair coin flips.\nExpected length is $\\Theta(\\lg n)$.\n**Upper bound $O(\\lg n)$**: $\\text{Pr}\\{\\text{streak of length at least } 2\\lceil \\lg n \\rceil \\text{ starts at } i\\} = 1/2^{2\\lceil \\lg n \\rceil} \\le 1/n^2$.\nProb. any such streak occurs $\\le n \\cdot (1/n^2) = 1/n$.\nUse this to bound $E[L] = \\sum j \\text{Pr}\\{L_j\\} \\le 2\\lceil \\lg n \\rceil \\cdot 1 + n \\cdot (1/n) = O(\\lg n)$.\n**Lower bound $\\Omega(\\lg n)$**: Partition $n$ flips into $\\approx n/s$ groups of $s = \\lfloor (\\lg n)/2 \\rfloor$ flips.\nProb. a specific group is all heads is $1/2^s \\ge 1/\\sqrt{n}$.\nProb. a specific group is NOT all heads $\\le 1 - 1/\\sqrt{n}$.\nProb. ALL groups are NOT all heads $\\le (1 - 1/\\sqrt{n})^{n/s} \\approx e^{-(n/s)/\\sqrt{n}} \\approx e^{-\\text{poly}(\\lg n)\\sqrt{n}} = O(1/n)$. (The book's derivation $(1-1/\\sqrt{n})^{2n/\\lg n - 1} \\le e^{-(2n/\\lg n - 1)/\\sqrt{n}} = O(1/n)$ is correct.)\nSo, prob. at least one group is all heads is $1 - O(1/n)$. This means a streak of length $s = \\Omega(\\lg n)$ is likely. $E[L] = \\Omega(\\lg n)$.\n\n### 5.4.4 The online hiring problem\nVariant: Hire exactly once. After each interview, must hire or reject immediately.\nStrategy: Select $k < n$. Interview and reject first $k$ candidates. Then hire the first subsequent candidate better than all previous $k$ (i.e., better than best of first $k$). If no such candidate by end, hire $n$-th candidate.\nProcedure `ONLINE-MAXIMUM(k, n)`. [[PartI Foundations Algorithms.md#C5 5.4 ONLINE-MAXIMUM]]\nLet $S$ be event you succeed (hire the best overall candidate). Let $S_i$ be event you succeed and best is $i$-th candidate. $\\text{Pr}\\{S_i\\}=0$ for $i \\le k$.\nFor $i > k$, to succeed with $i$-th candidate (who must be the overall best): \n1. Candidate $i$ must be the best: event $B_i$, $\\text{Pr}\\{B_i\\} = 1/n$.\n2. Algorithm must not hire anyone from $k+1, \\dots, i-1$. This means best of first $k$ candidates must also be best of first $i-1$ candidates: event $O_i$. $\\text{Pr}\\{O_i\\} = k/(i-1)$ (best of $1, \\dots, i-1$ is in first $k$ positions).\nEvents $B_i, O_i$ are independent.\n$\\text{Pr}\\{S_i\\} = \\text{Pr}\\{B_i\\} \\text{Pr}\\{O_i\\} = (1/n) \\cdot (k/(i-1))$.\n$\\text{Pr}\\{S\\} = \\sum_{i=k+1}^n \\text{Pr}\\{S_i\\} = \\sum_{i=k+1}^n \\frac{k}{n(i-1)} = \\frac{k}{n} \\sum_{j=k}^{n-1} \\frac{1}{j}$.\nUsing integral bounds for sum: $\\frac{k}{n} (\\ln n - \\ln k) \\le \\text{Pr}\\{S\\} \\le \\frac{k}{n} (\\ln(n-1) - \\ln(k-1))$.\nTo maximize lower bound $\frac{k}{n}(\\ln n - \\ln k)$, differentiate w.r.t $k$ and set to 0:\n$(\\ln n - \\ln k - 1)/n = 0 \rmsg \\ln k = \\ln n - 1 = \\ln(n/e) \rmsg k=n/e$.\nWith $k=n/e$, $\\text{Pr}\\{S\\} \\approx \\frac{n/e}{n}(\\ln n - \\ln(n/e)) = (1/e)(\\ln n - (\\ln n - \\ln e)) = (1/e)(\\ln e) = 1/e$.\nProbability of success is at least $1/e \\approx 0.368$."
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartII Sorting and Order Statistics Algorithms.md",
                "contents": "# C6 Heapsort Algorithms\n\n## 6.1 PARENT\n```\nPARENT(i)\n  1 return floor(i/2)\n```\n\n## 6.1 LEFT\n```\nLEFT(i)\n  1 return 2i\n```\n\n## 6.1 RIGHT\n```\nRIGHT(i)\n  1 return 2i + 1\n```\n\n## 6.2 MAX-HEAPIFY\n```\nMAX-HEAPIFY(A, i)\n  1  l = LEFT(i)\n  2  r = RIGHT(i)\n  3  if l <= A.heap-size and A[l] > A[i]\n  4      largest = l\n  5  else largest = i\n  6  if r <= A.heap-size and A[r] > A[largest]\n  7      largest = r\n  8  if largest != i\n  9      exchange A[i] with A[largest]\n  10     MAX-HEAPIFY(A, largest)\n```\n\n## 6.3 BUILD-MAX-HEAP\n```\nBUILD-MAX-HEAP(A, n) // n is the number of elements in A, implicitly A.length\n  1  A.heap-size = n // or A.length\n  2  for i = floor(A.length / 2) downto 1\n  3      MAX-HEAPIFY(A, i)\n```\n\n## 6.4 HEAPSORT\n```\nHEAPSORT(A, n) // n is the number of elements in A, implicitly A.length\n  1  BUILD-MAX-HEAP(A, n)\n  2  for i = A.length downto 2\n  3      exchange A[1] with A[i]\n  4      A.heap-size = A.heap-size - 1\n  5      MAX-HEAPIFY(A, 1)\n```\n\n## 6.5 MAX-HEAP-MAXIMUM\n```\nMAX-HEAP-MAXIMUM(A)\n  1  if A.heap-size < 1\n  2      error \"heap underflow\"\n  3  return A[1]\n```\n\n## 6.5 MAX-HEAP-EXTRACT-MAX\n```\nMAX-HEAP-EXTRACT-MAX(A)\n  1  max = MAX-HEAP-MAXIMUM(A) // Checks for underflow\n  2  A[1] = A[A.heap-size]\n  3  A.heap-size = A.heap-size - 1\n  4  MAX-HEAPIFY(A, 1)\n  5  return max\n```\n\n## 6.5 MAX-HEAP-INCREASE-KEY\n```\nMAX-HEAP-INCREASE-KEY(A, x_index, k) // x_index is the index of the element, k is the new key\n  1  if k < A[x_index].key\n  2      error \"new key is smaller than current key\"\n  3  A[x_index].key = k\n  4  i = x_index // Use i for the index from here\n  5  while i > 1 and A[PARENT(i)].key < A[i].key\n  6      exchange A[i] with A[PARENT(i)] // updating mapping info if necessary\n  7      i = PARENT(i)\n```\n\n## 6.5 MAX-HEAP-INSERT\n```\nMAX-HEAP-INSERT(A, x_object, array_capacity_n) // x_object is the new object, n is capacity\n  1  if A.heap-size == array_capacity_n\n  2      error \"heap overflow\"\n  3  A.heap-size = A.heap-size + 1\n  4  k_orig = x_object.key\n  5  x_object.key = -infinity // Sentinel key value\n  6  A[A.heap-size] = x_object // Store object pointer/value\n  7  // map x_object to index A.heap-size in the array (if handles are used)\n  8  MAX-HEAP-INCREASE-KEY(A, A.heap-size, k_orig)\n```\n\n## P6-1 BUILD-MAX-HEAP'\n```\nBUILD-MAX-HEAP'(A, n) // n is A.length\n  1  A.heap-size = 1\n  2  for i = 2 to n\n  3      MAX-HEAP-INSERT(A, A[i], n) // Assuming A[i] is the object/key\n```\n\n# C7 Quicksort Algorithms\n\n## 7.1 QUICKSORT\n```\nQUICKSORT(A, p, r)\n  1  if p < r\n  2      // Partition the subarray around the pivot, which ends up in A[q].\n  3      q = PARTITION(A, p, r)\n  4      QUICKSORT(A, p, q - 1)  // recursively sort the low side\n  5      QUICKSORT(A, q + 1, r) // recursively sort the high side\n```\n\n## 7.1 PARTITION\n```\nPARTITION(A, p, r)\n  1  x = A[r]                // the pivot\n  2  i = p - 1               // highest index into the low side\n  3  for j = p to r - 1      // process each element other than the pivot\n  4      if A[j] <= x        // does this element belong on the low side?\n  5          i = i + 1           // index of a new slot in the low side\n  6          exchange A[i] with A[j] // put this element there\n  7  exchange A[i + 1] with A[r] // pivot goes just to the right of the low side\n  8  return i + 1             // new index of the pivot\n```\n\n## 7.3 RANDOMIZED-PARTITION\n```\nRANDOMIZED-PARTITION(A, p, r)\n  1  i = RANDOM(p, r)\n  2  exchange A[r] with A[i]\n  3  return PARTITION(A, p, r)\n```\n\n## 7.3 RANDOMIZED-QUICKSORT\n```\nRANDOMIZED-QUICKSORT(A, p, r)\n  1  if p < r\n  2      q = RANDOMIZED-PARTITION(A, p, r)\n  3      RANDOMIZED-QUICKSORT(A, p, q - 1)\n  4      RANDOMIZED-QUICKSORT(A, q + 1, r)\n```\n\n## P7-1 HOARE-PARTITION\n```\nHOARE-PARTITION(A, p, r)\n  1  x = A[p]\n  2  i = p - 1\n  3  j = r + 1\n  4  while TRUE\n  5      repeat\n  6          j = j - 1\n  7      until A[j] <= x\n  8      repeat\n  9          i = i + 1\n  10     until A[i] >= x\n  11     if i < j\n  12         exchange A[i] with A[j]\n  13     else return j\n```\n\n## P7-4 STOOGE-SORT\n```\nSTOOGE-SORT(A, p, r)\n  1  if A[p] > A[r]\n  2      exchange A[p] with A[r]\n  3  if p + 1 < r\n  4      k = floor((r - p + 1) / 3) // round down\n  5      STOOGE-SORT(A, p, r - k)   // first two-thirds\n  6      STOOGE-SORT(A, p + k, r)   // last two-thirds\n  7      STOOGE-SORT(A, p, r - k)   // first two-thirds again\n```\n\n## P7-5 TRE-QUICKSORT\n```\nTRE-QUICKSORT(A, p, r)\n  1  while p < r\n       // Partition and then sort the low side.\n  2    q = PARTITION(A, p, r)\n  3    TRE-QUICKSORT(A, p, q - 1)\n  4    p = q + 1\n```\n\n# C8 Sorting in Linear Time Algorithms\n\n## 8.2 COUNTING-SORT\n```\nCOUNTING-SORT(A, n, k) // B is output array, C is temp working storage\n  1  let B[1..n] and C[0..k] be new arrays\n  2  for i = 0 to k\n  3      C[i] = 0\n  4  for j = 1 to n\n  5      C[A[j]] = C[A[j]] + 1\n  6  // C[i] now contains the number of elements equal to i.\n  7  for i = 1 to k\n  8      C[i] = C[i] + C[i - 1]\n  9  // C[i] now contains the number of elements less than or equal to i.\n  10 // Copy A to B, starting from the end of A.\n  11 for j = n downto 1\n  12     B[C[A[j]]] = A[j]\n  13     C[A[j]] = C[A[j]] - 1 // to handle duplicate values\n  14 return B\n```\n\n## 8.3 RADIX-SORT\n```\nRADIX-SORT(A, n, d)\n  1  for i = 1 to d\n  2      use a stable sort to sort array A[1..n] on digit i\n```\n\n## P8-7 COMPARE-EXCHANGE\n```\nCOMPARE-EXCHANGE(A, i, j)\n  1  if A[i] > A[j]\n  2      exchange A[i] with A[j]\n```\n\n## P8-7 COMPARE-EXCHANGE-INSERTION-SORT\n```\nCOMPARE-EXCHANGE-INSERTION-SORT(A, n)\n  1  for i = 2 to n\n  2      for j = i - 1 downto 1\n  3          COMPARE-EXCHANGE(A, j, j + 1)\n```\n\n# C9 Medians and Order Statistics Algorithms\n\n## 9.1 MINIMUM\n```\nMINIMUM(A, n)\n  1  min = A[1]\n  2  for i = 2 to n\n  3      if min > A[i]\n  4          min = A[i]\n  5  return min\n```\n\n## 9.2 RANDOMIZED-SELECT\n```\nRANDOMIZED-SELECT(A, p, r, i)\n  1  if p == r\n  2      return A[p]         // 1 <= i <= r - p + 1 when p == r means that i = 1\n  3  q = RANDOMIZED-PARTITION(A, p, r)\n  4  k = q - p + 1\n  5  if i == k\n  6      return A[q]         // the pivot value is the answer\n  7  elseif i < k\n  8      return RANDOMIZED-SELECT(A, p, q - 1, i)\n  9  else return RANDOMIZED-SELECT(A, q + 1, r, i - k)\n```\n\n## 9.3 SELECT\n```\nSELECT(A, p, r, i)\n  1  while (r - p + 1) mod 5 != 0\n       // put the minimum into A[p]\n  2    for j = p + 1 to r\n  3        if A[p] > A[j]\n  4            exchange A[p] with A[j]\n       // If we want the minimum of A[p:r], we're done.\n  5    if i == 1\n  6        return A[p]\n       // Otherwise, we want the (i - 1)st element of A[p + 1:r].\n  7    p = p + 1\n  8    i = i - 1\n  9  g = (r - p + 1) / 5                         // number of 5-element groups\n  10 for j = p to p + g - 1                     // sort each group\n  11     sort (A[j], A[j+g], A[j+2g], A[j+3g], A[j+4g]) in place\n     // All group medians now lie in the middle fifth of A[p:r].\n  12 // Find the pivot x recursively as the median of the group medians.\n  13 x = SELECT(A, p + 2g, p + 3g - 1, floor(g/2))\n  14 q = PARTITION-AROUND(A, p, r, x)          // partition around the pivot\n     // The rest is just like lines 3-9 of RANDOMIZED-SELECT.\n  15 k = q - p + 1\n  16 if i == k\n  17     return A[q]                            // the pivot value is the answer\n  18 elseif i < k\n  19     return SELECT(A, p, q - 1, i)\n  20 else return SELECT(A, q + 1, r, i - k)\n```\n\n## P9-2 SIMPLER-RANDOMIZED-SELECT\n```\nSIMPLER-RANDOMIZED-SELECT(A, p, r, i)\n  1  if p == r\n  2      return A[p]         // 1 <= i <= r - p + 1 means that i = 1\n  3  q = RANDOMIZED-PARTITION(A, p, r)\n  4  k = q - p + 1\n  5  if i <= k\n  6      return SIMPLER-RANDOMIZED-SELECT(A, p, q, i)\n  7  else return SIMPLER-RANDOMIZED-SELECT(A, q + 1, r, i - k)\n```\n\n## P9-6 SELECT3\n```\nSELECT3(A, p, r, i)\n  1  while (r - p + 1) mod 9 != 0\n  2      for j = p + 1 to r\n  3          if A[p] > A[j]\n  4              exchange A[p] with A[j] // put the minimum into A[p]\n  5      if i == 1 // If we want the minimum of A[p:r], we're done.\n  6          return A[p]\n  7      // Otherwise, we want the (i - 1)st element of A[p + 1 : r].\n  8      p = p + 1\n  9      i = i - 1\n  10 g = (r - p + 1) / 3                        // number of 3-element groups\n  11 for j = p to p + g - 1                    // run through the groups\n  12     sort (A[j], A[j + g], A[j + 2g]) in place\n     // All group medians now lie in the middle third of A[p:r].\n  13 g_prime = g / 3                            // number of 3-element subgroups\n  14 for j = p + g to p + g + g_prime - 1      // sort the subgroups\n  15     sort (A[j], A[j + g_prime], A[j + 2g_prime]) in place\n     // All subgroup medians now lie in the middle ninth of A[p:r].\n  16 // Find the pivot x recursively as the median of the subgroup medians.\n  17 x = SELECT3(A, p + 4g_prime, p + 5g_prime - 1, floor(g_prime/2))\n  18 q = PARTITION-AROUND(A, p, r, x)          // partition around the pivot\n     // The rest is just like lines 19-24 of SELECT.\n  19 k = q - p + 1\n  20 if i == k\n  21     return A[q]                            // the pivot value is the answer\n  22 elseif i < k\n  23     return SELECT3(A, p, q - 1, i)\n  24 else return SELECT3(A, q + 1, r, i - k)\n```\n"
            },
            {
                "file": "PartII Sorting and Order Statistics/PartII Introduction.md",
                "contents": "# Part II Introduction: Sorting and Order Statistics\n\nThis part presents several algorithms that solve the sorting problem and problems related to order statistics.\n\n## The Sorting Problem\nInput: A sequence of $n$ numbers $(a_1, a_2, \\dots, a_n)$.\nOutput: A permutation (reordering) $(a'_1, a'_2, \\dots, a'_n)$ of the input sequence such that $a'_1 \\le a'_2 \\le \\dots \\le a'_n$.\nThe input sequence is usually an $n$-element array, though it might be represented in other ways, like a linked list.\n\n## The structure of the data\nNumbers to be sorted are rarely isolated values. They are often part of a collection of data called a **record**. Each record contains a **key**, which is the value to be sorted. The remainder of the record consists of **satellite data**, carried around with the key. When a sorting algorithm permutes keys, it must also permute the satellite data. If satellite data is large, it's often more efficient to permute an array of pointers to the records.\nA sorting algorithm describes the *method* to determine the sorted order, regardless of whether individual numbers or large records are being sorted. For simplicity, we typically assume the input consists only of numbers.\n\n## Why sorting?\nSorting is a fundamental problem in computer science for several reasons:\n-   Applications inherently need sorted information (e.g., bank statements sorted by check number).\n-   Algorithms often use sorting as a key subroutine (e.g., rendering graphical objects by an \"above\" relation).\n-   A wide variety of sorting algorithms exist, employing a rich set of techniques. Many important algorithm design techniques appear in sorting algorithms.\n-   We can prove a nontrivial lower bound for sorting (e.g., $\\Omega(n \\lg n)$ for comparison sorts). Some algorithms are asymptotically optimal.\n-   Many engineering issues arise when implementing sorting algorithms, related to data characteristics, memory hierarchy, and software environment.\n\n## Sorting algorithms overview\n-   **Insertion sort** (Chapter 2): Takes $\\Theta(n^2)$ time in the worst case. Fast for small inputs. Sorts *in place*.\n-   **Merge sort** (Chapter 2): Has a better asymptotic running time, $\\Theta(n \\lg n)$. Does not sort in place (standard version).\n-   **Heapsort** (Chapter 6): Sorts $n$ numbers in place in $O(n \\lg n)$ time. Uses a heap data structure.\n-   **Quicksort** (Chapter 7): Sorts $n$ numbers in place. Worst-case time is $\\Theta(n^2)$, but expected running time is $\\Theta(n \\lg n)$. Often outperforms heapsort in practice due to tight code and small constant factors.\n\nInsertion sort, merge sort, heapsort, and quicksort are **comparison sorts**: they determine the sorted order by comparing elements.\nChapter 8 introduces the decision-tree model to study limitations of comparison sorts, proving a lower bound of $\\Omega(n \\lg n)$.\n\nChapter 8 also presents algorithms that beat this lower bound by using information other than comparisons:\n-   **Counting sort**: Assumes input numbers are in a set \\{0, 1, ..., k\\}. Sorts $n$ numbers in $\\Theta(k+n)$ time. Linear if $k=O(n)$.\n-   **Radix sort**: Extends counting sort. Sorts $n$ $d$-digit numbers (each digit up to $k$ values) in $\\Theta(d(n+k))$ time. Linear if $d$ is constant and $k=O(n)$.\n-   **Bucket sort**: Requires knowledge of probabilistic distribution. Sorts $n$ real numbers uniformly distributed in $[0,1)$ in average-case $O(n)$ time.\n\n### Summary of Running Times (from page 160 of OCR)\n| Algorithm      | Worst-case running time | Average-case/expected running time |\n|----------------|-------------------------|------------------------------------|\n| Insertion sort | $\\Theta(n^2)$             | $\\Theta(n^2)$                       |\n| Merge sort     | $\\Theta(n \\lg n)$          | $\\Theta(n \\lg n)$                    |\n| Heapsort       | $O(n \\lg n)$             | --- (not analyzed in book)         |\n| Quicksort      | $\\Theta(n^2)$             | $\\Theta(n \\lg n)$ (expected)         |\n| Counting sort  | $\\Theta(k+n)$            | $\\Theta(k+n)$                      |\n| Radix sort     | $\\Theta(d(n+k))$         | $\\Theta(d(n+k))$                   |\n| Bucket sort    | $\\Theta(n^2)$             | $\\Theta(n)$ (average-case)         |\n\n## Order statistics\nThe $i$th order statistic of a set of $n$ numbers is the $i$th smallest number. Selecting it by sorting takes $\\Omega(n \\lg n)$ time.\nChapter 9 shows how to find the $i$th smallest element in $O(n)$ time (worst-case and expected for different algorithms).\n\n## Background\nSome analyses (quicksort, bucket sort, order-statistic algorithm) use probability and randomized algorithms, reviewed in Appendix C and Chapter 5."
            },
            {
                "file": "PartII Sorting and Order Statistics/C6 Heapsort.md",
                "contents": "# 6 Heapsort\n\nThis chapter introduces heapsort. Like merge sort, but unlike insertion sort, heapsort's running time is $O(n \\lg n)$. Like insertion sort, but unlike merge sort, heapsort sorts in place: only a constant number of array elements are stored outside the input array at any time. Heapsort combines attributes of merge sort and insertion sort. It also introduces the heap data structure, useful for priority queues.\nThe term \"heap\" in this context refers to the data structure, not garbage-collected storage.\n\n## 6.1 Heaps\nThe (binary) **heap** data structure is an array object that can be viewed as a nearly complete binary tree. Each node of the tree corresponds to an element of the array. The tree is completely filled on all levels except possibly the lowest, which is filled from the left. An array $A[1..n]$ representing a heap has an attribute $A.heap-size$, indicating how many elements in the heap are stored within array $A$. Only elements in $A[1..A.heap-size]$ are valid elements of the heap. The root of the tree is $A[1]$.\nGiven the index $i$ of a node, indices of its parent, left child, and right child can be computed:\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.1 PARENT]]: `PARENT(i)` returns $\\lfloor i/2 \\rfloor$.\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.1 LEFT]]: `LEFT(i)` returns $2i$.\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.1 RIGHT]]: `RIGHT(i)` returns $2i+1$.\nThese can be implemented efficiently using bit shifts.\n\nThere are two kinds of binary heaps: max-heaps and min-heaps. They differ in the **heap property** they satisfy.\n-   In a **max-heap**, the max-heap property is that for every node $i$ other than the root, $A[\\text{PARENT}(i)] \\ge A[i]$. The largest element is at the root. Subtrees rooted at a node contain values no larger than the node itself.\n-   In a **min-heap**, the min-heap property is that for every node $i$ other than the root, $A[\\text{PARENT}(i)] \\le A[i]$. The smallest element is at the root.\n\nHeapsort uses max-heaps. Min-heaps are common for priority queues.\nThe **height** of a node in a heap is the number of edges on the longest simple downward path from the node to a leaf. The height of the heap is the height of its root. For an $n$-element heap, the height is $\\Theta(\\lg n)$. Basic heap operations run in time proportional to the tree's height, i.e., $O(\\lg n)$.\n\nKey procedures for heaps and heapsort:\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.2 MAX-HEAPIFY]]: Runs in $O(\\lg n)$ time, maintains the max-heap property.\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.3 BUILD-MAX-HEAP]]: Runs in linear time, produces a max-heap from an unordered input array.\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.4 HEAPSORT]]: Runs in $O(n \\lg n)$ time, sorts an array in place.\n-   Procedures for priority queues ([[PartII Sorting and Order Statistics Algorithms.md#6.5 MAX-HEAP-INSERT]], etc.): Run in $O(\\lg n)$ time.\n\n## 6.2 Maintaining the heap property\nThe [[PartII Sorting and Order Statistics Algorithms.md#6.2 MAX-HEAPIFY]] procedure maintains the max-heap property. Its inputs are an array $A$ and an index $i$. When called, `MAX-HEAPIFY` assumes binary trees rooted at `LEFT(i)` and `RIGHT(i)` are max-heaps, but $A[i]$ might be smaller than its children, violating the property. `MAX-HEAPIFY` lets $A[i]$ \"float down\" in the max-heap so the subtree at $i$ obeys the property. Figure 6.2 of the book illustrates its action.\n\nAnalysis of `MAX-HEAPIFY`: The running time $T(n)$ on a subtree of size $n$ is $\\Theta(1)$ (to fix relationships among $A[i]$, $A[\\text{LEFT}(i)]$, $A[\\text{RIGHT}(i)]$) plus the time for a recursive call on a child's subtree. Children's subtrees have size at most $2n/3$. The recurrence is $T(n) \\le T(2n/3) + \\Theta(1)$, which by the master theorem (case 2) solves to $T(n) = O(\\lg n)$. Alternatively, running time on a node of height $h$ is $O(h)$.\n\n## 6.3 Building a heap\nThe [[PartII Sorting and Order Statistics Algorithms.md#6.3 BUILD-MAX-HEAP]] procedure converts an array $A[1..n]$ into a max-heap by calling `MAX-HEAPIFY` bottom-up. Elements $A[\\lfloor n/2 \\rfloor + 1 .. n]$ are leaves, forming 1-element heaps. `BUILD-MAX-HEAP` iterates through remaining nodes (from $\\lfloor n/2 \\rfloor$ down to 1) running `MAX-HEAPIFY` on each. Figure 6.3 of the book shows an example.\n\nCorrectness of `BUILD-MAX-HEAP` uses a loop invariant:\n-   **Loop Invariant**: At the start of each iteration of the `for` loop (lines 2-3), each node $i+1, i+2, \\dots, n$ is the root of a max-heap.\n-   **Initialization**: Before the first iteration, $i = \\lfloor n/2 \\rfloor$. Nodes $\\lfloor n/2 \\rfloor + 1, \\dots, n$ are leaves, thus roots of trivial max-heaps.\n-   **Maintenance**: Children of node $i$ are numbered higher than $i$. By the invariant, they are roots of max-heaps. This allows `MAX-HEAPIFY(A, i)` to make node $i$ a max-heap root. `MAX-HEAPIFY` preserves the property for nodes $i+1, \\dots, n$. Decrementing $i$ reestablishes the invariant for the next iteration.\n-   **Termination**: The loop terminates when $i=0$. Each node $1, \\dots, n$ is a root of a max-heap. Node 1 is the root of the entire heap.\n\nAnalysis of `BUILD-MAX-HEAP`: A simple upper bound is $O(n \\lg n)$ (O(n) calls to `MAX-HEAPIFY` at $O(\\lg n)$ each). A tighter bound observes that `MAX-HEAPIFY` time varies with node height. An $n$-element heap has height $\\lfloor \\lg n \\rfloor$ and at most $\\lceil n/2^{h+1} \\rceil$ nodes of height $h$. Total cost is bounded by $\\sum_{h=0}^{\\lfloor \\lg n \\rfloor} \\lceil n/2^{h+1} \\rceil O(h)$. This sum can be shown to be $O(n)$. Thus, a max-heap can be built from an unordered array in linear time.\n\n## 6.4 The heapsort algorithm\nThe [[PartII Sorting and Order Statistics Algorithms.md#6.4 HEAPSORT]] algorithm starts by using `BUILD-MAX-HEAP` to build a max-heap on $A[1..n]$. The maximum element $A[1]$ is placed in its correct final position by exchanging it with $A[n]$. The heap size is reduced by 1 (discarding node $n$). The children of the new root remain max-heaps, but the new root element might violate the max-heap property. `MAX-HEAPIFY(A,1)` restores it. This process repeats for $A[1..n-1]$ down to $A[1..2]$. Figure 6.4 of the book shows an example.\n\nAnalysis of `HEAPSORT`: `BUILD-MAX-HEAP` takes $O(n)$ time. There are $n-1$ calls to `MAX-HEAPIFY`, each taking $O(\\lg n)$ time. Total time is $O(n) + (n-1)O(\\lg n) = O(n \\lg n)$.\n\n## 6.5 Priority queues\nA **priority queue** is a data structure for maintaining a set $S$ of elements, each with an associated value called a **key**. A **max-priority queue** supports:\n-   `INSERT(S, x, k)`: Inserts element $x$ with key $k$ into $S$.\n-   `MAXIMUM(S)`: Returns element of $S$ with the largest key.\n-   `EXTRACT-MAX(S)`: Removes and returns element of $S$ with the largest key.\n-   `INCREASE-KEY(S, x, k)`: Increases key of element $x$ to new value $k$.\nApplications include job scheduling. A **min-priority queue** supports `INSERT`, `MINIMUM`, `EXTRACT-MIN`, `DECREASE-KEY`, used in event-driven simulators.\n\nHeaps implement priority queues efficiently. When mapping application objects to heap elements, **handles** (e.g., array indices) can be used.\nUsing a max-heap for a max-priority queue:\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.5 MAX-HEAP-MAXIMUM]]: Returns $A[1]$ in $\\Theta(1)$ time.\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.5 MAX-HEAP-EXTRACT-MAX]]: Saves $A[1]$, moves $A[A.heap-size]$ to $A[1]$, decrements heap size, calls `MAX-HEAPIFY(A,1)$. Runs in $O(\\lg n)$ time.\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.5 MAX-HEAP-INCREASE-KEY]]: Updates key of element at index $i$. If new key violates max-heap property (larger than parent), traverses up towards root, exchanging with parent until property holds. Runs in $O(\\lg n)$ time.\n-   [[PartII Sorting and Order Statistics Algorithms.md#6.5 MAX-HEAP-INSERT]]: Adds new leaf with key $-\\infty$, then calls `MAX-HEAP-INCREASE-KEY` to set its correct key and position. Runs in $O(\\lg n)$ time.\n\n## Chapter Notes\nHeapsort was invented by Williams (1964). `BUILD-MAX-HEAP` by Floyd (1964). Analysis details by Schaffer and Sedgewick. Advanced priority queue structures (e.g., Fibonacci heaps) offer better amortized times for some operations."
            },
            {
                "file": "PartII Sorting and Order Statistics/C7 Quicksort.md",
                "contents": "# 7 Quicksort\n\nQuicksort has a worst-case running time of $\\Theta(n^2)$ but is often the best practical choice for sorting due to its average-case efficiency of $\\Theta(n \\lg n)$ with small constant factors. It sorts in place and works well in virtual-memory environments.\n\n## 7.1 Description of quicksort\nQuicksort applies the divide-and-conquer method:\n-   **Divide**: Partition (rearrange) subarray $A[p..r]$ into two (possibly empty) subarrays $A[p..q-1]$ and $A[q+1..r]$ such that each element in $A[p..q-1]$ is $\\le A[q]$ (the **pivot**), which is, in turn, $\\le$ each element in $A[q+1..r]$. Compute index $q$ of the pivot.\n-   **Conquer**: Sort the two subarrays $A[p..q-1]$ and $A[q+1..r]$ by recursive calls to quicksort.\n-   **Combine**: No work needed, as subarrays are sorted in place.\n\nThe [[PartII Sorting and Order Statistics Algorithms.md#7.1 QUICKSORT]] procedure implements this. Initial call is `QUICKSORT(A, 1, n)`.\n\n### Partitioning the array\nThe key is the [[PartII Sorting and Order Statistics Algorithms.md#7.1 PARTITION]] procedure. It rearranges $A[p..r]$ in place. It always selects $A[r]$ as the pivot $x$. It maintains four regions: $A[p..i]$ (elements $\\le x$), $A[i+1..j-1]$ (elements $> x$), $A[j..r-1]$ (unprocessed), and $A[r]$ (pivot $x$). Figure 7.1 of the book shows its operation.\n\nCorrectness of `PARTITION` uses a loop invariant for the `for` loop (lines 3-6):\nAt the start of each iteration, for any array index $k$:\n1.  If $p \\le k \\le i$, then $A[k] \\le x$.\n2.  If $i+1 \\le k \\le j-1$, then $A[k] > x$.\n3.  If $k=r$, then $A[k]=x$.\n-   **Initialization**: Before the loop, $i=p-1$, $j=p$. Conditions 1 and 2 are trivially true (empty regions). Condition 3 holds by $x=A[r]$.\n-   **Maintenance**: If $A[j] > x$, $j$ is incremented, $A[j-1]$ now satisfies condition 2. If $A[j] \\le x$, $i$ is incremented, $A[i]$ is swapped with $A[j]$, then $j$ is incremented. $A[i]$ now satisfies condition 1. $A[j-1]$ (which received an element $>x$ or was $A[j]$ that was $>x$ before swap) satisfies condition 2.\n-   **Termination**: Loop terminates when $j=r$. All elements are in one of the three sets. The final swap in line 7 places pivot $x$ between the low and high sides.\n`PARTITION` takes $\\Theta(n)$ time for $n=r-p+1$ elements.\n\n## 7.2 Performance of quicksort\nRunning time depends on partitioning balance, which depends on pivot choice.\nMemory usage: Quicksort is in-place, but recursion stack depth can be $\\Theta(n)$ in worst case.\n\n-   **Worst-case partitioning**: If partitioning always produces one subproblem of $n-1$ elements and one of 0. Recurrence: $T(n) = T(n-1) + \\Theta(n)$. Solution: $T(n) = \\Theta(n^2)$. Occurs if array is already sorted or reverse sorted (with $A[r]$ as pivot).\n-   **Best-case partitioning**: If partitioning splits problem into two subproblems of size $\\approx n/2$. Recurrence: $T(n) = 2T(n/2) + \\Theta(n)$. Solution: $T(n) = \\Theta(n \\lg n)$.\n-   **Balanced partitioning**: Even a constant-proportional split (e.g., 9-to-1) yields $T(n) = \\Theta(n \\lg n)$. The recursion tree depth is $\\Theta(\\lg n)$, and cost at each level is $O(n)$. Figure 7.4 of the book illustrates this.\n\n### Intuition for the average case\nAssuming all input permutations are equally likely, `PARTITION` produces a mix of good and bad splits. Even if good and bad splits alternate, the cost of a bad split can be absorbed into the cost of a subsequent good split at the next level, effectively leading to an $O(n \\lg n)$ running time. Figure 7.5 of the book illustrates this concept.\n\n## 7.3 A randomized version of quicksort\nTo avoid worst-case behavior on specific inputs (like sorted arrays), we can randomize. Instead of always using $A[r]$ as pivot, [[PartII Sorting and Order Statistics Algorithms.md#7.3 RANDOMIZED-PARTITION]] chooses a random element from $A[p..r]$ as pivot, swaps it with $A[r]$, then proceeds as `PARTITION`. The [[PartII Sorting and Order Statistics Algorithms.md#7.3 RANDOMIZED-QUICKSORT]] procedure uses this.\nWith randomized pivot selection, no particular input elicits worst-case behavior; the behavior depends only on random choices. Expected split is reasonably balanced.\n\n## 7.4 Analysis of quicksort\n\n### 7.4.1 Worst-case analysis\nFor `QUICKSORT` (or `RANDOMIZED-QUICKSORT`), let $T(n)$ be worst-case time. The recurrence is $T(n) = \\max_{0 \\le q \\le n-1} (T(q) + T(n-1-q)) + \\Theta(n)$.\nThe expression $q^2 + (n-1-q)^2$ is maximized when $q=0$ or $q=n-1$. So, $T(n) \\le c(n-1)^2 + \\Theta(n) = cn^2 - c(2n-1) + \\Theta(n)$. For large enough $c$, $c(2n-1)$ dominates $\\Theta(n)$, so $T(n) \\le cn^2$. Thus $T(n) = O(n^2)$. Since worst-case partitioning gives $\\Theta(n^2)$, the worst-case running time is $\\Theta(n^2)$.\n\n### 7.4.2 Expected running time\nAssume distinct elements. Let $X$ be total comparisons. Running time is $O(n+X)$ (Lemma 7.1).\nLet $z_1 < z_2 < \\dots < z_n$ be the sorted elements. Let $Z_{ij} = \\{z_i, \\dots, z_j\\}$.\n**Lemma 7.2**: $z_i$ is compared with $z_j$ ($i<j$) iff one of them is the first pivot chosen from $Z_{ij}$. No two elements are compared more than once.\n**Lemma 7.3**: For $i<j$, Pr{$z_i$ is compared with $z_j$} = $2/(j-i+1)$. This is because any element in $Z_{ij}$ is equally likely to be the first pivot from $Z_{ij}$. There are $j-i+1$ elements in $Z_{ij}$. Comparison happens if $z_i$ or $z_j$ is chosen first.\n\n**Theorem 7.4**: Expected running time of `RANDOMIZED-QUICKSORT` is $O(n \\lg n)$.\nProof: Let $X_{ij}$ be indicator variable for $z_i$ being compared with $z_j$. $X = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n X_{ij}$.\n$E[X] = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n E[X_{ij}] = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n \\text{Pr}\\{z_i \\text{ compared with } z_j\\}$\n$E[X] = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^n \\frac{2}{j-i+1}$. Let $k = j-i+1$.\n$E[X] = \\sum_{i=1}^{n-1} \\sum_{k=2}^{n-i+1} \\frac{2}{k} < \\sum_{i=1}^{n-1} \\sum_{k=1}^n \\frac{2}{k} = \\sum_{i=1}^{n-1} O(\\lg n) = O(n \\lg n)$.\nThus, expected running time is $O(n + n \\lg n) = O(n \\lg n)$. Combined with the $\\Omega(n \\lg n)$ average-case lower bound for comparison sorts, this is $\\Theta(n \\lg n)$.\n\n## Chapter Notes\nQuicksort invented by Hoare. `PARTITION` in text by Lomuto. Analysis based on Motwani and Raghavan. Sedgewick and Bentley discuss implementation details. McIlroy showed how to construct inputs that degrade most quicksort implementations."
            },
            {
                "file": "PartII Sorting and Order Statistics/C8 Sorting in Linear Time.md",
                "contents": "# 8 Sorting in Linear Time\n\nComparison sorts like merge sort and heapsort have a worst-case running time of $O(n \\lg n)$, and quicksort has an average-case time of $O(n \\lg n)$. These algorithms determine sorted order based only on comparisons.\nThis chapter proves that any comparison sort must make $\\Omega(n \\lg n)$ comparisons in the worst case. It then examines three algorithms (counting sort, radix sort, bucket sort) that run in linear time by using operations other than comparisons.\n\n## 8.1 Lower bounds for sorting\nA **comparison sort** uses only comparisons ($<, \\le, =, \\ge, >$) to determine element order. Assume distinct input elements, so comparisons effectively become $a_i \\le a_j$.\n\n### The decision-tree model\nComparison sorts can be modeled by **decision trees**. A decision tree is a full binary tree representing comparisons for a given input size $n$. Internal nodes are $i:j$ (compare $A[i]$ and $A[j]$). Leaves are permutations $\\langle \\pi(1), \\dots, \np(n) \\rangle$ representing the sorted order. Execution traces a path from root to a leaf. Each of the $n!$ permutations must be a reachable leaf for correctness. Figure 8.1 of the book shows an example for $n=3$.\n\n### A lower bound for the worst case\nWorst-case comparisons for an algorithm equals its decision tree height $h$.\n**Theorem 8.1**: Any comparison sort algorithm requires $\\Omega(n \\lg n)$ comparisons in the worst case.\nProof: A decision tree of height $h$ has at most $2^h$ leaves. Let $l$ be number of reachable leaves. We need $n! \\le l$. So $n! \\le 2^h$. Taking logarithms: $h \\ge \\lg(n!)$.\nSince $\\lg(n!) = \\Omega(n \\lg n)$ (e.g., using Stirling's approximation or $\\sum_{k=1}^n \\lg k$), $h = \\Omega(n \\lg n)$.\n\n**Corollary 8.2**: Heapsort and merge sort are asymptotically optimal comparison sorts.\nTheir $O(n \\lg n)$ upper bounds match the $\\Omega(n \\lg n)$ worst-case lower bound.\n\n## 8.2 Counting sort\n[[PartII Sorting and Order Statistics Algorithms.md#8.2 COUNTING-SORT]] assumes input elements are integers in range $0$ to $k$.\nIdea: For each input element $x$, determine count of elements $\\le x$. Use this to place $x$ into its output position. Modification needed for duplicate values.\nAlgorithm uses an array $A[1..n]$ (input), $B[1..n]$ (output), and $C[0..k]$ (temporary storage).\n1. Initialize $C[0..k]$ to zeros. (Lines 2-3)\n2. For each $A[j]$, increment $C[A[j]]$. Now $C[i]$ has count of elements equal to $i$. (Lines 4-5)\n3. For $i=1$ to $k$, set $C[i] = C[i] + C[i-1]$. Now $C[i]$ has count of elements $\\le i$. (Lines 7-8)\n4. Iterate $j$ from $n$ down to $1$. Place $A[j]$ into $B[C[A[j]]]$. Decrement $C[A[j]]$. (Lines 11-13)\nFigure 8.2 of the book illustrates this. Processing $A$ from end to beginning ensures stability.\n\nAnalysis: Lines 2-3 take $\\Theta(k)$. Lines 4-5 take $\\Theta(n)$. Lines 7-8 take $\\Theta(k)$. Lines 11-13 take $\\Theta(n)$. Total time: $\\Theta(n+k)$. If $k=O(n)$, time is $\\Theta(n)$.\nCounting sort is **stable**: elements with same value appear in output in same order as input. Stability is important for radix sort.\n\n## 8.3 Radix sort\n[[PartII Sorting and Order Statistics Algorithms.md#8.3 RADIX-SORT]] sorts numbers by processing digits. Intuitively, one might sort on most significant digit first. Radix sort counterintuitively sorts on least significant digit first.\nAlgorithm: For $d$-digit numbers, sort $d$ times, from digit 1 (least significant) to digit $d$ (most significant), using a stable sort for each digit sort.\nFigure 8.3 of the book shows an example.\n\nCorrectness (Lemma 8.3): Proved by induction on digit $i$ being sorted. Stability of intermediate sort is crucial. If using counting sort (where digits range $0$ to $k-1$), each pass takes $\\Theta(n+k)$. Total time for $d$ passes: $\\Theta(d(n+k))$.\nIf $d$ is constant and $k=O(n)$, radix sort is $\\Theta(n)$.\n\n**Lemma 8.4**: For $n$ $b$-bit numbers, and $r \\le b$, radix sort can treat numbers as having $d = \\lceil b/r \\rceil$ digits of $r$ bits each. Each $r$-bit digit is in range $0$ to $2^r-1$. Using counting sort, time is $\\Theta((b/r)(n+2^r))$.\nChoosing $r$: If $b < \\lg n$, choose $r=b$, time is $O(n)$. If $b \\ge \\lg n$, choose $r \\approx \\lg n$, time is $\\Theta(b n / \\lg n)$. If $b=O(\\lg n)$, then $r=\\lg n$ gives $\\Theta(n)$ running time.\nRadix sort is not always better than $O(n \\lg n)$ comparison sorts due to constant factors and memory (not in-place if counting sort is used).\n\n## 8.4 Bucket sort\n[[PartII Sorting and Order Statistics Algorithms.md#8.4 BUCKET-SORT]] assumes input is drawn from a uniform distribution over $[0,1)$. Average-case time is $O(n)$.\nIdea:\n1. Divide $[0,1)$ into $n$ equal-sized subintervals (buckets).\n2. Distribute $n$ input numbers into buckets.\n3. Sort numbers in each bucket (e.g., with insertion sort).\n4. Concatenate sorted buckets in order.\nFigure 8.4 of the book illustrates this. Assumes $A[i] \\in [0,1)$. Input $A[i]$ goes into bucket $B[\\lfloor n A[i] \\rfloor]$.\n\nAnalysis: Steps 1, 2, 4 take $O(n)$ in worst case. Step 3: Let $n_i$ be number of elements in bucket $B[i]$. Total time $T(n) = \\Theta(n) + \\sum_{i=0}^{n-1} O(n_i^2)$.\nAverage-case: $E[T(n)] = \\Theta(n) + \\sum_{i=0}^{n-1} O(E[n_i^2])$.\nFor uniform input, each $n_i$ follows a binomial distribution $B(n, 1/n)$.\n$E[n_i] = n(1/n) = 1$. $\\text{Var}[n_i] = n(1/n)(1-1/n) = 1 - 1/n$.\n$E[n_i^2] = \\text{Var}[n_i] + (E[n_i])^2 = (1-1/n) + 1^2 = 2 - 1/n$.\nSo $E[T(n)] = \\Theta(n) + n \\cdot O(2-1/n) = \\Theta(n) + O(n) = \\Theta(n)$.\nBucket sort can run in linear time even for non-uniform inputs if $\\sum n_i^2$ is linear in $n$.\nWorst-case is $\\Theta(n^2)$ if all elements fall into one bucket.\n\n## Chapter Notes\nDecision-tree model by Ford and Johnson. Counting sort by Seward (1954). Radix sort use is older, from card-sorting machines. Bucket sort by Isaac and Singleton (1956). Various advanced sorting algorithms exist providing better bounds under specific models or for integer sorting, e.g., fusion trees, Thorup's algorithm, Han's algorithm."
            },
            {
                "file": "PartII Sorting and Order Statistics/C9 Medians and Order Statistics.md",
                "contents": "# 9 Medians and Order Statistics\n\nThe $i$th **order statistic** of a set of $n$ elements is the $i$th smallest element.\n-   The minimum is the 1st order statistic ($i=1$).\n-   The maximum is the $n$th order statistic ($i=n$).\n-   A **median** is the \"halfway point\". If $n$ is odd, the median is unique at $i=(n+1)/2$. If $n$ is even, there are two medians: lower median at $i=n/2$ and upper median at $i=n/2+1$. This text consistently refers to the lower median as \"the median\", i.e., at $i=\\lfloor (n+1)/2 \\rfloor$.\n\nThe **selection problem**: Given a set $A$ of $n$ distinct numbers and an integer $i$ ($1 \\le i \\le n$), find the element $x \\in A$ that is larger than exactly $i-1$ other elements of $A$.\nThis problem can be solved in $O(n \\lg n)$ time by sorting. This chapter presents faster algorithms.\n\n## 9.1 Minimum and maximum\nFinding the minimum (or maximum) of $n$ elements:\n[[PartII Sorting and Order Statistics Algorithms.md#9.1 MINIMUM]] procedure iterates through elements, keeping track of smallest. Requires $n-1$ comparisons.\nThis is optimal: in a tournament model, every element except the winner must lose at least one match (comparison).\n\nSimultaneous minimum and maximum:\nCan be found independently using $(n-1) + (n-1) = 2n-2$ comparisons.\nMore efficiently: process elements in pairs. Compare elements within a pair. Then compare smaller with current min, larger with current max. Cost: 3 comparisons for every 2 elements.\nTotal comparisons: at most $3 \\lfloor n/2 \\rfloor$. If $n$ is odd, initialize min/max to $A[1]$, process $n-1$ elements in pairs. If $n$ is even, compare $A[1], A[2]$ (1 comp), initialize min/max, process $n-2$ elements in pairs.\n-   $n$ odd: $3(n-1)/2 = 3 \\lfloor n/2 \\rfloor$ comparisons.\n-   $n$ even: $1 + 3(n-2)/2 = 1 + 3n/2 - 3 = 3n/2 - 2$ comparisons.\nBoth are $\\le 3 \\lfloor n/2 \\rfloor$.\n\n## 9.2 Selection in expected linear time\nThe [[PartII Sorting and Order Statistics Algorithms.md#9.2 RANDOMIZED-SELECT]] algorithm finds the $i$th smallest element in expected $O(n)$ time. It's modeled after randomized quicksort, but only recurses on one side of the partition.\nAlgorithm `RANDOMIZED-SELECT(A, p, r, i)`:\n1. Base case: If $p=r$, return $A[p]$.\n2. Partition $A[p..r]$ using `RANDOMIZED-PARTITION` around pivot $A[q]$.\n3. Let $k = q-p+1$ be rank of pivot in $A[p..r]$.\n4. If $i=k$, pivot $A[q]$ is the answer.\n5. If $i < k$, recurse on $A[p..q-1]$ to find $i$th smallest.\n6. If $i > k$, recurse on $A[q+1..r]$ to find $(i-k)$th smallest.\nFigure 9.1 of the book illustrates its action.\n\nWorst-case running time: $\\Theta(n^2)$, if unlucky partitions always reduce problem size by 1. Recurrence: $T(n) = T(n-1) + \\Theta(n)$.\nExpected running time:\nIntuition: If pivot is in "
            },
            {
                "file": "PartII Sorting and Order Statistics/C9 Medians and Order Statistics.md",
                "contents": "# 9 Medians and Order Statistics\n\nThe $i$th **order statistic** of a set of $n$ elements is the $i$th smallest element.\n-   The minimum is the 1st order statistic ($i=1$).\n-   The maximum is the $n$th order statistic ($i=n$).\n-   A **median** is the \"halfway point\". If $n$ is odd, the median is unique at $i=(n+1)/2$. If $n$ is even, there are two medians: lower median at $i=n/2$ and upper median at $i=n/2+1$. This text consistently refers to the lower median as \"the median\", i.e., at $i=\\lfloor (n+1)/2 \\rfloor$.\n\nThe **selection problem**: Given a set $A$ of $n$ distinct numbers and an integer $i$ ($1 \\le i \\le n$), find the element $x \\in A$ that is larger than exactly $i-1$ other elements of $A$.\nThis problem can be solved in $O(n \\lg n)$ time by sorting. This chapter presents faster algorithms.\n\n## 9.1 Minimum and maximum\nFinding the minimum (or maximum) of $n$ elements:\n[[PartII Sorting and Order Statistics Algorithms.md#9.1 MINIMUM]] procedure iterates through elements, keeping track of smallest. Requires $n-1$ comparisons.\nThis is optimal: in a tournament model, every element except the winner must lose at least one match (comparison).\n\nSimultaneous minimum and maximum:\nCan be found independently using $(n-1) + (n-1) = 2n-2$ comparisons.\nMore efficiently: process elements in pairs. Compare elements within a pair. Then compare smaller with current min, larger with current max. Cost: 3 comparisons for every 2 elements.\nTotal comparisons: at most $3 \\lfloor n/2 \\rfloor$. If $n$ is odd, initialize min/max to $A[1]$, process $n-1$ elements in pairs. If $n$ is even, compare $A[1], A[2]$ (1 comp), initialize min/max, process $n-2$ elements in pairs.\n-   $n$ odd: $3(n-1)/2 = 3 \\lfloor n/2 \\rfloor$ comparisons.\n-   $n$ even: $1 + 3(n-2)/2 = 1 + 3n/2 - 3 = 3n/2 - 2$ comparisons.\nBoth are $\\le 3 \\lfloor n/2 \\rfloor$.\n\n## 9.2 Selection in expected linear time\nThe [[PartII Sorting and Order Statistics Algorithms.md#9.2 RANDOMIZED-SELECT]] algorithm finds the $i$th smallest element in expected $O(n)$ time. It's modeled after randomized quicksort, but only recurses on one side of the partition.\nAlgorithm `RANDOMIZED-SELECT(A, p, r, i)`:\n1. Base case: If $p=r$, return $A[p]$.\n2. Partition $A[p..r]$ using `RANDOMIZED-PARTITION` around pivot $A[q]$.\n3. Let $k = q-p+1$ be rank of pivot in $A[p..r]$.\n4. If $i=k$, pivot $A[q]$ is the answer.\n5. If $i < k$, recurse on $A[p..q-1]$ to find $i$th smallest.\n6. If $i > k$, recurse on $A[q+1..r]$ to find $(i-k)$th smallest.\nFigure 9.1 of the book illustrates its action.\n\nWorst-case running time: $\\Theta(n^2)$, if unlucky partitions always reduce problem size by 1. Recurrence: $T(n) = T(n-1) + \\Theta(n)$.\nExpected running time:\nIntuition: If pivot is in \"middle half\" (e.g., between 1st and 3rd quartiles), problem size reduces by at least $1/4$, to at most $3/4$. $T(n) = T(3n/4) + \\Theta(n) \\implies T(n) = \\Theta(n)$. A pivot lands in middle half with probability $\\approx 1/2$.\n**Lemma 9.1**: A partitioning is helpful (reduces number of elements in play to $\\le (3/4)$ of original) with probability at least $1/2$. A pivot falling in the \"middle half\" (elements not in smallest $\\lfloor n/4 \\rfloor - 1$ or largest $\\lfloor n/4 \\rfloor - 1$) ensures a helpful partition.\n**Theorem 9.2**: `RANDOMIZED-SELECT` has expected running time $\\Theta(n)$.\nProof sketch: Define $A^{(j)}$ as set of elements in play after $j$ partitions. A partitioning is *helpful* if $|A^{(j)}| \\le (3/4)|A^{(j-1)}|$. Let $X_k$ be number of partitions in $k$th generation (between $k$th and $(k+1)$th helpful partition). $E[X_k] \\le 2$. Total comparisons $\\approx \\sum X_k |A^{(h_k)}| \\le \\sum X_k (3/4)^k n_0$. Expected total comparisons $\\le \\sum E[X_k] (3/4)^k n_0 \\le \\sum 2 (3/4)^k n_0 = 2n_0 \\sum (3/4)^k = 2n_0 \\cdot 4 = 8n_0 = O(n)$.\nSince $\\Omega(n)$ work is done, expected time is $\\Theta(n)$.\n\n## 9.3 Selection in worst-case linear time\nThe [[PartII Sorting and Order Statistics Algorithms.md#9.3 SELECT]] algorithm finds the $i$th smallest element in $O(n)$ worst-case time.\nIdea: Guarantee a good pivot by finding median of medians.\nAlgorithm `SELECT(A, p, r, i)`:\n1. Preprocessing (lines 1-10): Ensure number of elements $(r-p+1)$ is divisible by 5. If not, find minimum, place in $A[p]$. If $i=1$, return $A[p]$. Else, reduce problem to $A[p+1..r]$ for $(i-1)$th element. This takes $O(n)$ time across at most 4 iterations.\n2. Divide $n' = (r-p+1)$ elements (now $n'$ is divisible by 5) into $g = n'/5$ groups of 5 elements each.\n3. Sort each group of 5 elements (e.g., insertion sort). Takes $O(1)$ per group, total $O(g) = O(n)$ for lines 12-14. (Actually, line 13 in prompt seems to have sort for groups (A[j], A[j+g] .. A[j+4g]), which is correct way to visualize the columns of groups.)\n4. Find median $x$ of the $g$ group medians by recursively calling `SELECT` on the array of medians. This is `SELECT(A, p+2g, p+3g-1, \\lfloor g/2 \\rfloor)` (line 16 in prompt, which is `SELECT(A, p + 2g, p + 3g - 1, ceil(g/2))` in some CLRS editions for median of $g$ items). This call is on $\\approx n/5$ elements.\n5. Partition $A[p..r]$ around $x$ using a modified `PARTITION-AROUND` procedure. Let $q$ be index of $x$.\n6. Let $k = q-p+1$. If $i=k$, $x$ is the answer.\n7. If $i < k$, recurse `SELECT(A, p, q-1, i)$.\n8. If $i > k$, recurse `SELECT(A, q+1, r, i-k)$.\nFigure 9.3 of the book illustrates element relationships.\n\nAnalysis of `SELECT`:\nStep 1 (preprocessing) takes $O(n)$. Step 2, 3 take $O(n)$. Step 4 takes $T(n/5)$. Step 5 takes $O(n)$.\nCrucial part: Size of recursive call in step 7 or 8. At least $3(\\lceil g/2 \\rceil - 1)$ group medians are $\\le x$. Each contributes 3 elements $\\le x$ (itself and 2 smaller in its group). So at least $3(\\lceil g/2 \\rceil - 2)$ elements are $\\le x$. This is $\\approx 3(n/10) = 3n/10$. Similarly, at least $3n/10$ elements are $\\ge x$.\nSo, in worst case, recursive call in step 7/8 is on at most $n - 3n/10 = 7n/10$ elements.\nRecurrence: $T(n) \\le T(n/5) + T(7n/10) + O(n)$.\nUsing substitution: Assume $T(n) \\le cn$. $cn \\le c(n/5) + c(7n/10) + dn = c(9n/10) + dn$. This is $cn \\le cn - cn/10 + dn$. So $cn/10 \\ge dn$, which means $c \\ge 10d$. This holds if $c$ is large enough. Base cases $T(n) \\le cn$ for $n < n_0$ (e.g., $n \\le 4$ as per prompt example of line 1's loop or more generally for $n <$ a threshold for recursive base) can be handled by choosing large $c$. Thus $T(n)=O(n)$. Since $\\Omega(n)$ work is done (e.g. line 13 in pseudocode), $T(n) = \\Theta(n)$.\n\n## Chapter Notes\nWorst-case linear-time median finding by Blum, Floyd, Pratt, Rivest, Tarjan (1973). Randomized version by Hoare (1961). Improved randomized version by Floyd and Rivest (1975). Research on exact number of comparisons for median is ongoing."
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartIV Advanced Design and Analysis Techniques Algorithms.md",
                "contents": "# Part IV Advanced Design and Analysis Techniques Algorithms\n\n## C14 Dynamic Programming Algorithms\n\n### C14.1 CUT-ROD\n```\nCUT-ROD(p, n)\n1 if n == 0\n2   return 0\n3 q = -\u221e\n4 for i = 1 to n\n5   q = max(q, p[i] + CUT-ROD(p, n - i))\n6 return q\n```\n\n### C14.1 MEMOIZED-CUT-ROD\n```\nMEMOIZED-CUT-ROD(p, n)\n1 let r[0..n] be a new array\n2 for i = 0 to n\n3   r[i] = -\u221e\n4 return MEMOIZED-CUT-ROD-AUX(p, n, r)\n```\n\n### C14.1 MEMOIZED-CUT-ROD-AUX\n```\nMEMOIZED-CUT-ROD-AUX(p, n, r)\n1 if r[n] \u2265 0\n2   return r[n]\n3 if n == 0\n4   q = 0\n5 else q = -\u221e\n6   for i = 1 to n\n7     q = max(q, p[i] + MEMOIZED-CUT-ROD-AUX(p, n - i, r))\n8 r[n] = q\n9 return q\n```\n\n### C14.1 BOTTOM-UP-CUT-ROD\n```\nBOTTOM-UP-CUT-ROD(p, n)\n1 let r[0..n] be a new array\n2 r[0] = 0\n3 for j = 1 to n\n4   q = -\u221e\n5   for i = 1 to j\n6     q = max(q, p[i] + r[j - i])\n7   r[j] = q\n8 return r[n]\n```\n\n### C14.1 EXTENDED-BOTTOM-UP-CUT-ROD\n```\nEXTENDED-BOTTOM-UP-CUT-ROD(p, n)\n1 let r[0..n] and s[1..n] be new arrays\n2 r[0] = 0\n3 for j = 1 to n\n4   q = -\u221e\n5   for i = 1 to j\n6     if q < p[i] + r[j - i]\n7       q = p[i] + r[j - i]\n8       s[j] = i\n9   r[j] = q\n10 return r and s\n```\n\n### C14.1 PRINT-CUT-ROD-SOLUTION\n```\nPRINT-CUT-ROD-SOLUTION(p, n)\n1 (r, s) = EXTENDED-BOTTOM-UP-CUT-ROD(p, n)\n2 while n > 0\n3   print s[n]\n4   n = n - s[n]\n```\n\n### C14.2 RECTANGULAR-MATRIX-MULTIPLY\n```\nRECTANGULAR-MATRIX-MULTIPLY(A, B, C, p, q, r)\n1 for i = 1 to p\n2   for j = 1 to r\n3     for k = 1 to q\n4       c_ij = c_ij + a_ik * b_kj\n```\n\n### C14.2 MATRIX-CHAIN-ORDER\n```\nMATRIX-CHAIN-ORDER(p, n)\n1 let m[1..n, 1..n] and s[1..n-1, 2..n] be new tables\n2 for i = 1 to n\n3   m[i, i] = 0\n4 for l = 2 to n // l is the chain length\n5   for i = 1 to n - l + 1\n6     j = i + l - 1\n7     m[i, j] = \u221e\n8     for k = i to j - 1\n9       q = m[i, k] + m[k + 1, j] + p_{i-1}*p_k*p_j\n10      if q < m[i, j]\n11        m[i, j] = q\n12        s[i, j] = k\n13 return m and s\n```\n\n### C14.2 PRINT-OPTIMAL-PARENS\n```\nPRINT-OPTIMAL-PARENS(s, i, j)\n1 if i == j\n2   print \"A\"_i\n3 else print \"(\"\n4   PRINT-OPTIMAL-PARENS(s, i, s[i, j])\n5   PRINT-OPTIMAL-PARENS(s, s[i, j] + 1, j)\n6   print \")\"\n```\n\n### C14.3 RECURSIVE-MATRIX-CHAIN\n```\nRECURSIVE-MATRIX-CHAIN(p, i, j)\n1 if i == j\n2   return 0\n3 m[i, j] = \u221e\n4 for k = i to j - 1\n5   q = RECURSIVE-MATRIX-CHAIN(p, i, k) +\n        RECURSIVE-MATRIX-CHAIN(p, k + 1, j) +\n        p_{i-1}*p_k*p_j\n6   if q < m[i, j]\n7     m[i, j] = q\n8 return m[i, j]\n```\n\n### C14.3 MEMOIZED-MATRIX-CHAIN\n```\nMEMOIZED-MATRIX-CHAIN(p, n)\n1 let m[1..n, 1..n] be a new table\n2 for i = 1 to n\n3   for j = i to n\n4     m[i, j] = \u221e\n5 return LOOKUP-CHAIN(m, p, 1, n)\n```\n\n### C14.3 LOOKUP-CHAIN\n```\nLOOKUP-CHAIN(m, p, i, j)\n1 if m[i, j] < \u221e\n2   return m[i, j]\n3 if i == j\n4   m[i, j] = 0\n5 else for k = i to j - 1\n6     q = LOOKUP-CHAIN(m, p, i, k) +\n           LOOKUP-CHAIN(m, p, k + 1, j) + p_{i-1}*p_k*p_j\n7     if q < m[i, j]\n8       m[i, j] = q\n9 return m[i, j]\n```\n\n### C14.4 LCS-LENGTH\n```\nLCS-LENGTH(X, Y, m, n)\n1 let b[1..m, 1..n] and c[0..m, 0..n] be new tables\n2 for i = 1 to m\n3   c[i, 0] = 0\n4 for j = 0 to n\n5   c[0, j] = 0\n6 for i = 1 to m\n7   for j = 1 to n\n8     if x_i == y_j\n9       c[i, j] = c[i - 1, j - 1] + 1\n10      b[i, j] = \"\u2196\"\n11    elseif c[i - 1, j] \u2265 c[i, j - 1]\n12      c[i, j] = c[i - 1, j]\n13      b[i, j] = \"\u2191\"\n14    else c[i, j] = c[i, j - 1]\n15      b[i, j] = \"\u2190\"\n16 return c and b\n```\n\n### C14.4 PRINT-LCS\n```\nPRINT-LCS(b, X, i, j)\n1 if i == 0 or j == 0\n2   return\n3 if b[i, j] == \"\u2196\"\n4   PRINT-LCS(b, X, i - 1, j - 1)\n5   print x_i\n6 elseif b[i, j] == \"\u2191\"\n7   PRINT-LCS(b, X, i - 1, j)\n8 else PRINT-LCS(b, X, i, j - 1)\n```\n\n### C14.5 OPTIMAL-BST\n```\nOPTIMAL-BST(p, q, n)\n1 let e[1..n+1, 0..n], w[1..n+1, 0..n],\n    and root[1..n, 1..n] be new tables\n2 for i = 1 to n + 1\n3   e[i, i - 1] = q_{i-1}\n4   w[i, i - 1] = q_{i-1}\n5 for l = 1 to n\n6   for i = 1 to n - l + 1\n7     j = i + l - 1\n8     e[i, j] = \u221e\n9     w[i, j] = w[i, j - 1] + p_j + q_j\n10    for r = i to j\n11      t = e[i, r - 1] + e[r + 1, j] + w[i, j]\n12      if t < e[i, j]\n13        e[i, j] = t\n14        root[i, j] = r\n15 return e and root\n```\n\n## C15 Greedy Algorithms Algorithms\n\n### C15.1 RECURSIVE-ACTIVITY-SELECTOR\n```\nRECURSIVE-ACTIVITY-SELECTOR(s, f, k, n)\n1 m = k + 1\n2 while m \u2264 n and s[m] < f[k]  // find the first activity in S_k to finish\n3   m = m + 1\n4 if m \u2264 n\n5   return {a_m} \u222a RECURSIVE-ACTIVITY-SELECTOR(s, f, m, n)\n6 else return \u2205\n```\n\n### C15.1 GREEDY-ACTIVITY-SELECTOR\n```\nGREEDY-ACTIVITY-SELECTOR(s, f, n)\n1 A = {a_1}\n2 k = 1\n3 for m = 2 to n\n4   if s[m] \u2265 f[k]    // is a_m in S_k?\n5     A = A \u222a {a_m}   // yes, so choose it\n6     k = m           // and continue from there\n7 return A\n```\n\n### C15.3 HUFFMAN\n```\nHUFFMAN(C)\n1 n = |C|\n2 Q = C\n3 for i = 1 to n - 1\n4   allocate a new node z\n5   x = EXTRACT-MIN(Q)\n6   y = EXTRACT-MIN(Q)\n7   z.left = x\n8   z.right = y\n9   z.freq = x.freq + y.freq\n10  INSERT(Q, z)\n11 return EXTRACT-MIN(Q) // the root of the tree is the only node left\n```\n\n## C16 Amortized Analysis Algorithms\n\n### C16.1 MULTIPOP\n```\nMULTIPOP(S, k)\n1 while not STACK-EMPTY(S) and k > 0\n2   POP(S)\n3   k = k - 1\n```\n\n### C16.1 INCREMENT\n```\nINCREMENT(A, k)\n1 i = 0\n2 while i < k and A[i] == 1\n3   A[i] = 0\n4   i = i + 1\n5 if i < k\n6   A[i] = 1\n```\n\n### C16.4 TABLE-INSERT\n```\nTABLE-INSERT(T, x)\n1 if T.size == 0\n2   allocate T.table with 1 slot\n3   T.size = 1\n4 if T.num == T.size\n5   allocate new-table with 2 * T.size slots\n6   insert all items in T.table into new-table\n7   free T.table\n8   T.table = new-table\n9   T.size = 2 * T.size\n10 insert x into T.table\n11 T.num = T.num + 1\n```\n"
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C14 Dynamic Programming.md",
                "contents": "# 14 Dynamic Programming\n\nDynamic programming, like the divide-and-conquer method, solves problems by combining the solutions to subproblems. Divide-and-conquer algorithms partition the problem into disjoint subproblems, solve them recursively, and then combine their solutions. In contrast, dynamic programming applies when subproblems overlap, meaning subproblems share subsubproblems. A dynamic-programming algorithm solves each subsubproblem just once and saves its answer, avoiding recomputation.\n\nDynamic programming is typically applied to optimization problems. Such problems can have many possible solutions, each with a value, and we wish to find a solution with an optimal (minimum or maximum) value.\n\nThe development of a dynamic-programming algorithm generally follows four steps:\n1. Characterize the structure of an optimal solution.\n2. Recursively define the value of an optimal solution.\n3. Compute the value of an optimal solution, typically in a bottom-up fashion.\n4. Construct an optimal solution from computed information (if needed, step 3 alone gives the value).\n\n## 14.1 Rod cutting\n\nThe rod-cutting problem is: given a rod of length $n$ inches and a table of prices $p_i$ for $i = 1, 2, \text{...} , n$, determine the maximum revenue $r_n$ obtainable by cutting up the rod and selling the pieces. If the price $p_n$ for a rod of length $n$ is large enough, an optimal solution might require no cutting at all.\n\nConsider a rod of length $n$. An optimal solution cuts the rod into $k$ pieces, for some $1 \text{\u2264} k \text{\u2264} n$. Let the lengths be $i_1, i_2, \text{...} , i_k$, where $\\sum_{j=1}^k i_j = n$. The maximum revenue is $r_n = p_{i_1} + p_{i_2} + \text{...} + p_{i_k}$.\n\nIf we make an initial cut of length $i$, the remaining piece has length $n-i$. The optimal revenue $r_n$ can be expressed in terms of optimal revenues from shorter rods:\n$r_n = \text{max} (p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, \text{...} , r_{n-1} + r_1)$.\nThe first argument, $p_n$, corresponds to making no cuts. The other arguments correspond to making an initial cut and then optimally cutting up the two resulting pieces.\n\nThis problem exhibits **optimal substructure**: optimal solutions to a problem incorporate optimal solutions to related subproblems, which may be solved independently.\n\nA simpler way to arrange a recursive structure is to view a decomposition as consisting of a first piece of length $i$ cut off the left-hand end, and then a right-hand remainder of length $n-i$. Only the remainder, not the first piece, may be further divided. This yields the recurrence:\n$r_n = \text{max}_{1 \text{\u2264} i \text{\u2264} n} (p_i + r_{n-i})$, where $r_0 = 0$.\n\nA naive recursive implementation, `CUT-ROD`, as shown in [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 CUT-ROD]], is inefficient because it solves the same subproblems repeatedly. Its running time $T(n)$ is $T(n) = 2^n$.\n\nDynamic programming offers two ways to improve this:\n1.  **Top-down with memoization**: The procedure is written recursively, but modified to save the result of each subproblem. If the subproblem is encountered again, the saved value is returned. The procedure `MEMOIZED-CUT-ROD` and its helper `MEMOIZED-CUT-ROD-AUX` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 MEMOIZED-CUT-ROD]] and [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 MEMOIZED-CUT-ROD-AUX]]) implement this. It initializes an array $r$ to indicate unknown values, then calls the auxiliary procedure.\n2.  **Bottom-up method**: This approach typically depends on some natural notion of the \"size\" of a subproblem, such that solving any particular subproblem depends only on solving \"smaller\" subproblems. Subproblems are sorted by size and solved smallest first. The procedure `BOTTOM-UP-CUT-ROD` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 BOTTOM-UP-CUT-ROD]]) implements this by iteratively computing solutions for $j = 1, 2, \text{...} , n$.\n\nBoth dynamic-programming approaches run in $\\Theta(n^2)$ time for the rod-cutting problem. The bottom-up version often has better constant factors due to less overhead for procedure calls.\n\n### Subproblem graphs\nThe subproblem graph for a problem visualizes subproblem dependencies. It contains one vertex for each distinct subproblem. There's a directed edge from subproblem $x$ to subproblem $y$ if an optimal solution for $x$ directly involves an optimal solution for $y$. For rod cutting with length $n=4$, the graph shows dependencies like node 4 depending on 3, 2, 1, 0. The size of the subproblem graph $G=(V,E)$ helps determine running time. Dynamic programming is often linear in $|V| + |E|$.\n\n### Reconstructing a solution\nTo reconstruct the actual cuts, not just the optimal revenue, we can extend the bottom-up approach. `EXTENDED-BOTTOM-UP-CUT-ROD` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 EXTENDED-BOTTOM-UP-CUT-ROD]]) saves not only the optimal revenue $r_j$ but also $s_j$, the optimal size of the first piece to cut off for a rod of size $j$. The procedure `PRINT-CUT-ROD-SOLUTION` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 PRINT-CUT-ROD-SOLUTION]]) then uses this $s$ table to print the cuts.\n\n## 14.2 Matrix-chain multiplication\n\nGiven a sequence (chain) $\\langle A_1, A_2, \text{...} , A_n \\rangle$ of $n$ matrices, where matrix $A_i$ has dimension $p_{i-1} \\times p_i$, the goal is to fully parenthesize the product $A_1 A_2 \text{...} A_n$ to minimize the number of scalar multiplications. Matrix multiplication is associative, so any parenthesization yields the same product. The standard algorithm for multiplying two matrices $A_{p \\times q}$ and $B_{q \\times r}$ is `RECTANGULAR-MATRIX-MULTIPLY` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.2 RECTANGULAR-MATRIX-MULTIPLY]]), which takes $pqr$ scalar multiplications.\n\nThe number of alternative parenthesizations for $n$ matrices is $P(n)$, which is related to Catalan numbers and is $\\Omega(2^n)$. Brute-force checking is infeasible.\n\nFollowing the four dynamic programming steps:\n\n**Step 1: The structure of an optimal parenthesization**\nLet $A_{i..j}$ denote the matrix resulting from $A_i A_{i+1} \text{...} A_j$. If an optimal parenthesization of $A_{i..j}$ splits the product between $A_k$ and $A_{k+1}$ (for $i \\text{\u2264} k < j$), then the parenthesizations of $A_{i..k}$ and $A_{k+1..j}$ within this optimal parenthesization must themselves be optimal. This is the optimal substructure property.\n\n**Step 2: A recursive solution**\nLet $m[i, j]$ be the minimum number of scalar multiplications needed to compute $A_{i..j}$.\n- If $i = j$, $m[i, i] = 0$ (a single matrix).\n- If $i < j$, $m[i, j] = \text{min}_{i \\text{\u2264} k < j} \\{ m[i, k] + m[k+1, j] + p_{i-1}p_k p_j \\}$.\nWe also define $s[i, j]$ to be a value of $k$ that achieves this minimum.\n\n**Step 3: Computing the optimal costs**\nThere are $\\Theta(n^2)$ distinct subproblems. The `MATRIX-CHAIN-ORDER` procedure (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.2 MATRIX-CHAIN-ORDER]]) computes $m[i, j]$ and $s[i, j]$ bottom-up. It iterates over chain lengths $l = 2, \text{...} , n$. For each length $l$, it computes costs for all chains of that length. This takes $O(n^3)$ time and $\\Theta(n^2)$ space.\n\n**Step 4: Constructing an optimal solution**\nThe `PRINT-OPTIMAL-PARENS` procedure (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.2 PRINT-OPTIMAL-PARENS]]) uses the $s$ table to recursively print the optimal parenthesization.\n\n## 14.3 Elements of dynamic programming\n\nTwo key ingredients for dynamic programming to apply:\n1.  **Optimal substructure**: An optimal solution to the problem contains within it optimal solutions to subproblems.\n    - Common pattern: Make a choice (leaving subproblems), assume the choice is optimal, determine ensuing subproblems, use cut-and-paste to prove subproblem solutions must be optimal.\n    - Subproblem space simplicity vs. generality (e.g., rod cutting vs. matrix-chain).\n    - Variations: number of subproblems used, number of choices for subproblems.\n    - Running time: (number of subproblems overall) $\\times$ (number of choices per subproblem).\n    - Subtleties: Not all problems with apparent substructure have optimal substructure (e.g., unweighted longest simple path). This often relates to whether subproblems are independent (solutions to one don't affect others).\n\n2.  **Overlapping subproblems**: The space of subproblems must be small, and a recursive algorithm solves the same subproblems repeatedly.\n    - Dynamic programming solves each subproblem once and stores its solution.\n    - `RECURSIVE-MATRIX-CHAIN` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.3 RECURSIVE-MATRIX-CHAIN]]) without memoization is exponential. With memoization, it becomes efficient.\n    - **Memoization**: A top-down approach where the recursive procedure is modified to save subproblem results. `MEMOIZED-MATRIX-CHAIN` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.3 MEMOIZED-MATRIX-CHAIN]]) and `LOOKUP-CHAIN` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.3 LOOKUP-CHAIN]]) illustrate this. It runs in $O(n^3)$ time for matrix-chain. Bottom-up generally has better constant factors.\n\n### Reconstructing an optimal solution\nStoring choices (like the $s$ table in matrix-chain) allows efficient reconstruction of the optimal solution structure, often more efficient than re-deriving choices from the cost table.\n\n## 14.4 Longest common subsequence\n\nGiven two sequences $X = \\langle x_1, \text{...} , x_m \\rangle$ and $Y = \\langle y_1, \text{...} , y_n \\rangle$, find a maximum-length common subsequence (LCS) of $X$ and $Y$. A subsequence does not require consecutive elements.\n\n**Step 1: Characterizing a longest common subsequence**\nLet $X_i = \\langle x_1, \text{...} , x_i \\rangle$ be the $i$-th prefix of $X$. Theorem 14.1 (Optimal substructure of an LCS):\nLet $Z = \\langle z_1, \text{...} , z_k \\rangle$ be any LCS of $X$ and $Y$.\n1. If $x_m = y_n$, then $z_k = x_m = y_n$, and $Z_{k-1}$ is an LCS of $X_{m-1}$ and $Y_{n-1}$.\n2. If $x_m \\ne y_n$ and $z_k \\ne x_m$, then $Z$ is an LCS of $X_{m-1}$ and $Y$.\n3. If $x_m \\ne y_n$ and $z_k \\ne y_n$, then $Z$ is an LCS of $X$ and $Y_{n-1}$.\n\n**Step 2: A recursive solution**\nLet $c[i, j]$ be the length of an LCS of $X_i$ and $Y_j$.\n$c[i, j] = \n  \\begin{cases} \n    0 & \\text{if } i=0 \\text{ or } j=0 \\\\ \n    c[i-1, j-1] + 1 & \\text{if } i,j > 0 \\text{ and } x_i = y_j \\\\ \n    \\text{max}(c[i, j-1], c[i-1, j]) & \\text{if } i,j > 0 \\text{ and } x_i \\ne y_j \n  \\end{cases}$\n\n**Step 3: Computing the length of an LCS**\nThe `LCS-LENGTH` procedure (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.4 LCS-LENGTH]]) computes $c[i, j]$ and an auxiliary table $b[i, j]$ (to reconstruct the LCS) in $\\Theta(mn)$ time. It fills tables in row-major order.\n\n**Step 4: Constructing an LCS**\nThe `PRINT-LCS` procedure (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.4 PRINT-LCS]]) uses the $b$ table to trace back the LCS in $O(m+n)$ time. It is possible to reconstruct the LCS without the $b$ table, by re-deriving choices from the $c$ table values.\n\nSpace can be reduced to $\\Theta(\\text{min}(m,n))$ if only the length is needed, by keeping only two rows (or even one row plus $O(1)$ extra space) of the $c$ table.\n\n## 14.5 Optimal binary search trees\n\nGiven a sequence $K = \\langle k_1, \text{...} , k_n \\rangle$ of $n$ distinct keys in sorted order, and probabilities $p_i$ that a search is for key $k_i$, and probabilities $q_i$ that a search is for a value in $(k_i, k_{i+1})$ (represented by dummy key $d_i$), construct a binary search tree (BST) with minimum expected search cost. $d_0$ represents values less than $k_1$, $d_n$ values greater than $k_n$. We have $\\sum p_i + \\sum q_i = 1$.\nThe expected cost of a search in a BST $T$ is: $E[\\text{search cost in } T] = \\sum_{i=1}^n (\text{depth}_T(k_i) + 1)p_i + \\sum_{i=0}^n (\text{depth}_T(d_i) + 1)q_i$. This can be rewritten as $1 + \\sum \text{depth}_T(k_i)p_i + \\sum \text{depth}_T(d_i)q_i$.\n\n**Step 1: The structure of an optimal BST**\nIf an optimal BST $T$ has $k_r$ as its root, its left subtree must be an optimal BST for keys $k_i, \text{...} , k_{r-1}$ and dummy keys $d_{i-1}, \text{...} , d_{r-1}$, and its right subtree must be an optimal BST for keys $k_{r+1}, \text{...} , k_j$ and dummy keys $d_r, \text{...} , d_j$. This is optimal substructure.\n\n**Step 2: A recursive solution**\nLet $e[i, j]$ be the expected cost of searching an optimal BST containing keys $k_i, \text{...} , k_j$ and dummy keys $d_{i-1}, \text{...} , d_j$. Let $w(i, j) = \\sum_{l=i}^j p_l + \\sum_{l=i-1}^j q_l$.\n- If $j = i-1$ (only dummy key $d_{i-1}$), $e[i, i-1] = q_{i-1}$.\n- If $j \\ge i$, choose root $k_r$ ($i \\text{\u2264} r \\text{\u2264} j$):\n  $e[i, j] = p_r + (e[i, r-1] + w(i, r-1)) + (e[r+1, j] + w(r+1, j))$.\n  Since $w(i, j) = w(i, r-1) + p_r + w(r+1, j)$, this simplifies to:\n  $e[i, j] = e[i, r-1] + e[r+1, j] + w(i, j)$.\n  Minimizing over $r$: $e[i, j] = \text{min}_{i \\text{\u2264} r \\text{\u2264} j} \\{e[i, r-1] + e[r+1, j] + w(i, j)\\}$ for $i \\text{\u2264} j$.\n  If $r=i$, $e[i, i-1]$ is cost of left subtree. If $r=j$, $e[j+1, j]$ is cost of right subtree.\nLet $\text{root}[i, j]$ be the index $r$ for which $k_r$ is the root of an optimal BST for keys $k_i, \text{...} , k_j$.\n\n**Step 3: Computing the expected search cost of an optimal BST**\nThe `OPTIMAL-BST` procedure (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.5 OPTIMAL-BST]]) computes $e[i, j]$, $w[i, j]$ and $\text{root}[i, j]$ in a bottom-up manner. It uses tables $e[1..n+1, 0..n]$, $w[1..n+1, 0..n]$, and $\text{root}[1..n, 1..n]$.\n$w[i, i-1] = q_{i-1}$. For $j \\ge i$, $w[i, j] = w[i, j-1] + p_j + q_j$.\nThe algorithm iterates on length $l=1, \text{...} , n$. The running time is $\\Theta(n^3)$ and space is $\\Theta(n^2)$. Knuth showed that $\text{root}[i, j-1] \\text{\u2264} \text{root}[i, j] \\text{\u2264} \text{root}[i+1, j]$, which can optimize this to $\\Theta(n^2)$ time.\n"
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C15 Greedy Algorithms.md",
                "contents": "# 15 Greedy Algorithms\n\nGreedy algorithms solve optimization problems by making a sequence of choices. At each step, a greedy algorithm makes the choice that looks best at the moment, i.e., a locally optimal choice, hoping it leads to a globally optimal solution. Greedy algorithms do not always yield optimal solutions, but they do for many problems. This chapter often builds upon dynamic programming insights, showing how a greedy choice can simplify the problem.\n\n## 15.1 An activity-selection problem\n\nThe problem is to select a maximum-size subset of mutually compatible activities from a set $S = \\{a_1, a_2, \text{...} , a_n\\}$ that require exclusive use of a common resource. Each activity $a_i$ has a start time $s_i$ and a finish time $f_i$, with $0 \\text{\u2264} s_i < f_i < \\infty$. Activities $a_i$ and $a_j$ are compatible if their intervals $[s_i, f_i)$ and $[s_j, f_j)$ do not overlap ($s_i \\ge f_j$ or $s_j \\ge f_i$).\nAssume activities are sorted by monotonically increasing finish times: $f_1 \\text{\u2264} f_2 \\text{\u2264} \text{...} \\text{\u2264} f_n$. (Equation 15.1)\n\n### The optimal substructure of the activity-selection problem\nLet $S_{ij}$ be the set of activities that start after $a_i$ finishes and finish before $a_j$ starts. If an optimal solution $A_{ij}$ for $S_{ij}$ includes activity $a_k$, then it also includes optimal solutions for $S_{ik}$ and $S_{kj}$. The size of the optimal solution is $c[i, j] = c[i, k] + c[k, j] + 1$. If $S_{ij} = \\emptyset$, $c[i, j] = 0$. Otherwise, $c[i, j] = \\text{max}_{a_k \\in S_{ij}} \\{c[i, k] + c[k, j] + 1\\}$. (Equation 15.2)\n\n### Making the greedy choice\nThe greedy choice is to select the activity with the earliest finish time. Intuitively, this leaves the resource available for as many subsequent activities as possible. If activities are sorted by finish times, this is $a_1$.\n\n**Theorem 15.1**: Consider any nonempty subproblem $S_k$ (activities starting after $a_k$ finishes), and let $a_m$ be an activity in $S_k$ with the earliest finish time. Then $a_m$ is included in some maximum-size subset of mutually compatible activities of $S_k$.\nProof: Let $A_k$ be a maximum-size subset of $S_k$, and let $a_j$ be the activity in $A_k$ with the earliest finish time. If $a_j = a_m$, we are done. If $a_j \\ne a_m$, form $A'_k = (A_k - \\{a_j\\}) \\cup \\{a_m\\}$. Since $f_m \\text{\u2264} f_j$ and activities in $A_k - \\{a_j\\}$ are compatible with $a_j$ (and thus start after $f_j$), they must also be compatible with $a_m$. So $A'_k$ is a set of mutually compatible activities, and $|A'_k| = |A_k|$. Thus, $a_m$ is in $A'_k$, a maximum-size compatible subset.\n\nThis theorem implies that we can make the greedy choice, and only one subproblem remains: finding activities that start after the chosen activity finishes.\n\n### A recursive greedy algorithm\n`RECURSIVE-ACTIVITY-SELECTOR(s, f, k, n)` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15.1 RECURSIVE-ACTIVITY-SELECTOR]]) finds the first activity $a_m$ in $S_k$ (activities in $S$ that start after $a_k$ finishes) that is compatible with $a_k$ (i.e., $s_m \\ge f_k$). If such an $a_m$ is found, the solution is $a_m$ plus the solution to $S_m$. The initial call is often with a fictitious activity $a_0$ with $f_0=0$. If activities are sorted by finish time, the algorithm runs in $\\Theta(n)$ time because each activity is examined at most once across all recursive calls.\n\n### An iterative greedy algorithm\n`GREEDY-ACTIVITY-SELECTOR(s, f, n)` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15.1 GREEDY-ACTIVITY-SELECTOR]]) iterates through activities sorted by finish times. It selects $a_1$, then finds the next activity $a_m$ such that $s_m \\ge f_k$ where $a_k$ was the last activity selected. This also runs in $\\Theta(n)$ time if activities are presorted.\n\n## 15.2 Elements of the greedy strategy\n\nGeneral steps for developing a greedy algorithm:\n1. Determine the optimal substructure of the problem.\n2. Develop a recursive solution.\n3. Show that if you make the greedy choice, then only one subproblem remains.\n4. Prove that it is always safe to make the greedy choice.\n5. Develop a recursive algorithm that implements the greedy strategy.\n6. Convert the recursive algorithm to an iterative algorithm.\n\nAlternatively, a more direct approach:\n1. Cast the optimization problem as one where you make a choice and are left with one subproblem.\n2. Prove that there is always an optimal solution to the original problem that makes the greedy choice (greedy choice is safe).\n3. Demonstrate optimal substructure: show that an optimal solution to the subproblem, combined with the greedy choice, yields an optimal solution to the original problem.\n\n### Greedy-choice property\nThis property means a globally optimal solution can be arrived at by making locally optimal choices. A greedy algorithm makes its choice before solving subproblems, differing from dynamic programming which typically solves subproblems first or makes choices dependent on subproblem solutions.\n\n### Optimal substructure\nIf an optimal solution to the problem contains optimal solutions to its subproblems, the problem exhibits optimal substructure. This is key for both greedy and dynamic programming approaches.\n\n### Greedy versus dynamic programming\nBoth exploit optimal substructure. A key difference is how choices are made.\n- **0-1 knapsack problem**: Given $n$ items, each with weight $w_i$ and value $v_i$, and a knapsack of capacity $W$. Choose items to maximize total value without exceeding $W$. Each item is either taken or not. Dynamic programming can solve this.\nA greedy strategy (e.g., highest value per pound) does not work for the 0-1 problem.\n- **Fractional knapsack problem**: Same setup, but fractions of items can be taken. A greedy strategy (take as much as possible of the item with highest value per pound, then next highest, etc.) yields an optimal solution.\nThis illustrates that the greedy choice must be proven safe for the specific problem.\n\n## 15.3 Huffman codes\n\nHuffman codes are used for data compression, providing savings of 20-90%. They use a greedy algorithm to build an optimal way of representing each character as a binary string (codeword).\n- A **fixed-length code** for $n$ characters requires $\\lceil \\lg n \\rceil$ bits per character.\n- A **variable-length code** can do better by assigning short codewords to frequent characters and long codewords to infrequent ones.\n\n### Prefix-free codes\nCodes where no codeword is a prefix of another. Also known as prefix codes. They simplify decoding: the first codeword in an encoded file is unambiguous. An optimal data compression among character codes can always be achieved by a prefix-free code.\nPrefix-free codes can be represented by full binary trees, where leaves are characters, and paths from the root define codewords (0 for left child, 1 for right child). A full binary tree for an alphabet $C$ has $|C|$ leaves and $|C|-1$ internal nodes.\nThe cost of a tree $T$ (number of bits to encode a file) is $B(T) = \\sum_{c \\in C} c.\\text{freq} \\cdot d_T(c)$, where $d_T(c)$ is the depth of character $c$'s leaf (length of its codeword). (Equation 15.4)\n\n### Constructing a Huffman code\nThe `HUFFMAN` algorithm (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15.3 HUFFMAN]]) builds an optimal prefix-free code.\n1. Initialize a min-priority queue $Q$ with characters, keyed by their frequencies.\n2. For $i = 1$ to $n-1$ (where $n = |C|$):\n   a. Allocate a new node $z$.\n   b. $z.\\text{left} = x = \\text{EXTRACT-MIN}(Q)$.\n   c. $z.\\text{right} = y = \\text{EXTRACT-MIN}(Q)$.\n   d. $z.\\text{freq} = x.\\text{freq} + y.\\text{freq}$.\n   e. $\\text{INSERT}(Q, z)$.\n3. Return $\\text{EXTRACT-MIN}(Q)$ (the root of the tree).\nRunning time is $O(n \\lg n)$ if $Q$ is a binary min-heap.\n\n### Correctness of Huffman's algorithm\nRelies on the greedy-choice property and optimal-substructure property.\n\n**Lemma 15.2 (Greedy-choice property)**: Let $C$ be an alphabet, $x, y \\in C$ be two characters with the lowest frequencies. Then there exists an optimal prefix-free code for $C$ in which codewords for $x$ and $y$ have the same length and differ only in the last bit.\nProof idea: Take any optimal tree $T$. Let $a, b$ be sibling leaves of maximum depth. Assume $x.\\text{freq} \\le a.\\text{freq}$ and $y.\\text{freq} \\le b.\\text{freq}$. Exchange $x$ with $a$, and $y$ with $b$. The cost does not increase. The new tree $T''$ is optimal, and $x,y$ are siblings at maximum depth.\n\n**Lemma 15.3 (Optimal-substructure property)**: Let $x, y$ be two characters in $C$ with minimum frequency. Let $C' = (C - \\{x,y\\}) \\cup \\{z\\}$, where $z$ is a new character with $z.\\text{freq} = x.\\text{freq} + y.\\text{freq}$. Let $T'$ be any tree representing an optimal prefix-free code for $C'$. Then the tree $T$, obtained from $T'$ by replacing leaf $z$ with an internal node having $x$ and $y$ as children, represents an optimal prefix-free code for $C$.\nProof idea: $B(T) = B(T') + x.\\text{freq} + y.\\text{freq}$. If $T$ were not optimal for $C$, then an optimal tree $T''$ for $C$ would exist with $B(T'') < B(T)$. By Lemma 15.2, $x,y$ can be siblings in $T''$. Construct $T'''$ from $T''$ by replacing $x,y$ and their parent with leaf $z$. Then $B(T''') < B(T')$, contradicting $T'$ optimality for $C'$.\n\n**Theorem 15.4**: Procedure HUFFMAN produces an optimal prefix-free code. (Follows from Lemmas 15.2 and 15.3).\n\n## 15.4 Offline caching\nThe offline caching problem is to minimize cache misses given a known sequence of $n$ memory requests $b_1, b_2, \text{...} , b_n$ and a cache of size $k$. When a requested block $b_i$ is not in the cache (a miss) and the cache is full, a block must be evicted.\n\nThe **furthest-in-future** strategy evicts the block in the cache whose next access in the request sequence is furthest in the future. If a block will never be accessed again, it's an ideal candidate.\n\n### Optimal substructure of offline caching\nLet $(C, i)$ be the subproblem of processing requests $b_i, \text{...} , b_n$ with current cache configuration $C$. An optimal solution $S$ to $(C,i)$ implies that if $C'$ is the cache after processing $b_i$, then the remainder of $S$ is an optimal solution to $(C', i+1)$.\nLet $\text{miss}(C, i)$ be the minimum cache misses for subproblem $(C, i)$.\n- If $b_i \\in C$ (hit): $\text{miss}(C, i) = \text{miss}(C, i+1)$.\n- If $b_i \\notin C$ (miss):\n  - If $|C| < k$: $\text{miss}(C, i) = 1 + \text{miss}(C \\cup \\{b_i\\}, i+1)$.\n  - If $|C| = k$: $\text{miss}(C, i) = 1 + \\text{min}_{x \\in C} \\{\text{miss}((C - \\{x\\}) \\cup \\{b_i\\}, i+1)\\}$.\n\n**Theorem 15.5 (Greedy-choice property for offline caching)**: Consider subproblem $(C, i)$ where cache $C$ is full ($|C|=k$) and $b_i \\notin C$. Let $z$ be the block in $C$ whose next access is furthest in the future. Then evicting $z$ is part of some optimal solution.\nProof idea: Let $S$ be an optimal solution that evicts block $x \\ne z$. Construct solution $S'$ that evicts $z$. Show that $S'$ performs no more misses than $S$. The proof involves comparing $S$ and $S'$ step-by-step. If $S$ evicts $x$ and $S'$ evicts $z$ at request $b_i$: For subsequent requests until the next request for $x$ or $z$, $S'$ can mimic $S$ or do better. When $x$ is next requested (if before $z$), $S'$ will miss (it evicted $x$), $S$ hits. $S'$ evicts $z$. If $z$ is requested (if before $x$), $S$ misses, $S'$ hits. A detailed argument shows $S'$ is no worse than $S$.\n\nFurthest-in-future is an optimal offline caching algorithm.\n"
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C16 Amortized Analysis.md",
                "contents": "# 16 Amortized Analysis\n\nAmortized analysis averages the time required to perform a sequence of data-structure operations over all operations performed. It guarantees the average performance of each operation in the worst case, without involving probability (unlike average-case analysis). A single operation might be expensive, but the average cost over a sequence is small.\nThree common techniques: aggregate analysis, accounting method, potential method.\n\n## 16.1 Aggregate analysis\n\nIn aggregate analysis, we show that for all $n$, a sequence of $n$ operations takes $T(n)$ worst-case time in total. The amortized cost per operation is then $T(n)/n$. This applies to each operation, even if there are different types.\n\n### Stack operations\nStandard PUSH and POP operations cost $O(1)$. Consider a new operation `MULTIPOP(S, k)` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16.1 MULTIPOP]]) which pops $\\text{min}(s, k)$ items from a stack $S$ of size $s$. Its cost is $\\text{min}(s, k)$.\nA sequence of $n$ PUSH, POP, MULTIPOP operations on an initially empty stack:\n- Worst-case cost of MULTIPOP is $O(n)$. A sequence of $n$ operations could cost $O(n^2)$.\n- Aggregate analysis: An object can be popped (by POP or MULTIPOP) only if it was pushed. Total number of POPs (including those in MULTIPOP) $\\text{\u2264}$ total PUSHs $\\text{\u2264}} n$. Total cost for $n$ operations is $O(n)$.\n- Amortized cost per operation = $O(n)/n = O(1)$.\n\n### Incrementing a binary counter\nConsider a $k$-bit binary counter $A[0..k-1]$ initialized to 0. `INCREMENT(A,k)` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16.1 INCREMENT]]) adds 1 (modulo $2^k$). The cost is the number of bits flipped.\n- Worst-case cost of INCREMENT is $O(k)$ (when all bits flip from 1 to 0, and one from 0 to 1, or all $k$ bits are 1 and it overflows to 0s).\n- Aggregate analysis: For $n$ INCREMENT operations:\n  - $A[0]$ flips $n$ times.\n  - $A[1]$ flips $\\lfloor n/2 \\rfloor$ times.\n  - $A[i]$ flips $\\lfloor n/2^i \\rfloor$ times.\n  Total flips = $\\sum_{i=0}^{k-1} \\lfloor n/2^i \\rfloor < \\sum_{i=0}^{\\infty} n/2^i = n \\sum_{i=0}^{\\infty} (1/2)^i = 2n$.\n- Total cost for $n$ operations is $O(n)$.\n- Amortized cost per operation = $O(n)/n = O(1)$.\n\n## 16.2 The accounting method\n\nAssign different charges (amortized costs $\\hat{c}_i$) to different operations. If $\\hat{c}_i > c_i$ (actual cost), the difference is stored as **credit** on specific objects in the data structure. This credit can pay for later operations where $\\hat{c}_i < c_i$.\nThe total credit $\\sum \\hat{c}_i - \\sum c_i$ must always be non-negative.\n\n### Stack operations\nActual costs: PUSH=1, POP=1, MULTIPOP=$\\text{min}(s,k)$.\nAmortized costs: $\\hat{c}_{PUSH}=2$, $\\hat{c}_{POP}=0$, $\\hat{c}_{MULTIPOP}=0$.\n- PUSH: Pay $2. $1 for actual cost, $1 as credit stored on the pushed item.\n- POP: Pay $0. Use $1 credit from the popped item to pay its actual cost of $1.\n- MULTIPOP: Pay $0. Each item popped uses its $1 credit.\nSince each item on stack has $1 credit, total credit is non-negative. Total amortized cost for $n$ operations is $O(n)$, so total actual cost is $O(n)$.\n\n### Incrementing a binary counter\nCost = number of bits flipped.\nAmortized cost: Setting a bit from 0 to 1 costs $2. $1 for actual flip, $1 as credit on that bit.\nFlipping a bit from 1 to 0 costs $0. Use $1 credit from the bit to pay its actual cost.\n- INCREMENT: The `while` loop flips 1s to 0s (cost 0 each). At most one bit is set from 0 to 1 (cost $2).\n- Amortized cost of INCREMENT $\\text{\u2264}} 2 = O(1)$.\nTotal credit (sum of credits on all 1-bits) is non-negative. Total amortized cost for $n$ operations is $O(n)$.\n\n## 16.3 The potential method\n\nRepresents prepaid work as **potential energy** $\\Phi$ associated with the data structure as a whole. $\\Phi$ maps data structure $D_i$ to a real number $\\Phi(D_i)$.\nAmortized cost $\\hat{c}_i$ of $i$-th operation: $\\hat{c}_i = c_i + \\Phi(D_i) - \\Phi(D_{i-1}) = c_i + \\Delta\\Phi_i$. (Equation 16.2)\nTotal amortized cost for $n$ operations: $\\sum_{i=1}^n \\hat{c}_i = \\sum_{i=1}^n c_i + \\Phi(D_n) - \\Phi(D_0)$. (Equation 16.3)\nIf $\\Phi(D_n) \\ge \\Phi(D_0)$, then total amortized cost is an upper bound on total actual cost. Often, set $\\Phi(D_0)=0$ and show $\\Phi(D_i) \\ge 0$ for all $i$.\n\n### Stack operations\nPotential function $\\Phi(S) = $ number of objects in stack $S$. $\\Phi(D_0)=0$. $\\Phi(D_i) \\ge 0$.\n- PUSH (on stack with $s$ objects): $c_i=1$. $D_{i-1}$ has $s$ objects, $D_i$ has $s+1$. $\\Delta\\Phi_i = (s+1) - s = 1$. $\\hat{c}_i = 1+1=2$.\n- POP (on stack with $s$ objects): $c_i=1$. $D_{i-1}$ has $s$ objects, $D_i$ has $s-1$. $\\Delta\\Phi_i = (s-1) - s = -1$. $\\hat{c}_i = 1+(-1)=0$.\n- MULTIPOP (pops $k'$ objects from stack with $s$ objects): $c_i=k'$. $D_{i-1}$ has $s$ objects, $D_i$ has $s-k'$. $\\Delta\\Phi_i = (s-k') - s = -k'$. $\\hat{c}_i = k' + (-k')=0$.\nAll operations have $O(1)$ amortized cost.\n\n### Incrementing a binary counter\nPotential function $\\Phi(\text{counter}) = b_i = $ number of 1s in counter after $i$-th operation. $\\Phi(D_0)=0$ if counter starts at 0. $\\Phi(D_i) \\ge 0$.\n- $i$-th INCREMENT operation: Resets $t_i$ bits (1s to 0s). Sets at most one bit (0 to 1). Actual cost $c_i \\text{\u2264}} t_i+1$.\n  Number of 1s after operation $b_i \\text{\u2264}} b_{i-1} - t_i + 1$.\n  $\\Delta\\Phi_i = b_i - b_{i-1} \\text{\u2264}} (b_{i-1} - t_i + 1) - b_{i-1} = 1 - t_i$.\n  $\\hat{c}_i = c_i + \\Delta\\Phi_i \\text{\u2264}} (t_i+1) + (1-t_i) = 2$.\nAmortized cost is $O(1)$. If counter starts with $b_0$ ones, after $n$ ops has $b_n$ ones: $\\sum c_i = \\sum \\hat{c}_i - \\Phi(D_n) + \\Phi(D_0) \\text{\u2264}} 2n - b_n + b_0$.\n\n## 16.4 Dynamic tables\n\nTables that expand or contract. Load factor $\\alpha(T) = T.\\text{num} / T.\\text{size}$.\n\n### 16.4.1 Table expansion\n`TABLE-INSERT(T, x)` (see [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16.4 TABLE-INSERT]]): If table is full ($T.\\text{num} == T.\\text{size}$), allocate new table (double size), copy items, free old table. Then insert.\nCost $c_i$: 1 if no expansion. $i$ if $i$-th operation causes expansion (copy $i-1$ items, insert 1).\n- **Aggregate analysis**: $i$-th op causes expansion if $i-1$ is power of 2. $\\sum c_i \\text{\u2264}} n + \\sum_{j=0}^{\\lfloor \\lg n \\rfloor} 2^j < n+2n = 3n$. Amortized cost $\\text{\u2264}} 3$.\n- **Accounting method**: $\\hat{c}_{INSERT}=3$. $1 for actual elementary insertion. $1 as credit on new item (pays for its future move). $1 as credit on an old item (pays for its move).\n- **Potential method**: $\\Phi(T) = 2 \\cdot T.\\text{num} - T.\\text{size}$. (This is for $\\alpha(T) \\ge 1/2$, after expansion $\\alpha=1/2$, so $\\Phi=0$).\n  - No expansion: $c_i=1$. $T.\\text{num}$ increases by 1. $T.\\text{size}$ unchanged. $\\Delta\\Phi_i = 2(T.\\text{num}_i - T.\\text{size}_i/2) - 2(T.\\text{num}_{i-1} - T.\\text{size}_{i-1}/2) = 2((T.\\text{num}_{i-1}+1) - T.\\text{size}/2) - 2(T.\\text{num}_{i-1} - T.\\text{size}/2) = 2$. $\\hat{c}_i = 1+2=3$.\n  - Expansion (table was full, $T.\\text{num}_{i-1} = T.\\text{size}_{i-1} = i-1$): $c_i=i$. $\\Phi(D_{i-1}) = 2(i-1) - (i-1) = i-1$. After expansion and insertion, $T.\\text{num}_i = i$, $T.\\text{size}_i = 2(i-1)$. $\\Phi(D_i) = 2i - 2(i-1) = 2$. $\\Delta\\Phi_i = 2 - (i-1) = 3-i$. $\\hat{c}_i = i + (3-i) = 3$.\n\n### 16.4.2 Table expansion and contraction\nTo avoid high amortized costs when alternating insertions/deletions near boundary: contract when $\\alpha < 1/4$, not $1/2$. Load factor $\\alpha$ stays in $[1/4, 1]$. Immediately after expansion or contraction, $\\alpha = 1/2$.\nPotential function (Equation 16.5):\n$\\Phi(T) = \n  \\begin{cases} \n    2 \\cdot T.\\text{num} - T.\\text{size} & \\text{if } \\alpha(T) \\ge 1/2 \\\\ \n    T.\\text{size}/2 - T.\\text{num} & \\text{if } \\alpha(T) < 1/2 \n  \\end{cases}$\n$\\{\\Phi}(D_0)=0$, $\\{\\Phi}(D_i) \\ge 0$.\n- **Insertion**:\n  - No expansion, $\\alpha_{i-1} \\ge 1/2$: $\\hat{c}_i=3$.\n  - Expansion (triggers at $\\alpha=1$): $\\hat{c}_i=3$.\n  - No expansion, $\\alpha_{i-1} < 1/2$, $\\alpha_i < 1/2$: $c_i=1$. $\\Delta\\Phi_i = (T.\\text{size}/2 - (T.\\text{num}_{i-1}+1)) - (T.\\text{size}/2 - T.\\text{num}_{i-1}) = -1$. $\\hat{c}_i = 1-1=0$.\n  - No expansion, $\\alpha_{i-1} < 1/2$, $\\alpha_i = 1/2$: $c_i=1$. $T.\\text{num}_i = T.\\text{size}/2$. $\\Phi(D_{i-1}) = T.\\text{size}/2 - (T.\\text{size}/2 - 1) = 1$. $\\Phi(D_i)=0$. $\\Delta\\Phi_i = -1$. $\\hat{c}_i = 1-1=0$.\n- **Deletion**:\n  - No contraction, $\\alpha_i \\ge 1/2$: $c_i=1$. $\\Delta\\Phi_i = (2(T.\\text{num}_{i-1}-1) - T.\\text{size}) - (2T.\\text{num}_{i-1} - T.\\text{size}) = -2$. $\\hat{c}_i = 1-2=-1$.\n  - No contraction, $\\alpha_{i-1} < 1/2$: $c_i=1$. $\\Delta\\Phi_i = (T.\\text{size}/2 - (T.\\text{num}_{i-1}-1)) - (T.\\text{size}/2 - T.\\text{num}_{i-1}) = 1$. $\\hat{c}_i = 1+1=2$.\n  - No contraction, $\\alpha_{i-1} = 1/2$, $\\alpha_i < 1/2$: $c_i=1$. $T.\\text{num}_{i-1} = T.\\text{size}/2$. $\\Phi(D_{i-1})=0$. $\\Phi(D_i)=1$. $\\Delta\\Phi_i = 1$. $\\hat{c}_i = 1+1=2$.\n  - Contraction (triggers when $T.\\text{num}_i = T.\\text{size}_{i-1}/4 - 1$, implies $T.\\text{num}_{i-1} = T.\\text{size}_{i-1}/4$): $c_i = T.\\text{num}_{i-1} = T.\\text{size}_{i-1}/4$. $\\Phi(D_{i-1}) = T.\\text{size}_{i-1}/2 - T.\\text{num}_{i-1} = T.\\text{size}_{i-1}/4$. After contraction, $T.\\text{size}_i = T.\\text{size}_{i-1}/2$, $T.\\text{num}_i = T.\\text{num}_{i-1}$. $\\alpha_i = 1/2$. $\\Phi(D_i)=0$. $\\Delta\\Phi_i = 0 - T.\\text{size}_{i-1}/4 = -T.\\text{size}_{i-1}/4$. $\\hat{c}_i = T.\\text{size}_{i-1}/4 - T.\\text{size}_{i-1}/4 = 0$. (Slight adjustment: if item is first deleted, then contraction, $c_i = 1 + (T.num_{i-1}-1)$ if $T.num_{i-1}-1$ items copied. Or $c_i = T.num_{i-1}$ if all items copied before deletion considered. The text says $c_i = \text{num}_{i-1}$ for delete triggering contraction. Then $\text{num}_i = \text{num}_{i-1}-1$. $\text{size}_i = \text{size}_{i-1}/2$. $\text{num}_i = (\text{size}_{i-1}/4) - 1$. $\text{size}_i = \text{num}_i \times 2 + 2$. $\\Phi(D_i)=0$. So $\\hat{c}_i = \text{num}_{i-1} - \text{num}_{i-1} = 0$. The given analysis for deletion leading to contraction yields amortized cost of 1, using $\text{num}_{i-1} = \text{size}_{i-1}/4$ as actual number of items *before* deletion. $c_i = 1 (\text{delete}) + (\text{num}_{i-1}-1)(\text{copy})$. $\\Phi_{i-1} = \text{size}_{i-1}/2 - \text{num}_{i-1} = \text{size}_{i-1}/4$. $\\Phi_i=0$. $\\hat{c}_i = 1 + (\text{num}_{i-1}-1) - \text{size}_{i-1}/4 = 1 + \text{size}_{i-1}/4 -1 - \text{size}_{i-1}/4 = 0$. For this problem, $c_i$ = actual cost of items moved. $num_i$ items in table. $size_i$ slots. $num_{i-1}$ = $size_{i-1}/4$. $\\Phi_{i-1} = size_{i-1}/2 - num_{i-1} = size_{i-1}/4$. Item $i$ deleted, $num_i' = num_{i-1}-1$. Cost of deletion = 1. New table $size_i = size_{i-1}/2$. $num_i = num_i'$. Cost of copying $num_i'$ items = $num_i'$. So $c_i = 1+num_i'$. $\\Phi_i=0$. $\\hat{c}_i = 1+num_i' - size_{i-1}/4 = 1+num_{i-1}-1 - num_{i-1} = 0$. CLRS page 470: cost $c_i$ equals $size_{i-1}/4$ deleting one item and copying $size_{i-1}/4-1$ items. This might mean $num_{i-1}$ is the items *before* deletion, and $num_i$ *after*. If $num_{i-1} = size_{i-1}/4$, and this item is deleted, the remaining $num_{i-1}-1$ items are copied. Cost = $1 + (num_{i-1}-1)$. $\\hat{c}_i = 1 + num_{i-1}-1 - (size_{i-1}/4) = 1+size_{i-1}/4-1-size_{i-1}/4 = 0$. This is correct if $c_i$ includes the cost of copying. If $c_i=1$ is just the basic deletion cost, and copying is handled by potential, then $\\hat{c}_i = 1 + 0 - size_{i-1}/4$. This would not be constant. The text states (p.470) actual cost $c_i$ is $size_{i-1}/4$ (which is $num_{i-1}$), this covers deletion of one item and copying $num_{i-1}-1$ items. And $\\Phi_i=1$. $\\hat{c}_i = c_i + \\Delta\\Phi_i = \text{size}_{i-1}/4 + (1 - \text{size}_{i-1}/4) = 1$.)\n\nAmortized cost of each operation is bounded by a constant. Total actual time for $n$ operations is $O(n)$.\n"
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartIII Data Structures Algorithms.md",
                "contents": "# Part III Data Structures Algorithms\n\n## C10 Elementary Data Structures Algorithms\n\n### STACK-EMPTY(S)\n```\nSTACK-EMPTY(S)\n1 if S.top == 0\n2   return TRUE\n3 else return FALSE\n```\n\n### PUSH(S, x)\n```\nPUSH(S, x)\n1 if S.top == S.size\n2   error \"overflow\"\n3 else S.top = S.top + 1\n4   S[S.top] = x\n```\n\n### POP(S)\n```\nPOP(S)\n1 if STACK-EMPTY(S)\n2   error \"underflow\"\n3 else S.top = S.top - 1\n4   return S[S.top + 1]\n```\n\n### ENQUEUE(Q, x)\n```\nENQUEUE(Q, x)\n1 Q[Q.tail] = x\n2 if Q.tail == Q.size\n3   Q.tail = 1\n4 else Q.tail = Q.tail + 1\n```\n\n### DEQUEUE(Q)\n```\nDEQUEUE(Q)\n1 x = Q[Q.head]\n2 if Q.head == Q.size\n3   Q.head = 1\n4 else Q.head = Q.head + 1\n5 return x\n```\n\n### LIST-SEARCH(L, k)\n```\nLIST-SEARCH(L, k)\n1 x = L.head\n2 while x != NIL and x.key != k\n3   x = x.next\n4 return x\n```\n\n### LIST-PREPEND(L, x)\n```\nLIST-PREPEND(L, x)\n1 x.next = L.head\n2 x.prev = NIL\n3 if L.head != NIL\n4   L.head.prev = x\n5 L.head = x\n```\n\n### LIST-INSERT(x, y)\n```\nLIST-INSERT(x, y)\n1 x.next = y.next\n2 x.prev = y\n3 if y.next != NIL\n4   y.next.prev = x\n5 y.next = x\n```\n\n### LIST-DELETE(L, x)\n```\nLIST-DELETE(L, x)\n1 if x.prev != NIL\n2   x.prev.next = x.next\n3 else L.head = x.next\n4 if x.next != NIL\n5   x.next.prev = x.prev\n```\n\n### LIST-DELETE'(x) (with sentinel)\n```\nLIST-DELETE'(x)\n1 x.prev.next = x.next\n2 x.next.prev = x.prev\n```\n\n### LIST-SEARCH'(L, k) (with sentinel)\n```\nLIST-SEARCH'(L, k)\n1 L.nil.key = k // store the key in the sentinel to guarantee it is in list\n2 x = L.nil.next // start at the head of the list\n3 while x.key != k\n4   x = x.next\n5 if x == L.nil // found k in the sentinel\n6   return NIL // k was not really in the list\n7 else return x // found k in element x\n```\n\n### COMPACT-LIST-SEARCH(key, next, head, n, k)\n```\nCOMPACT-LIST-SEARCH(key, next, head, n, k)\n1 i = head\n2 while i != NIL and key[i] < k\n3   j = RANDOM(1, n)\n4   if key[i] < key[j] and key[j] <= k\n5     i = j\n6   if key[i] == k\n7     return i\n8   i = next[i]\n9 if i == NIL or key[i] > k\n10  return NIL\n11 else return i\n```\n\n### COMPACT-LIST-SEARCH'(key, next, head, n, k, t)\n```\nCOMPACT-LIST-SEARCH'(key, next, head, n, k, t)\n1 i = head\n2 for q = 1 to t\n3   j = RANDOM(1, n)\n4   if key[i] < key[j] and key[j] <= k\n5     i = j\n6   if key[i] == k\n7     return i\n8 while i != NIL and key[i] < k\n9   i = next[i]\n10 if i == NIL or key[i] > k\n11  return NIL\n12 else return i\n```\n\n## C11 Hash Tables Algorithms\n\n### DIRECT-ADDRESS-SEARCH(T, k)\n```\nDIRECT-ADDRESS-SEARCH(T, k)\n1 return T[k]\n```\n\n### DIRECT-ADDRESS-INSERT(T, x)\n```\nDIRECT-ADDRESS-INSERT(T, x)\n1 T[x.key] = x\n```\n\n### DIRECT-ADDRESS-DELETE(T, x)\n```\nDIRECT-ADDRESS-DELETE(T, x)\n1 T[x.key] = NIL\n```\n\n### CHAINED-HASH-INSERT(T, x)\n```\nCHAINED-HASH-INSERT(T, x)\n1 LIST-PREPEND(T[h(x.key)], x)\n```\n\n### CHAINED-HASH-SEARCH(T, k)\n```\nCHAINED-HASH-SEARCH(T, k)\n1 return LIST-SEARCH(T[h(k)], k)\n```\n\n### CHAINED-HASH-DELETE(T, x)\n```\nCHAINED-HASH-DELETE(T, x)\n1 LIST-DELETE(T[h(x.key)], x)\n```\n\n### HASH-INSERT(T, k) (Open Addressing)\n```\nHASH-INSERT(T, k)\n1 i = 0\n2 repeat\n3   q = h(k, i)\n4   if T[q] == NIL\n5     T[q] = k\n6     return q\n7   else i = i + 1\n8 until i == m\n9 error \"hash table overflow\"\n```\n\n### HASH-SEARCH(T, k) (Open Addressing)\n```\nHASH-SEARCH(T, k)\n1 i = 0\n2 repeat\n3   q = h(k, i)\n4   if T[q] == k\n5     return q\n6   i = i + 1\n7 until T[q] == NIL or i == m\n8 return NIL\n```\n\n### LINEAR-PROBING-HASH-DELETE(T, q)\n```\nLINEAR-PROBING-HASH-DELETE(T, q)\n1 while TRUE\n2   T[q] = NIL // make slot q empty\n3   q' = q // starting point for search\n4   repeat\n5     q' = (q' + 1) mod m // next slot number with linear probing\n6     k' = T[q'] // next key to try to move\n7     if k' == NIL\n8       return // return when an empty slot is found\n9   until g(k', q) < g(k', q') // was empty slot q probed before q'?\n10  T[q] = k' // move k' into slot q\n11  q = q' // free up slot q'\n```\n\n### WEE(k, a, b, t, r, m)\n```\nWEE(k, a, b, t, r, m)\n1 u = ceil(t/w)\n2 <k1, k2, ..., ku> = chop(k)\n3 q = b\n4 for i = 1 to u\n5   q = f^(r)_{a+2t}(ki + q)\n6 return q mod m\n```\n\n## C12 Binary Search Trees Algorithms\n\n### INORDER-TREE-WALK(x)\n```\nINORDER-TREE-WALK(x)\n1 if x != NIL\n2   INORDER-TREE-WALK(x.left)\n3   print x.key\n4   INORDER-TREE-WALK(x.right)\n```\n\n### TREE-SEARCH(x, k)\n```\nTREE-SEARCH(x, k)\n1 if x == NIL or k == x.key\n2   return x\n3 if k < x.key\n4   return TREE-SEARCH(x.left, k)\n5 else return TREE-SEARCH(x.right, k)\n```\n\n### ITERATIVE-TREE-SEARCH(x, k)\n```\nITERATIVE-TREE-SEARCH(x, k)\n1 while x != NIL and k != x.key\n2   if k < x.key\n3     x = x.left\n4   else x = x.right\n5 return x\n```\n\n### TREE-MINIMUM(x)\n```\nTREE-MINIMUM(x)\n1 while x.left != NIL\n2   x = x.left\n3 return x\n```\n\n### TREE-MAXIMUM(x)\n```\nTREE-MAXIMUM(x)\n1 while x.right != NIL\n2   x = x.right\n3 return x\n```\n\n### TREE-SUCCESSOR(x)\n```\nTREE-SUCCESSOR(x)\n1 if x.right != NIL\n2   return TREE-MINIMUM(x.right) // leftmost node in right subtree\n3 else // find the lowest ancestor of x whose left child is an ancestor of x\n4   y = x.p\n5   while y != NIL and x == y.right\n6     x = y\n7     y = y.p\n8   return y\n```\n\n### TREE-INSERT(T, z)\n```\nTREE-INSERT(T, z)\n1 x = T.root // node being compared with z\n2 y = NIL // y will be parent of z\n3 while x != NIL // descend until reaching a leaf\n4   y = x\n5   if z.key < x.key\n6     x = x.left\n7   else x = x.right\n8 z.p = y // found the location\u2014insert z with parent y\n9 if y == NIL\n10  T.root = z // tree T was empty\n11 elseif z.key < y.key\n12  y.left = z\n13 else y.right = z\n```\n\n### TRANSPLANT(T, u, v)\n```\nTRANSPLANT(T, u, v)\n1 if u.p == NIL\n2   T.root = v\n3 elseif u == u.p.left\n4   u.p.left = v\n5 else u.p.right = v\n6 if v != NIL\n7   v.p = u.p\n```\n\n### TREE-DELETE(T, z)\n```\nTREE-DELETE(T, z)\n1 if z.left == NIL\n2   TRANSPLANT(T, z, z.right) // replace z by its right child\n3 elseif z.right == NIL\n4   TRANSPLANT(T, z, z.left) // replace z by its left child\n5 else y = TREE-MINIMUM(z.right) // y is z's successor\n6   if y != z.right // is y farther down the tree?\n7     TRANSPLANT(T, y, y.right) // replace y by its right child\n8     y.right = z.right // z's right child becomes\n9     y.right.p = y // y's right child\n10  TRANSPLANT(T, z, y) // replace z by its successor y\n11  y.left = z.left // and give z's left child to y,\n12  y.left.p = y // which had no left child\n```\n\n### PERSISTENT-TREE-INSERT(T, z) (Conceptual, for problem 12-1)\nThis is a high-level concept from the problems, specific pseudocode depends on COPY-NODE implementation. The logic involves copying the path from the root to the insertion point and updating pointers.\n\n## C13 Red-Black Trees Algorithms\n\n### LEFT-ROTATE(T, x)\n```\nLEFT-ROTATE(T, x)\n1 y = x.right\n2 x.right = y.left // turn y\u2019s left subtree into x\u2019s right subtree\n3 if y.left != T.nil // if y\u2019s left subtree is not empty ...\n4   y.left.p = x // ... then x becomes the parent of the subtree\u2019s root\n5 y.p = x.p // x\u2019s parent becomes y\u2019s parent\n6 if x.p == T.nil // if x was the root ...\n7   T.root = y // ... then y becomes the root\n8 elseif x == x.p.left // otherwise, if x was a left child ...\n9   x.p.left = y // ... then y becomes a left child\n10 else x.p.right = y // otherwise, x was a right child, and now y is\n11 y.left = x // make x become y\u2019s left child\n12 x.p = y\n```\n\n### RB-INSERT(T, z)\n```\nRB-INSERT(T, z)\n1 x = T.root // node being compared with z\n2 y = T.nil // y will be parent of z\n3 while x != T.nil // descend until reaching the sentinel\n4   y = x\n5   if z.key < x.key\n6     x = x.left\n7   else x = x.right\n8 z.p = y // found the location\u2014insert z with parent y\n9 if y == T.nil\n10  T.root = z // tree T was empty\n11 elseif z.key < y.key\n12  y.left = z\n13 else y.right = z\n14 z.left = T.nil // both of z\u2019s children are the sentinel\n15 z.right = T.nil\n16 z.color = RED // the new node starts out red\n17 RB-INSERT-FIXUP(T, z) // correct any violations of red-black properties\n```\n\n### RB-INSERT-FIXUP(T, z)\n```\nRB-INSERT-FIXUP(T, z)\n1 while z.p.color == RED\n2   if z.p == z.p.p.left // is z\u2019s parent a left child?\n3     y = z.p.p.right // y is z\u2019s uncle\n4     if y.color == RED // are z\u2019s parent and uncle both red? (Case 1)\n5       z.p.color = BLACK\n6       y.color = BLACK\n7       z.p.p.color = RED\n8       z = z.p.p\n9     else \n10      if z == z.p.right // (Case 2)\n11        z = z.p\n12        LEFT-ROTATE(T, z)\n13      z.p.color = BLACK // (Case 3)\n14      z.p.p.color = RED\n15      RIGHT-ROTATE(T, z.p.p)\n16  else // same as lines 3-15, but with \"right\" and \"left\" exchanged\n17    y = z.p.p.left\n18    if y.color == RED\n19      z.p.color = BLACK\n20      y.color = BLACK\n21      z.p.p.color = RED\n22      z = z.p.p\n23    else\n24      if z == z.p.left\n25        z = z.p\n26        RIGHT-ROTATE(T, z)\n27      z.p.color = BLACK\n28      z.p.p.color = RED\n29      LEFT-ROTATE(T, z.p.p)\n30 T.root.color = BLACK\n```\n\n### RB-TRANSPLANT(T, u, v)\n```\nRB-TRANSPLANT(T, u, v)\n1 if u.p == T.nil\n2   T.root = v\n3 elseif u == u.p.left\n4   u.p.left = v\n5 else u.p.right = v\n6 v.p = u.p\n```\n\n### RB-DELETE(T, z)\n```\nRB-DELETE(T, z)\n1 y = z\n2 y-original-color = y.color\n3 if z.left == T.nil\n4   x = z.right\n5   RB-TRANSPLANT(T, z, z.right)\n6 elseif z.right == T.nil\n7   x = z.left\n8   RB-TRANSPLANT(T, z, z.left)\n9 else y = TREE-MINIMUM(z.right)\n10  y-original-color = y.color\n11  x = y.right\n12  if y != z.right \n13    RB-TRANSPLANT(T, y, y.right)\n14    y.right = z.right\n15    y.right.p = y\n16  else x.p = y \n17  RB-TRANSPLANT(T, z, y)\n18  y.left = z.left\n19  y.left.p = y\n20  y.color = z.color\n21 if y-original-color == BLACK\n22  RB-DELETE-FIXUP(T, x)\n```\n\n### RB-DELETE-FIXUP(T, x)\n```\nRB-DELETE-FIXUP(T, x)\n1 while x != T.root and x.color == BLACK\n2   if x == x.p.left\n3     w = x.p.right\n4     if w.color == RED // Case 1\n5       w.color = BLACK\n6       x.p.color = RED\n7       LEFT-ROTATE(T, x.p)\n8       w = x.p.right\n9     if w.left.color == BLACK and w.right.color == BLACK // Case 2\n10      w.color = RED\n11      x = x.p\n12    else \n13      if w.right.color == BLACK // Case 3\n14        w.left.color = BLACK\n15        w.color = RED\n16        RIGHT-ROTATE(T, w)\n17        w = x.p.right\n18      w.color = x.p.color // Case 4\n19      x.p.color = BLACK\n20      w.right.color = BLACK\n21      LEFT-ROTATE(T, x.p)\n22      x = T.root\n23  else // same as lines 3-22, but with \"right\" and \"left\" exchanged\n24    w = x.p.left\n25    if w.color == RED\n26      w.color = BLACK\n27      x.p.color = RED\n28      RIGHT-ROTATE(T, x.p)\n29      w = x.p.left\n30    if w.right.color == BLACK and w.left.color == BLACK\n31      w.color = RED\n32      x = x.p\n33    else\n34      if w.left.color == BLACK\n35        w.right.color = BLACK\n36        w.color = RED\n37        LEFT-ROTATE(T, w)\n38        w = x.p.left\n39      w.color = x.p.color\n40      x.p.color = BLACK\n41      w.left.color = BLACK\n42      RIGHT-ROTATE(T, x.p)\n43      x = T.root\n44 x.color = BLACK\n```"
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": "# 10 Elementary Data Structures\n\nSets are fundamental to computer science. Mathematical sets are unchanging, but sets manipulated by algorithms can grow, shrink, or change over time; these are called **dynamic sets**. This part presents basic techniques for representing finite dynamic sets and manipulating them on a computer.\n\nAlgorithms may require several types of operations on sets. Many algorithms need to insert elements, delete elements, and test membership. A dynamic set supporting these operations is a **dictionary**. Other algorithms require more complex operations, like min-priority queues which support inserting an element and extracting the smallest element.\n\n## Elements of a dynamic set\nEach element is typically represented by an object whose attributes can be examined and manipulated. Some dynamic sets assume one attribute is an identifying **key**. If keys are distinct, the set can be viewed as a set of key values. Objects may also contain **satellite data**, carried with the object but not used by set implementation. Attributes may contain data or pointers to other objects in the set.\nSome dynamic sets presuppose keys are from a totally ordered set (e.g., real numbers, words under alphabetic order), allowing definition of minimum/maximum elements or the next larger/smaller element.\n\n## Operations on dynamic sets\nOperations can be grouped into: **queries** (return information) and **modifying operations** (change the set).\nTypical operations include:\n- `SEARCH(S, k)`: Returns a pointer `x` to an element in set `S` such that `x.key == k`, or `NIL`.\n- `INSERT(S, x)`: Adds element `x` to set `S`. Assumes attributes in `x` needed by implementation are initialized.\n- `DELETE(S, x)`: Removes element `x` from set `S`. Takes a pointer to an element, not a key value.\n- `MINIMUM(S)` and `MAXIMUM(S)`: Return pointer to element in totally ordered set `S` with smallest/largest key.\n- `SUCCESSOR(S, x)`: Given element `x` from totally ordered set `S`, returns pointer to next larger element in `S`, or `NIL` if `x` is maximum.\n- `PREDECESSOR(S, x)`: Given element `x` from totally ordered set `S`, returns pointer to next smaller element in `S`, or `NIL` if `x` is minimum.\n\nTime for set operations is usually measured in terms of set size `n`. Implementing dynamic sets with arrays can be simple but inefficient for many operations (e.g., $\\Theta(n)$ worst-case). The data structures in this part improve on array implementations.\n\n## Overview of Part III\n- Chapter 10: Essentials of simple data structures like arrays, matrices, stacks, queues, linked lists, rooted trees.\n- Chapter 11: Hash tables, supporting dictionary operations. Expected $O(1)$ time for operations.\n- Chapter 12: Binary search trees, supporting all dynamic-set operations. Worst-case $\\Theta(n)$ time.\n- Chapter 13: Red-black trees, a variant of binary search trees. Guaranteed $O(\\lg n)$ worst-case time.\n\n## 10.1 Simple array-based data structures: arrays, matrices, stacks, queues\n\nThis chapter examines representation of dynamic sets by simple data structures using pointers, focusing on arrays, matrices, stacks, queues, linked lists, and rooted trees.\n\n### 10.1.1 Arrays\nAn array is stored as a contiguous sequence of bytes in memory. If the first element (index `s`) is at memory address `a`, and each element occupies `b` bytes, the $i$-th element occupies bytes $a+b(i-s)$ through $a+b(i-s+1)-1$. For 1-origin indexing ($s=1$), this is $a+b(i-1)$ to $a+bi-1$. For 0-origin indexing ($s=0$), $a+bi$ to $a+b(i+1)-1$. Accessing any array element takes constant time in the RAM model.\nMost languages require elements to be the same size. If elements vary in size, array elements are usually pointers to objects, and pointer sizes are constant.\n\n### 10.1.2 Matrices\nA matrix (2D array) is typically represented by one or more 1D arrays. Common storage methods are row-major and column-major order.\nFor an $m \\times n$ matrix in row-major order, rows are stored consecutively. In column-major order, columns are stored consecutively. Consider $M = \\begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\end{pmatrix}$. Row-major: `1 2 3 4 5 6`. Column-major: `1 4 2 5 3 6`.\nIf rows, columns, and the array are $s$-indexed, element $M[i,j]$ is at index $s + n(i-s) + (j-s)$ in row-major and $s + m(j-s) + (i-s)$ in column-major. For 0-origin, indices are $ni+j$ (row-major) and $i+mj$ (column-major).\nMultiple-array strategies store each row (or column) in its own array, with another array pointing to these row/column arrays. This allows for "
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": "ragged arrays"
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": ".\nBlock representation divides the matrix into blocks, storing each block contiguously.\n\n### 10.1.3 Stacks and queues\nDynamic sets where the element removed by `DELETE` is prespecified.\n\n**Stacks**\nA stack implements a last-in, first-out (**LIFO**) policy. `INSERT` is often `PUSH`, and `DELETE` (no element argument) is `POP`.\nAn array `S[1..n]` can implement a stack of at most `n` elements. Attribute `S.top` indexes the most recently inserted element. `S.size` is array size. Stack elements are `S[1..S.top]`. `S[1]` is bottom, `S[S.top]` is top.\n- `STACK-EMPTY(S)`: [[PartIII Data Structures Algorithms.md#C10 STACK-EMPTY]] returns `TRUE` if `S.top == 0`.\n- If `S.top == 0`, stack is empty. Popping an empty stack causes **underflow**.\n- If `S.top` exceeds `S.size`, stack **overflows**.\n- `PUSH(S, x)`: [[PartIII Data Structures Algorithms.md#C10 PUSH]] increments `S.top`, stores `x`.\n- `POP(S)`: [[PartIII Data Structures Algorithms.md#C10 POP]] decrements `S.top`, returns element.\nAll three operations take $O(1)$ time.\n\n**Queues**\nA queue implements a first-in, first-out (**FIFO**) policy. `INSERT` is `ENQUEUE`, `DELETE` is `DEQUEUE`.\nA queue has a **head** and a **tail**. Elements are enqueued at the tail and dequeued from the head.\nAn array `Q[1..n]` can implement a queue of at most $n-1$ elements. `Q.size` is array size. `Q.head` indexes the head. `Q.tail` indexes the next location for a new element.\nElements are in `Q[Q.head], Q[Q.head+1], ..., Q[Q.tail-1]`, with "
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": "wrap around"
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": " (location 1 follows `n`).\n- If `Q.head == Q.tail`, queue is empty. Initially `Q.head = Q.tail = 1`.\n- Dequeuing from empty queue causes **underflow**.\n- If `Q.head == Q.tail + 1` (or `Q.head==1` and `Q.tail==Q.size`), queue is full, causing **overflow** on enqueue.\n- `ENQUEUE(Q, x)`: [[PartIII Data Structures Algorithms.md#C10 ENQUEUE]]. Stores `x` at `Q[Q.tail]`, updates `Q.tail` with wrap-around.\n- `DEQUEUE(Q)`: [[PartIII Data Structures Algorithms.md#C10 DEQUEUE]]. Retrieves `x` from `Q[Q.head]`, updates `Q.head` with wrap-around.\nBoth operations take $O(1)$ time (error checking omitted in basic pseudocode).\n\n## 10.2 Linked lists\nA **linked list** is a data structure where objects are in linear order, determined by a pointer in each object, not array indices. Often called search lists if they contain keys for searching.\nEach element of a **doubly linked list** `L` is an object with `key`, `next` pointer, and `prev` pointer. `L.head` points to the first element.\n- If `x.prev == NIL`, `x` is the head.\n- If `x.next == NIL`, `x` is the tail.\n- If `L.head == NIL`, list is empty.\nList forms: singly linked (only `next` pointer), doubly linked, sorted, unsorted, circular (head's `prev` points to tail, tail's `next` points to head).\nThis section assumes unsorted, doubly linked lists.\n\n### Searching a linked list\n`LIST-SEARCH(L, k)` ([[PartIII Data Structures Algorithms.md#C10 LIST-SEARCH]]) finds the first element with key `k` by linear search. Takes $\\Theta(n)$ worst-case time for a list of `n` objects.\n\n### Inserting into a linked list\n`LIST-PREPEND(L, x)` ([[PartIII Data Structures Algorithms.md#C10 LIST-PREPEND]]) adds element `x` to the front. Takes $O(1)$ time.\n`LIST-INSERT(x, y)` ([[PartIII Data Structures Algorithms.md#C10 LIST-INSERT]]) "
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": "splices"
            },
            {
                "file": "PartIII Data Structures/C10 Elementary Data Structures.md",
                "contents": " element `x` into the list immediately following element `y`. Takes $O(1)$ time. Does not reference list object `L`.\n\n### Deleting from a linked list\n`LIST-DELETE(L, x)` ([[PartIII Data Structures Algorithms.md#C10 LIST-DELETE]]) removes element `x` (given pointer to `x`) by updating pointers. Takes $O(1)$ time. If deleting by key, `LIST-SEARCH` is called first, making it $\\Theta(n)$ worst-case.\nInsertion/deletion are faster on doubly linked lists ($O(1)$) than arrays ($\\Theta(n)$ for first element operations). Finding $k$-th element is $O(1)$ in array, $\\Theta(k)$ in list.\n\n### Sentinels\nA **sentinel** is a dummy object simplifying boundary conditions. For a linked list `L`, `L.nil` represents `NIL` but has attributes of other objects. This turns a regular doubly linked list into a **circular, doubly linked list with a sentinel**.\n`L.nil` lies between head and tail. `L.nil.next` points to head, `L.nil.prev` points to tail. Head's `prev` and tail's `next` point to `L.nil`. `L.head` attribute is eliminated, replaced by `L.nil.next`.\nAn empty list consists of just `L.nil`, with `L.nil.next` and `L.nil.prev` pointing to `L.nil`.\n`LIST-DELETE'(x)` ([[PartIII Data Structures Algorithms.md#C10 LIST-DELETE']]) simplifies deletion. `LIST-INSERT'(x,y)` (similar to `LIST-INSERT` but potentially using `L.nil` for `y` to prepend/append) simplifies insertion.\n`LIST-SEARCH'(L,k)` ([[PartIII Data Structures Algorithms.md#C10 LIST-SEARCH']]) uses the sentinel to hold the key `k`, guaranteeing the key is found, simplifying the loop. This can decrease constant factor for search time.\nSentinels simplify code, may speed up by constant factor, but don't typically improve asymptotic running time. Use judiciously due to extra storage.\n\n## 10.3 Representing rooted trees\nLinked data structures for rooted trees.\n\n### Binary trees\nEach node object has `key`, `p` (parent), `left` (left child), `right` (right child) attributes. `T.root` points to the tree root. If `x.p == NIL`, `x` is root. If child is missing, attribute is `NIL`. If `T.root == NIL`, tree is empty.\n\n### Rooted trees with unbounded branching\nFor nodes with arbitrary number of children, the **left-child, right-sibling representation** uses $O(n)$ space for an $n$-node tree.\nEach node `x` has `p` (parent), `T.root` points to root.\nNode `x` has two pointers:\n1. `x.left-child` points to the leftmost child of `x`.\n2. `x.right-sibling` points to the sibling of `x` immediately to its right.\nIf `x` has no children, `x.left-child = NIL`. If `x` is rightmost child, `x.right-sibling = NIL`.\n\n### Other tree representations\nHeaps (Chapter 6) use a single array. Some applications only need parent pointers (traversing towards root).\n\n## Problems\nProblem 10-3 describes searching a sorted compact list using `COMPACT-LIST-SEARCH` ([[PartIII Data Structures Algorithms.md#C10 COMPACT-LIST-SEARCH]]) and `COMPACT-LIST-SEARCH'` ([[PartIII Data Structures Algorithms.md#C10 COMPACT-LIST-SEARCH']]). A compact list is stored in positions 1 to `n` of key/next arrays. The randomized algorithm is shown to search in $O(\\sqrt{n})$ expected time."
            },
            {
                "file": "PartIII Data Structures/C11 Hash Tables.md",
                "contents": "# 11 Hash Tables\n\nMany applications require a dynamic set supporting only dictionary operations: `INSERT`, `SEARCH`, `DELETE`. A hash table is an effective data structure for dictionaries. Average time for search is $O(1)$ under reasonable assumptions, though worst-case can be $\\Theta(n)$.\n\n## 11.1 Direct-address tables\nDirect addressing works well when the universe $U$ of keys is reasonably small. Each element has a distinct key from $U = \\{0, 1, ..., m-1\\}$.\nA **direct-address table** is an array $T[0..m-1]$. Each position, or **slot**, corresponds to a key in $U$. Slot $k$ points to an element with key $k$. If no such element, $T[k] = \\text{NIL}$.\nOperations:\n- `DIRECT-ADDRESS-SEARCH(T, k)`: [[PartIII Data Structures Algorithms.md#C11 DIRECT-ADDRESS-SEARCH]] Returns $T[k]$. $O(1)$ time.\n- `DIRECT-ADDRESS-INSERT(T, x)`: [[PartIII Data Structures Algorithms.md#C11 DIRECT-ADDRESS-INSERT]] Sets $T[x.\\text{key}] = x$. $O(1)$ time.\n- `DIRECT-ADDRESS-DELETE(T, x)`: [[PartIII Data Structures Algorithms.md#C11 DIRECT-ADDRESS-DELETE]] Sets $T[x.\\text{key}] = \\text{NIL}$. $O(1)$ time.\nFor some applications, the table itself can hold elements, not just pointers. Special key can indicate empty slot, or index itself is the key.\n\n## 11.2 Hash tables\nWhen universe $U$ is large, direct addressing is impractical. If set $K$ of keys stored is much smaller than $U$, a hash table uses storage $\\Theta(|K|)$ while maintaining $O(1)$ average search time (worst-case is $\\Theta(n)$).\nA **hash function** $h$ maps $U$ to slots of hash table $T[0..m-1]$: $h: U \\to \\{0, 1, ..., m-1\\}$. Size $m$ is typically much less than $|U|$.\nElement with key $k$ hashes to slot $h(k)$; $h(k)$ is the **hash value** of key $k$.\nWhen two keys hash to the same slot, this is a **collision**.\nIdeal solution is to avoid collisions, but since $|U| > m$, this is impossible (pigeonhole principle).\n\n### Independent uniform hashing\nAn ideal hash function $h$ would have $h(k)$ be a randomly and independently chosen element from $\\{0, ..., m-1\\}$ for each $k \n \text{in} U$. Once $h(k)$ is chosen, subsequent calls with $k$ yield the same $h(k)$. This is an **independent uniform hash function** (or random oracle). This is a theoretical ideal.\n\n### Collision resolution by chaining\nEach slot $T[j]$ points to a linked list of all keys whose hash value is $j$. If no such elements, $T[j]$ is NIL.\nOperations:\n- `CHAINED-HASH-INSERT(T, x)`: [[PartIII Data Structures Algorithms.md#C11 CHAINED-HASH-INSERT]] Prepends $x$ to list $T[h(x.\\text{key})]$. $O(1)$ worst-case (assumes element not already present).\n- `CHAINED-HASH-SEARCH(T, k)`: [[PartIII Data Structures Algorithms.md#C11 CHAINED-HASH-SEARCH]] Searches list $T[h(k)]$. Worst-case proportional to list length.\n- `CHAINED-HASH-DELETE(T, x)`: [[PartIII Data Structures Algorithms.md#C11 CHAINED-HASH-DELETE]] Deletes $x$ from list $T[h(x.\\text{key})]$. $O(1)$ time if lists are doubly linked and $x$ is given.\n\n### Analysis of hashing with chaining\n**Load factor** $\\alpha = n/m$ is the average number of elements stored per chain, where $n$ is total elements, $m$ is slots.\nWorst-case: All $n$ keys hash to one slot. Search is $\\Theta(n)$ plus hash computation time.\nAverage-case (assuming independent uniform hashing):\nLet $n_j$ be length of list $T[j]$. $E[n_j] = \\alpha$.\nAssume $O(1)$ to compute $h(k)$.\n**Theorem 11.1 (Unsuccessful search)**: An unsuccessful search takes $\\Theta(1+\\alpha)$ time on average.\n*Proof*: Any key $k$ not in table is equally likely to hash to any of $m$ slots. Expected time to search is expected length of list $T[h(k)]$, which is $E[n_{h(k)}] = \\alpha$. Total time is $\\Theta(1+\\alpha)$.\n**Theorem 11.2 (Successful search)**: A successful search takes $\\Theta(1+\\alpha)$ time on average.\n*Proof*: Assume element searched for is equally likely to be any of $n$ stored elements. Number of elements examined is 1 + number of elements before $x$ in its list. New elements are placed at front. Expected number of elements examined is $1 + \\alpha/2 - \\alpha/(2n)$. Total time is $\\Theta(1+\\alpha)$.\nIf $n=O(m)$, then $\\alpha = O(1)$, so all dictionary operations take $O(1)$ average time.\n\n## 11.3 Hash functions\nA good hash function should be efficiently computable and approximate independent uniform hashing.\n\n### Keys are integers, vectors, or strings\nTypically, keys are short non-negative integers or short vectors of non-negative integers (e.g., byte strings).\n\n### 11.3.1 Static hashing\nUses a single, fixed hash function. Randomization from input key distribution.\n\n**The division method**\n$h(k) = k \\pmod m$. Fast. Works well if $m$ is prime not too close to a power of 2.\n\n**The multiplication method**\n$h(k) = \\lfloor m (kA \\pmod 1) \\rfloor$, where $0 < A < 1$. $kA \\pmod 1$ is fractional part of $kA$. $m$ choice is not critical.\n\n**The multiply-shift method**\nA practical variant for $m=2^\\ell$, $w$-bit words. $h_a(k) = (ka \\pmod{2^w}) \\gg (w-\\ell)$ for fixed $w$-bit $a$. `ka mod 2^w` gives low-order $w$ bits of product. $\\gg$ is logical right shift. Effectively takes $\\ell$ most significant bits of the low-order $w$-bit word $r_0$ of $ka$. Works well if $a$ is a randomly chosen odd integer.\n\n### 11.3.2 Random hashing\nChoose hash function randomly from a family of functions, independent of keys. **Universal hashing** is a special case.\nA family $H$ of hash functions is **universal** if for each pair of distinct keys $k_1, k_2 \\in U$, the number of functions $h \\in H$ for which $h(k_1) = h(k_2)$ is at most $|H|/m$. The chance of collision between $k_1, k_2$ with a randomly chosen $h$ from $H$ is $\\le 1/m$.\n**Corollary 11.3**: Using universal hashing and chaining, any sequence of $s$ operations with $n=O(m)$ insertions takes $\\Theta(s)$ expected time.\n\n### 11.3.3 Achievable properties of random hashing\n- **Uniform**: For any key $k$, slot $q$, $P(h(k)=q) = 1/m$.\n- **Universal**: For distinct $k_1, k_2$, $P(h(k_1)=h(k_2)) \\le 1/m$.\n- **c-universal**: For distinct $k_1, k_2$, $P(h(k_1)=h(k_2)) \\le c/m$.\n- **d-independent**: For distinct $k_1, ..., k_d$ and any slots $q_1, ..., q_d$, $P(h(k_i)=q_i \\text{ for all } i) = 1/m^d$.\n\n### 11.3.4 Designing a universal family of hash functions\n**Number theory based**\nChoose prime $p$ large enough for all keys ($0 \\le k < p$). $p > m$. Let $Z_p = \\{0, ..., p-1\\}$, $Z_p^* = \\{1, ..., p-1\\}$.\nFor $a \\in Z_p^*$ and $b \\in Z_p$, define $h_{ab}(k) = ((ak+b) \\pmod p) \\pmod m$.\nThe family $H_{pm} = \\{h_{ab} : a \\in Z_p^*, b \\in Z_p\\}$ has $p(p-1)$ functions.\n**Theorem 11.4**: $H_{pm}$ is universal.\n*Proof idea*: For distinct $k_1, k_2$, they map to distinct $r_1, r_2 \\pmod p$. There's a 1-to-1 correspondence between $(a,b)$ and $(r_1,r_2)$. The probability $r_1 \\equiv r_2 \\pmod m$ for random distinct $r_1,r_2 \\pmod p$ is $\\le (\\lceil p/m \\rceil - 1)/(p-1)$ which is $\\approx 1/m$.\n\n**Multiply-shift based (2/m-universal)**\nUsing $h_a(k) = (ka \\pmod{2^w}) \\gg (w-\\ell)$ from section 11.3.1 (where $m=2^\\ell$).\nThe family $H = \\{h_a : a \\text{ is odd, } 1 \\le a < 2^w\\}$ (Note: text OCR says $1 \\le a < m$, but should be $1 \\le a < 2^w$ for $w$-bit multiplier $a$ as described in multiply-shift)\n**Theorem 11.5**: This family is $2/m$-universal.\n\n### 11.3.5 Hashing long inputs such as vectors or strings\n**Number-theoretic approaches**: Extend ideas from 11.3.4 (e.g., polynomial hashing).\n**Cryptographic hashing**: Use cryptographic hash functions like SHA-256. $h(k) = \\text{SHA-256}(k) \\pmod m$. A family can be defined by $h_a(k) = \\text{SHA-256}(a||k) \\pmod m$ where $a$ is a salt.\n\n## 11.4 Open addressing\nAll elements stored in hash table itself. No storage outside table. Load factor $\\alpha = n/m \\le 1$.\nInsertion probes table until an empty slot is found. Hash function $h: U \\times \\{0, ..., m-1\\} \\to \\{0, ..., m-1\\}$ where second arg is probe number.\nProbe sequence $\\langle h(k,0), h(k,1), ..., h(k,m-1) \\rangle$ must be a permutation of $\\langle 0, ..., m-1 \\rangle$.\n- `HASH-INSERT(T, k)`: [[PartIII Data Structures Algorithms.md#C11 HASH-INSERT]] Probes slots $h(k,0), h(k,1), ...$ until empty slot found or table full.\n- `HASH-SEARCH(T, k)`: [[PartIII Data Structures Algorithms.md#C11 HASH-SEARCH]] Probes similarly, stops if key found or empty slot encountered.\nDeletion is tricky: cannot just mark slot NIL. Use special value `DELETED`.\n**Independent uniform permutation hashing** (idealized assumption): Probe sequence for each key is equally likely to be any of $m!$ permutations.\n\n### Double hashing\n$h(k, i) = (h_1(k) + i \\cdot h_2(k)) \\pmod m$. $h_1, h_2$ are auxiliary hash functions.\nInitial probe $h_1(k)$, successive probes offset by $h_2(k) \\pmod m$.\n$h_2(k)$ must be relatively prime to $m$ for entire table to be searched. Achieved if $m$ is power of 2 and $h_2(k)$ is odd, or if $m$ is prime and $h_2(k)$ is positive and $<m$.\nExample: $m$ prime, $h_1(k)=k \\pmod m$, $h_2(k)=1+(k \\pmod{m'})$ for $m' < m$.\nDouble hashing provides $\\Theta(m^2)$ probe sequences.\n\n### Linear probing\nA special case of double hashing with $h_2(k)=1$. $h(k,i) = (h_1(k)+i) \\pmod m$.\nEasy to implement, but suffers from **primary clustering**: long runs of occupied slots build up.\nOnly $m$ distinct probe sequences.\n\n### Analysis of open-address hashing\nAssuming independent uniform permutation hashing, $\\alpha < 1$, no deletions.\n**Theorem 11.6 (Unsuccessful search)**: Expected number of probes $\\le 1/(1-\\alpha)$.\n*Proof idea*: Let $X$ be probes. $P(X \\ge i) \\le \\alpha^{i-1}$. $E[X] = \\sum P(X \\ge i)$.\n**Corollary 11.7 (Insertion)**: Expected probes $\\le 1/(1-\\alpha)$.\n**Theorem 11.8 (Successful search)**: Expected probes $\\le (1/\\alpha) \\ln(1/(1-\\alpha))$.\n*Proof idea*: A key $k$ was inserted when load factor was, say, $\\alpha_k = i/m$. Expected probes for $k$ is $\\le 1/(1-\\alpha_k)$. Average over all $n$ keys.\n\n## 11.5 Practical considerations\n\n### Memory hierarchies\nModern CPUs have fast registers, caches (L1, L2, etc.), main memory. Cache blocks (e.g., 64 bytes) are fetched together. Locality of reference is important. Standard RAM model (counting probes) is an approximation.\n\n### Advanced instruction sets\nE.g., AES-NI instructions for cryptography, can be used for efficient hash functions.\n\n### 11.5.1 Linear probing (revisited)\nPoor in RAM model due to primary clustering, but excels in hierarchical memory models due to locality (successive probes often in same cache block).\n**Deletion with linear probing**:\nCan be done without `DELETED` marker. Uses inverse function $g(k,q) = (q-h_1(k)) \\pmod m$ to find probe number for key $k$ to reach slot $q$.\n`LINEAR-PROBING-HASH-DELETE(T, q)`: [[PartIII Data Structures Algorithms.md#C11 LINEAR-PROBING-HASH-DELETE]] Deletes key from slot $q$. Then iterates through subsequent slots $q'$. If a key $k'$ in $q'$ should have been placed in $q$ (i.e., $q$ was probed before $q'$ during $k'$'s insertion, meaning $g(k',q) < g(k',q')$), $k'$ is moved to $q$. This process repeats for the now-vacated $q'$.\n**Analysis**: If $h_1$ is 5-independent and $\\alpha \\le 2/3$, expected $O(1)$ time for operations (Theorem 11.9).\n\n### 11.5.2 Hash functions for hierarchical memory models\n**The wee hash function**:\nFor $w$-bit words (e.g., $w=64$, even). Let $a$ be odd $w$-bit integer.\n`swap(x)` swaps $w/2$-bit halves of $x$.\n$f_a(k) = \\text{swap}((2k^2+ak) \\pmod{2^w})$.\n$f_a^{(r)}(k)$ is iterating $f_a$ for $r$ rounds on $k$.\nFor short $t$-bit input $k$, $h_{a,b,t,r}(k) = (f^{(r)}_{a+2t}(k+b)) \\pmod m$. $b$ randomizes input. $2t$ in $a+2t$ ensures different lengths hash differently (and $a+2t$ is odd if $a$ is odd).\n**Wee hash function for variable-length inputs**:\nBreak input $k$ of $t$ bits into $u = \\lceil t/w \\rceil$ words $\\langle k_1, ..., k_u \\rangle$ using `chop(k)`.\n`WEE(k, a, b, t, r, m)`: [[PartIII Data Structures Algorithms.md#C11 WEE]]\nInitialize $q=b$. For $i=1$ to $u$, $q = f^{(r)}_{a+2t}(k_i + q)$. Return $q \\pmod m$.\nUses only fast registers, low latency. Recommended $r=4$. Approximate 5-independence. Highly efficient in practice."
            },
            {
                "file": "PartIII Data Structures/C12 Binary Search Trees.md",
                "contents": "# 12 Binary Search Trees\n\nA search tree data structure supports dynamic-set operations like `SEARCH`, `MINIMUM`, `MAXIMUM`, `PREDECESSOR`, `SUCCESSOR`, `INSERT`, `DELETE`. It can be used as a dictionary and a priority queue.\nBasic operations on a binary search tree take time proportional to the height $h$ of the tree. For a complete binary tree with $n$ nodes, operations run in $\\Theta(\\lg n)$ worst-case time. If the tree is a linear chain of $n$ nodes, operations take $\\Theta(n)$ time. Randomly built BSTs have expected height $O(\\lg n)$.\n\n## 12.1 What is a binary search tree?\nA binary search tree is organized as a binary tree. Nodes are represented by linked data structures, each containing `key`, satellite data, and pointers `left`, `right`, and `p` (parent). If a child or parent is missing, the attribute is `NIL`. The tree itself has an attribute `root` pointing to the root node.\n\n**Binary-search-tree property**:\nLet $x$ be a node in a binary search tree.\n- If $y$ is a node in the left subtree of $x$, then $y.key \\le x.key$.\n- If $y$ is a node in the right subtree of $x$, then $y.key \\ge x.key$.\n\nThis property allows printing all keys in sorted order using an **inorder tree walk**.\n`INORDER-TREE-WALK(x)`: [[PartIII Data Structures Algorithms.md#C12 INORDER-TREE-WALK]]\nRecursively walks left subtree, prints key of $x$, then recursively walks right subtree.\n**Theorem 12.1**: If $x$ is the root of an $n$-node subtree, then `INORDER-TREE-WALK(x)` takes $\\Theta(n)$ time.\n*Proof*: Let $T(n)$ be the time. $T(0)=c$. For $n>0$, $T(n) \\le T(k) + T(n-k-1) + d$ for some constants $c,d > 0$, where $k$ is size of left subtree. By substitution, $T(n) = O(n)$. Since all $n$ nodes are visited, $T(n) = \\Omega(n)$. Thus $T(n) = \\Theta(n)$.\nOther traversals include preorder (root, left, right) and postorder (left, right, root).\n\n## 12.2 Querying a binary search tree\nOperations `SEARCH`, `MINIMUM`, `MAXIMUM`, `SUCCESSOR`, `PREDECESSOR` take $O(h)$ time on a BST of height $h$.\n\n### Searching\n`TREE-SEARCH(x, k)`: [[PartIII Data Structures Algorithms.md#C12 TREE-SEARCH]]\nStarts at root $x$. Compares $k$ with $x.key$. If equal, found. If $k < x.key$, search left subtree. If $k > x.key$, search right subtree. Traces a simple path downward.\n`ITERATIVE-TREE-SEARCH(x, k)`: [[PartIII Data Structures Algorithms.md#C12 ITERATIVE-TREE-SEARCH]] is an iterative version, generally more efficient on most computers.\nRunning time for both is $O(h)$.\n\n### Minimum and maximum\n`TREE-MINIMUM(x)`: [[PartIII Data Structures Algorithms.md#C12 TREE-MINIMUM]]\nFollows `left` pointers from node $x$ until `NIL` is encountered. The last node found is the minimum in subtree rooted at $x$.\n`TREE-MAXIMUM(x)`: [[PartIII Data Structures Algorithms.md#C12 TREE-MAXIMUM]]\nSymmetrically follows `right` pointers from $x$.\nRunning time for both is $O(h)$.\n\n### Successor and predecessor\nSuccessor of node $x$ is the node with the smallest key greater than $x.key$ (if keys distinct), or next node in inorder walk.\n`TREE-SUCCESSOR(x)`: [[PartIII Data Structures Algorithms.md#C12 TREE-SUCCESSOR]]\nTwo cases:\n1. If $x.right$ is not `NIL`, successor is `TREE-MINIMUM(x.right)` (leftmost node in $x$'s right subtree).\n2. If $x.right$ is `NIL`, successor $y$ is the lowest ancestor of $x$ whose left child is also an ancestor of $x$. Go up from $x$ via $x.p$ until a node $y$ is found that is the left child of its parent, or $y$ is `NIL` (if $x$ was maximum).\n`TREE-PREDECESSOR(x)` is symmetric.\nRunning time for both is $O(h)$.\n\n**Theorem 12.2**: `SEARCH`, `MINIMUM`, `MAXIMUM`, `SUCCESSOR`, `PREDECESSOR` run in $O(h)$ time on a BST of height $h$.\n\n## 12.3 Insertion and deletion\nThese operations modify the BST while maintaining the binary-search-tree property.\n\n### Insertion\n`TREE-INSERT(T, z)`: [[PartIII Data Structures Algorithms.md#C12 TREE-INSERT]]\nNode $z$ has $z.key$ filled, $z.left = \\text{NIL}$, $z.right = \\text{NIL}$.\nProcedure starts at $T.root$. Traces a path downward, maintaining pointer $y$ as parent of $x$. When $x$ becomes `NIL`, $y$ is the parent for $z$. $z$ is inserted as $y.left$ or $y.right$.\nRunning time is $O(h)$.\n\n### Deletion\n`TREE-DELETE(T, z)` removes node $z$ from tree $T$.\nStrategy depends on number of children of $z$.\nHelper procedure `TRANSPLANT(T, u, v)` replaces subtree rooted at $u$ with subtree rooted at $v$. Node $u$'s parent becomes $v$'s parent. $v.p$ updated. Does not update $v.left, v.right$.\n`TRANSPLANT(T, u, v)`: [[PartIII Data Structures Algorithms.md#C12 TRANSPLANT]]\n\n`TREE-DELETE(T, z)`: [[PartIII Data Structures Algorithms.md#C12 TREE-DELETE]]\nHandles four cases (slightly different from initial 3-case overview):\n1. If $z.left == \\text{NIL}$: Replace $z$ by $z.right$ using `TRANSPLANT(T, z, z.right)`. This covers $z$ having no children ( $z.right$ is also `NIL`) or only a right child.\n2. If $z.right == \\text{NIL}$ (and $z.left != \\text{NIL}$ from case 1): Replace $z$ by $z.left$ using `TRANSPLANT(T, z, z.left)`. This covers $z$ having only a left child.\n3. If $z$ has two children: Find $z$'s successor $y = \\text{TREE-MINIMUM}(z.right)$. $y$ is in $z$'s right subtree and has no left child.\n    a. If $y == z.right$: Replace $z$ by $y$. $y$ takes $z$'s left child. $y$'s right child remains. $y$ adopts $z$'s color if colors are used (not in this chapter but relevant later).\n       `TRANSPLANT(T, z, y)`. Set $y.left = z.left$, $y.left.p = y$.\n    b. If $y != z.right$: First, replace $y$ by its own right child $y.right$. Then, $y$ takes $z$'s place. $y$ adopts $z$'s right subtree and $z$'s left subtree.\n       `TRANSPLANT(T, y, y.right)`. Set $y.right = z.right$, $y.right.p = y$. Then `TRANSPLANT(T, z, y)`. Set $y.left = z.left$, $y.left.p = y$.\nRunning time is $O(h)$ due to `TREE-MINIMUM` and path traversals.\n\n**Theorem 12.3**: `INSERT` and `DELETE` can be implemented to run in $O(h)$ time on a BST of height $h$.\n\n## Problems\n- **12-2 Radix trees (tries)**: Store bit strings. Search by path based on bits. Can sort $S$ (set of distinct bit strings, total length $n$) in $\\Theta(n)$ time.\n- **12-3 Average node depth in a randomly built BST**: Proven to be $O(\\lg n)$. Relates to Randomized Quicksort.\n- **12-4 Number of different binary trees**: $b_n = \\frac{1}{n+1} \\binom{2n}{n}$ (nth Catalan number)."
            },
            {
                "file": "PartIII Data Structures/C13 Red-Black Trees.md",
                "contents": "# 13 Red-Black Trees\n\nChapter 12 showed binary search tree operations run in $O(h)$ time. Red-black trees are a type of balanced search tree that guarantee $h=O(\\lg n)$, so operations take $O(\\lg n)$ worst-case time.\n\n## 13.1 Properties of red-black trees\nA **red-black tree** is a binary search tree with an extra bit of storage per node: its **color**, which can be either RED or BLACK.\nIt satisfies the following **red-black properties**:\n1. Every node is either red or black.\n2. The root is black.\n3. Every leaf (NIL) is black.\n4. If a node is red, then both its children are black.\n5. For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.\n\nA single sentinel `T.nil` represents all NILs (leaves and root's parent). `T.nil.color` is BLACK. Other attributes of `T.nil` are immaterial.\nInternal nodes hold key values. Leaves (sentinels) are omitted in drawings for brevity.\nThe **black-height** of a node $x$, denoted `bh(x)`, is the number of black nodes on any simple path from, but not including, $x$ down to a leaf. Well-defined by property 5. Black-height of a red-black tree is `bh(T.root)`.\n\n**Lemma 13.1**: A red-black tree with $n$ internal nodes has height at most $2 \\lg(n+1)$.\n*Proof sketch*:\n1. Show that any subtree rooted at $x$ contains at least $2^{bh(x)}-1$ internal nodes. (By induction on height of $x$).\n   Base case: height of $x$ is 0 ($x$ is a leaf, $T.nil$). $bh(x)=0$. Subtree has $2^0-1 = 0$ internal nodes. True.\n   Inductive step: Node $x$ has positive height. Its children $c_1, c_2$ have $bh(c_i) \\ge bh(x)-1$ (if $c_i$ black) or $bh(c_i) = bh(x)$ (if $c_i$ red, then its children are black, leading to $bh(x)$ for $c_i$). By property 4, if $c_i$ is red, its children are black, so their black-height is $bh(x)-1$. If $c_i$ is black, its black-height is $bh(x)-1$. So each child's subtree has $\\ge 2^{bh(x)-1}-1$ internal nodes. Total internal nodes for $x$ $\\ge (2^{bh(x)-1}-1) + (2^{bh(x)-1}-1) + 1 = 2^{bh(x)}-1$.\n2. Let $h$ be height of tree. Root's black-height $bh(T.root) \\ge h/2$ (by property 4, at least half nodes on root-to-leaf path must be black).\n3. So, $n \\ge 2^{h/2}-1$. This implies $h \\le 2 \\lg(n+1)$.\nThus, height is $O(\\lg n)$, making search operations efficient.\n\n## 13.2 Rotations\n`INSERT` and `DELETE` may violate red-black properties. Rotations are used to restore properties by changing pointer structures. They preserve the binary-search-tree property.\n**Left rotation**: `LEFT-ROTATE(T, x)` [[PartIII Data Structures Algorithms.md#C13 LEFT-ROTATE]]\nAssumes $x.right \\neq T.nil$. Pivots around the link between $x$ and $y=x.right$. $y$ becomes new root of subtree. $x$ becomes $y.left$. $y$'s original left child $\\beta$ becomes $x.right$. Pointers $p, left, right$ are updated in $O(1)$ time.\n**Right rotation**: `RIGHT-ROTATE(T, y)` is symmetric to `LEFT-ROTATE(T, x)`.\nBoth run in $O(1)$ time.\n\n## 13.3 Insertion\n`RB-INSERT(T, z)` [[PartIII Data Structures Algorithms.md#C13 RB-INSERT]] inserts node $z$ into red-black tree $T$.\n1. $z$ is inserted as in a standard BST (`TREE-INSERT` logic, using `T.nil`).\n2. $z.left = T.nil$, $z.right = T.nil$.\n3. $z.color = \\text{RED}$.\n4. Call `RB-INSERT-FIXUP(T, z)` to restore red-black properties.\nColoring $z$ RED ensures property 5 is maintained. Properties 1 and 3 hold. Property 2 (root is black) or property 4 (red node has black children) might be violated.\n\n`RB-INSERT-FIXUP(T, z)` [[PartIII Data Structures Algorithms.md#C13 RB-INSERT-FIXUP]]\n**Loop Invariant** at start of `while` loop (line 1):\n  a. Node $z$ is RED.\n  b. If $z.p$ is the root, then $z.p$ is BLACK.\n  c. If tree violates any RB properties, it's at most one: either property 2 ( $z$ is root and RED) or property 4 ( $z$ and $z.p$ are RED). Not both.\n\nThe `while` loop (condition: $z.p.color == \\text{RED}$) handles violations of property 4. If $z.p$ is root, it's black by invariant (b), so loop terminates. Thus, $z.p.p$ (grandparent) exists. Code symmetric for $z.p$ being left/right child of $z.p.p$.\nAssume $z.p$ is a left child (lines 2-15):\n- Let $y = z.p.p.right$ (uncle of $z$).\n- **Case 1: Uncle $y$ is RED** (line 4).\n  - $z.p.color = \\text{BLACK}$.\n  - $y.color = \\text{BLACK}$.\n  - $z.p.p.color = \\text{RED}$.\n  - $z = z.p.p$ (move $z$ up two levels).\n  The problem (two adjacent REDs) is pushed up the tree. Loop continues.\n- **Case 2: Uncle $y$ is BLACK and $z$ is a right child** (line 10).\n  - $z = z.p$.\n  - `LEFT-ROTATE(T, z)`.\n  Transforms Case 2 into Case 3. $z$ and $z.p$ are still RED.\n- **Case 3: Uncle $y$ is BLACK and $z$ is a left child** (implicit after Case 2, or directly).\n  - $z.p.color = \\text{BLACK}$.\n  - $z.p.p.color = \\text{RED}$.\n  - `RIGHT-ROTATE(T, z.p.p)`.\n  Property 4 restored. New $z.p$ is BLACK. Loop terminates.\nSymmetric logic applies if $z.p$ is a right child (lines 16-29).\nAfter loop, `T.root.color = BLACK` (line 30) ensures property 2.\n\n**Analysis**: `RB-INSERT` takes $O(\\lg n)$ time. Initial insertion is $O(\\lg n)$. `RB-INSERT-FIXUP` loop runs $O(\\lg n)$ times (Case 1 moves $z$ up). Rotations are $O(1)$. At most 2 rotations occur.\n\n## 13.4 Deletion\n`RB-DELETE(T, z)` [[PartIII Data Structures Algorithms.md#C13 RB-DELETE]] removes node $z$. More complex than insertion.\nBased on `TREE-DELETE`. Uses `RB-TRANSPLANT`.\n`RB-TRANSPLANT(T, u, v)`: [[PartIII Data Structures Algorithms.md#C13 RB-TRANSPLANT]] similar to `TRANSPLANT` but uses `T.nil` and $v.p$ assignment is unconditional (even if $v=T.nil$).\n\nIn `RB-DELETE(T, z)`:\n- $y$ is the node either removed from tree or moved within tree (like in `TREE-DELETE`). If $z$ has $<2$ children, $y=z$. If $z$ has 2 children, $y=TREE-MINIMUM(z.right)$ (successor).\n- $x$ is the node that moves into $y$'s original position. $x$ might be $T.nil$.\n- `y-original-color` stores $y$'s color before any changes.\n- If $y$ has 2 children (i.e., $y = z$ and $z$ has 2 children), $y$'s successor is moved into $y$'s place, and $y$ takes $z$'s color.\n- If `y-original-color` was RED: RB properties hold. No black-heights change. No two REDs made adjacent. Root remains BLACK (if $y$ was RED, $y$ was not root).\n- If `y-original-color` was BLACK: Problems may arise.\n  1. If $y$ was root and a RED child $x$ becomes new root $\rightarrow$ violates prop 2.\n  2. If $x$ and $x.p$ both RED $\rightarrow$ violates prop 4.\n  3. Paths that contained $y$ have one fewer BLACK node $\rightarrow$ violates prop 5. We consider $x$ to have an "
            },
            {
                "file": "PartIII Data Structures/C13 Red-Black Trees.md",
                "contents": "extra black"
            },
            {
                "file": "PartIII Data Structures/C13 Red-Black Trees.md",
                "contents": ". Node $x$ is now "
            },
            {
                "file": "PartIII Data Structures/C13 Red-Black Trees.md",
                "contents": "doubly black"
            },
            {
                "file": "PartIII Data Structures/C13 Red-Black Trees.md",
                "contents": " or "
            },
            {
                "file": "PartIII Data Structures/C13 Red-Black Trees.md",
                "contents": "red-and-black"
            },
            {
                "file": "PartIII Data Structures/C13 Red-Black Trees.md",
                "contents": ". This violates prop 1.\n  Call `RB-DELETE-FIXUP(T, x)` to resolve.\n\n`RB-DELETE-FIXUP(T, x)` [[PartIII Data Structures Algorithms.md#C13 RB-DELETE-FIXUP]]\nGoal: Move extra black up the tree until $x$ points to RED&BLACK node, or $x$ is root, or rotations/recoloring solve it.\n`while` loop (condition: $x \\neq T.root$ and $x.color == \\text{BLACK}$ (meaning $x$ is doubly black)).\nAssume $x$ is a left child ($x == x.p.left$). $w = x.p.right$ is $x$'s sibling. $w \\neq T.nil$.\n- **Case 1: $x$'s sibling $w$ is RED** (line 4).\n  - $w.color = \\text{BLACK}$.\n  - $x.p.color = \\text{RED}$.\n  - `LEFT-ROTATE(T, x.p)`.\n  - New sibling $w$ (former child of old $w$) is BLACK.\n  Transforms Case 1 into Case 2, 3, or 4.\n- **Case 2: $x$'s sibling $w$ is BLACK, and both $w$'s children are BLACK** (line 9).\n  - $w.color = \\text{RED}$.\n  - $x = x.p$ (move extra black up to $x.p$).\n  Loop continues with new $x$. If this case entered from Case 1, $x.p$ (new $x$) was RED, so new $x$ is RED&BLACK. Loop terminates. Extra black removed by coloring $x$ singly BLACK (line 44).\n- **Case 3: $x$'s sibling $w$ is BLACK, $w.left$ is RED, $w.right$ is BLACK** (line 13).\n  - $w.left.color = \\text{BLACK}$.\n  - $w.color = \\text{RED}$.\n  - `RIGHT-ROTATE(T, w)`.\n  - New sibling $w$ (former $w.left$) is BLACK with a RED right child.\n  Transforms Case 3 into Case 4.\n- **Case 4: $x$'s sibling $w$ is BLACK, and $w.right$ is RED** (implicit after Case 3, or directly).\n  - $w.color = x.p.color$.\n  - $x.p.color = \\text{BLACK}$.\n  - $w.right.color = \\text{BLACK}$.\n  - `LEFT-ROTATE(T, x.p)`.\n  - $x = T.root$ (to terminate loop).\n  Extra black removed. Loop terminates.\nSymmetric logic if $x$ is a right child (lines 23-43).\nFinally, $x.color = \\text{BLACK}$ (line 44) ensures property 1 (if $x$ was RED&BLACK) and property 2 (if $x$ became root).\n\n**Analysis**: `RB-DELETE` takes $O(\\lg n)$ time. Initial part is $O(\\lg n)$. `RB-DELETE-FIXUP`: Cases 1, 3, 4 terminate after constant color changes and $\\le 3$ rotations. Case 2 moves $x$ up $O(\\lg n)$ times, no rotations. Total $\\le 3$ rotations. Total time $O(\\lg n)$.\n\n## Problems\n- **13-1 Persistent dynamic sets**: Explores making dynamic sets persistent. Using RB-trees, operations can be $O(\\lg n)$ time and space per update by path copying.\n- **13-3 AVL trees**: Another type of balanced BST. Height difference between left/right children subtrees is at most 1. Insertion involves rotations to maintain balance. Height $O(\\lg n)$. Operations $O(\\lg n)$."
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartV Advanced Data Structures/C_Intro PartV Advanced Data Structures.md",
                "contents": "# Introduction to Part V: Advanced Data Structures\n\nThis part returns to studying data structures that support operations on dynamic sets, but at a more advanced level than Part III. One of the chapters, for example, makes extensive use of the amortized analysis techniques from Chapter 16.\n\nChapter 17 shows how to augment red-black trees\u2014adding additional information in each node\u2014to support dynamic-set operations in addition to those covered in Chapters 12 and 13. The first example augments red-black trees to dynamically maintain order statistics for a set of keys. Another example augments them in a different way to maintain intervals of real numbers. Chapter 17 includes a theorem giving sufficient conditions for when a red-black tree can be augmented while maintaining the $O(\\lg n)$ running times for insertion and deletion.\n\nChapter 18 presents B-trees, which are balanced search trees specifically designed to be stored on disks. Since disks operate much more slowly than random-access memory, B-tree performance depends not only on how much computing time the dynamic-set operations consume but also on how many disk accesses they perform. For each B-tree operation, the number of disk accesses increases with the height of the B-tree, but B-tree operations keep the height low.\n\nChapter 19 examines data structures for disjoint sets. Starting with a universe of $n$ elements, each initially in its own singleton set, the operation UNION unites two sets. At all times, the $n$ elements are partitioned into disjoint sets, even as calls to the UNION operation change the members of a set dynamically. The query FIND-SET identifies the unique set that contains a given element at the moment. Representing each set as a simple rooted tree yields surprisingly fast operations: a sequence of $m$ operations runs in $O(m \\alpha(n))$ time, where $\\alpha(n)$ is an incredibly slowly growing function\u2014$\\alpha(n)$ is at most 4 in any conceivable application. The amortized analysis that proves this time bound is as complex as the data structure is simple.\n\n## Other Advanced Data Structures\n\nThe topics covered in this part are by no means the only examples of \u201cadvanced\u201d data structures. Other advanced data structures include the following:\n\nFibonacci heaps [156] implement mergeable heaps (see Problem 10-2 on page 268) with the operations INSERT, MINIMUM, and UNION taking only $O(1)$ actual and amortized time, and the operations EXTRACT-MIN and DELETE taking $O(\\lg n)$ amortized time. The most significant advantage of these data structures, however, is that DECREASE-KEY takes only $O(1)$ amortized time. Strict Fibonacci heaps [73], developed later, made all of these time bounds actual. Because the DECREASE-KEY operation takes constant amortized time, (strict) Fibonacci heaps constitute key components of some of the asymptotically fastest algorithms to date for graph problems.\n\nDynamic trees [415, 429] maintain a forest of disjoint rooted trees. Each edge in each tree has a real-valued cost. Dynamic trees support queries to find parents, roots, edge costs, and the minimum edge cost on a simple path from a node up to a root. Trees may be manipulated by cutting edges, updating all edge costs on a simple path from a node up to a root, linking a root into another tree, and making a node the root of the tree it appears in. One implementation of dynamic trees gives an $O(\\lg n)$ amortized time bound for each operation, while a more complicated implementation yields $O(\\lg n)$ worst-case time bounds. Dynamic trees are used in some of the asymptotically fastest network-flow algorithms.\n\nSplay trees [418, 429] are a form of binary search tree on which the standard search-tree operations run in $O(\\lg n)$ amortized time. One application of splay trees simplifies dynamic trees.\n\nPersistent data structures allow queries, and sometimes updates as well, on past versions of a data structure. For example, linked data structures can be made persistent with only a small time and space cost [126]. Problem 13-1 gives a simple example of a persistent dynamic set.\n\nSeveral data structures allow a faster implementation of dictionary operations (INSERT, DELETE, and SEARCH) for a restricted universe of keys. By taking advantage of these restrictions, they are able to achieve better worst-case asymptotic running times than comparison-based data structures. If the keys are unique integers drawn from the set $\\{0, 1, 2, \\dots, u-1\\}$, where $u$ is an exact power of 2, then a recursive data structure known as a van Emde Boas tree [440, 441] supports each of the operations SEARCH, INSERT, DELETE, MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR in $O(\\lg \\lg u)$ time. Fusion trees [157] were the first data structure to allow faster dictionary operations when the universe is restricted to integers, implementing these operations in $O(\\lg n / \\lg \\lg n)$ time. Several subsequent data structures, including exponential search trees [17], have also given improved bounds on some or all of the dictionary operations and are mentioned in the chapter notes throughout this book.\n\nDynamic graph data structures support various queries while allowing the structure of a graph to change through operations that insert or delete vertices or edges. Examples of the queries that they support include vertex connectivity [214], edge connectivity, minimum spanning trees [213], biconnectivity, and transitive closure [212].\n\nChapter notes throughout this book mention additional data structures."
            },
            {
                "file": "PartV Advanced Data Structures Algorithms.md",
                "contents": "# C17 Augmenting Data Structures Algorithms\n\n## C17.1 OS-SELECT\n```\nOS-SELECT(x, i)\n  1  r = x.left.size + 1  // rank of x within the subtree rooted at x\n  2  if i == r\n  3      return x\n  4  elseif i < r\n  5      return OS-SELECT(x.left, i)\n  6  else return OS-SELECT(x.right, i - r)\n```\n\n## C17.1 OS-RANK\n```\nOS-RANK(T, x)\n  1  r = x.left.size + 1  // rank of x within the subtree rooted at x\n  2  y = x                // root of subtree being examined\n  3  while y != T.root\n  4      if y == y.p.right\n  5          r = r + y.p.left.size + 1  // if root of a right subtree ...\n                                      // ... add in parent and its left subtree\n  6      y = y.p                      // move y toward the root\n  7  return r\n```\n\n## C17.1 LEFT-ROTATE Subtree Size Update\n(Lines to add to LEFT-ROTATE(T,x) from page 336, referenced on page 9 of Ch 17 OCR)\n```\n13 y.size = x.size\n14 x.size = x.left.size + x.right.size + 1\n```\n\n## C17.3 INTERVAL-SEARCH\n```\nINTERVAL-SEARCH(T, i)\n  1  x = T.root\n  2  while x != T.nil and i does not overlap x.int\n  3      if x.left != T.nil and x.left.max >= i.low\n  4          x = x.left  // overlap in left subtree or no overlap in right subtree\n  5      else x = x.right // no overlap in left subtree\n  6  return x\n```\n\n# C18 B-Trees Algorithms\n\n## C18.2 B-TREE-SEARCH\n```\nB-TREE-SEARCH(x, k)\n  1  i = 1\n  2  while i <= x.n and k > x.key_i\n  3      i = i + 1\n  4  if i <= x.n and k == x.key_i\n  5      return (x, i)\n  6  elseif x.leaf\n  7      return NIL\n  8  else DISK-READ(x.c_i)\n  9      return B-TREE-SEARCH(x.c_i, k)\n```\n\n## C18.2 B-TREE-CREATE\n```\nB-TREE-CREATE(T)\n  1  x = ALLOCATE-NODE()\n  2  x.leaf = TRUE\n  3  x.n = 0\n  4  DISK-WRITE(x)\n  5  T.root = x\n```\n\n## C18.2 B-TREE-SPLIT-CHILD\n```\nB-TREE-SPLIT-CHILD(x, i)\n  1  y = x.c_i                            // y is x's ith child, and y is full\n  2  z = ALLOCATE-NODE()\n  3  z.leaf = y.leaf\n  4  z.n = t - 1\n  5  for j = 1 to t - 1                   // z gets y's greatest t-1 keys\n  6      z.key_j = y.key_{j+t}\n  7  if not y.leaf\n  8      for j = 1 to t                   // ...and its corresponding t children\n  9          z.c_j = y.c_{j+t}\n 10  y.n = t - 1                          // y keeps t-1 keys\n 11  for j = x.n + 1 downto i + 1         // shift x's children to the right...\n 12      x.c_{j+1} = x.c_j\n 13  x.c_{i+1} = z                        // ...to make room for z as a child of x\n 14  for j = x.n downto i                 // shift the corresponding keys in x\n 15      x.key_{j+1} = x.key_j\n 16  x.key_i = y.key_t                    // insert y's median key into x\n 17  x.n = x.n + 1                        // x has gained a child\n 18  DISK-WRITE(y)\n 19  DISK-WRITE(z)\n 20  DISK-WRITE(x)\n```\n\n## C18.2 B-TREE-INSERT\n```\nB-TREE-INSERT(T, k)\n  1  r = T.root\n  2  if r.n == 2t - 1\n  3      s = B-TREE-SPLIT-ROOT(T)\n  4      B-TREE-INSERT-NONFULL(s, k)\n  5  else B-TREE-INSERT-NONFULL(r, k)\n```\n\n## C18.2 B-TREE-SPLIT-ROOT\n```\nB-TREE-SPLIT-ROOT(T)\n  1  s = ALLOCATE-NODE()\n  2  s.leaf = FALSE\n  3  s.n = 0\n  4  s.c_1 = T.root\n  5  T.root = s\n  6  B-TREE-SPLIT-CHILD(s, 1)\n  7  return s\n```\n\n## C18.2 B-TREE-INSERT-NONFULL\n```\nB-TREE-INSERT-NONFULL(x, k)\n  1  i = x.n\n  2  if x.leaf\n  3      while i >= 1 and k < x.key_i         // shift keys in x to make room for k\n  4          x.key_{i+1} = x.key_i\n  5          i = i - 1\n  6      x.key_{i+1} = k                      // insert key k in x\n  7      x.n = x.n + 1\n  8      DISK-WRITE(x)\n  9  else while i >= 1 and k < x.key_i      // find the child where k belongs\n 10          i = i - 1\n 11      i = i + 1\n 12      DISK-READ(x.c_i)\n 13      if x.c_i.n == 2t - 1\n 14          B-TREE-SPLIT-CHILD(x, i)       // split the child if it's full\n 15          if k > x.key_i                 // determine which of the two children is now the correct one\n 16              i = i + 1\n 17      B-TREE-INSERT-NONFULL(x.c_i, k)\n```\n\n# C19 Data Structures for Disjoint Sets Algorithms\n\n## C19.1 CONNECTED-COMPONENTS\n```\nCONNECTED-COMPONENTS(G)\n  1  for each vertex v in G.V\n  2      MAKE-SET(v)\n  3  for each edge (u,v) in G.E\n  4      if FIND-SET(u) != FIND-SET(v)\n  5          UNION(u,v)\n```\n\n## C19.1 SAME-COMPONENT\n```\nSAME-COMPONENT(u,v)\n  1  if FIND-SET(u) == FIND-SET(v)\n  2      return TRUE\n  3  else return FALSE\n```\n\n## C19.3 MAKE-SET (Forest)\n```\nMAKE-SET(x)\n  1  x.p = x\n  2  x.rank = 0\n```\n\n## C19.3 UNION (Forest)\n```\nUNION(x,y)\n  1  LINK(FIND-SET(x), FIND-SET(y))\n```\n\n## C19.3 LINK (Forest)\n```\nLINK(x,y)\n  1  if x.rank > y.rank\n  2      y.p = x\n  3  else x.p = y\n  4      if x.rank == y.rank\n  5          y.rank = y.rank + 1\n```\n\n## C19.3 FIND-SET (Forest with Path Compression)\n```\nFIND-SET(x)\n  1  if x != x.p\n  2      x.p = FIND-SET(x.p)\n  3  return x.p\n```\n\n## C19.P1 OFFLINE-MINIMUM\n```\nOFFLINE-MINIMUM(m, n) // m EXTRACT-MIN calls, n INSERT calls\n  1  for i = 1 to n  // for each key i from 1 to n\n  2      determine j such that i is in K_j // K_j is set of keys from I_j\n  3      if j != m + 1 // K_{m+1} is for keys not yet extracted\n  4          extracted[j] = i\n  5          let l be the smallest value greater than j for which set K_l exists\n  6          K_l = K_j U K_l, destroying K_j\n  7  return extracted\n```\n\n## C19.P3 LCA\n```\nLCA(u)\n  1  MAKE-SET(u)\n  2  FIND-SET(u).ancestor = u\n  3  for each child v of u in T\n  4      LCA(v)\n  5      UNION(u,v)\n  6      FIND-SET(u).ancestor = u\n  7  u.color = BLACK\n  8  for each node v such that {u,v} is in P // P is set of pairs\n  9      if v.color == BLACK\n 10          print \"The lowest common ancestor of\" u \"and\" v \"is\" FIND-SET(v).ancestor\n```"
            },
            {
                "file": "PartV Advanced Data Structures/C17 Augmenting Data Structures.md",
                "contents": "# 17 Augmenting Data Structures\n\nSome solutions require no more than a \u201ctextbook\u201d data structure, but many others require creativity. Rarely will you need to create an entirely new type of data structure. More often, you can augment a textbook data structure by storing additional information in it. You can then program new operations for the data structure to support your application. Augmenting a data structure is not always straightforward, however, since the added information must be updated and maintained by the ordinary operations on the data structure.\n\nThis chapter discusses two data structures based on red-black trees that are augmented with additional information. Section 17.1 describes a data structure that supports general order-statistic operations on a dynamic set. Section 17.2 abstracts the process of augmenting a data structure and provides a theorem for augmenting red-black trees. Section 17.3 uses this theorem to design a data structure for maintaining a dynamic set of intervals.\n\n## 17.1 Dynamic order statistics\n\nChapter 9 introduced the notion of an order statistic. The $i$th order statistic of a set of $n$ elements is the element with the $i$th smallest key. This section shows how to modify red-black trees to determine any order statistic for a dynamic set in $O(\\lg n)$ time and also compute the rank of an element (its position in the linear order) in $O(\\lg n)$ time.\n\nAn **order-statistic tree** $T$ is a red-black tree with additional information stored in each node. Each node $x$ contains the usual red-black tree attributes ($x.key, x.color, x.p, x.left, x.right$) and a new attribute, $x.size$. This attribute contains the number of internal nodes in the subtree rooted at $x$ (including $x$ itself, but not sentinels). If we define the sentinel's size to be 0 ($T.nil.size = 0$), then $x.size = x.left.size + x.right.size + 1$.\n\nKeys need not be distinct. When equal keys are present, the rank of an element is its position in an inorder tree walk. For example, in Figure 17.1 of the book, the key 14 in a black node has rank 5, and the key 14 in a red node has rank 6.\n\n### Retrieving the element with a given rank\n\nThe procedure `OS-SELECT(x, i)` returns a pointer to the node containing the $i$th smallest key in the subtree rooted at $x$. To find the node with the $i$th smallest key in an order-statistic tree $T$, call `OS-SELECT(T.root, i)`. See [[PartV Advanced Data Structures Algorithms.md#C17.1 OS-SELECT]].\n\nThe procedure works by first computing $r = x.left.size + 1$, which is the rank of node $x$ within its own subtree. If $i=r$, $x$ is the desired node. If $i < r$, the $i$th smallest element is in $x$'s left subtree, so recurse on $x.left$. If $i > r$, the $i$th smallest element is in $x$'s right subtree, and it is the $(i-r)$th smallest element in that subtree; recurse on $x.right$ with rank $i-r$.\n\nBecause each recursive call goes down one level, the total time for `OS-SELECT` is at worst proportional to the height of the tree. Since it's a red-black tree, height is $O(\\lg n)$. Thus, running time is $O(\\lg n)$.\n\n### Determining the rank of an element\n\nThe procedure `OS-RANK(T, x)` returns the position of $x$ in the linear order determined by an inorder tree walk of $T$. See [[PartV Advanced Data Structures Algorithms.md#C17.1 OS-RANK]].\n\nNode $x$'s rank is its initial rank within its own subtree ($x.left.size + 1$) plus the sum of sizes of all left subtrees and parent nodes that precede it in the inorder walk. The procedure iteratively goes up the tree from $x$ to the root. It maintains $r$ as the rank of $x.key$ in the subtree rooted at $y$. If $y$ is a right child of $y.p$, then all nodes in $y.p$'s left subtree, plus $y.p$ itself, precede $x$, so $r$ is incremented by $y.p.left.size + 1$.\n\nThe loop invariant is: At the start of each iteration of the `while` loop (lines 3-6), $r$ is the rank of $x.key$ in the subtree rooted at node $y$.\n**Initialization**: Line 1 sets $r$ to $x.left.size + 1$, the rank of $x.key$ within the subtree at $x$. Line 2 sets $y=x$. The invariant holds.\n**Maintenance**: If $y$ is a right child of $y.p$, then all nodes in $y.p$'s left subtree and $y.p$ itself precede $x.key$. Line 5 adds $y.p.left.size + 1$ to $r$. If $y$ is a left child, no nodes in $y.p$'s right subtree nor $y.p$ itself (relative to its position as parent of $y$) precede $x.key$ further than already accounted for. Line 6 moves $y$ up to $y.p$. The invariant is maintained for the new $y$.\n**Termination**: The loop terminates when $y = T.root$. At this point, the subtree is the entire tree, so $r$ is the rank of $x.key$ in the entire tree.\nSince each iteration takes $O(1)$ time and $y$ goes up one level, running time is $O(\\lg n)$.\n\n### Maintaining subtree sizes\n\nFor `OS-SELECT` and `OS-RANK` to work efficiently, subtree sizes must be maintained during insertions and deletions without affecting asymptotic running times.\n\n**Insertion**: Recall insertion has two phases. Phase 1 goes down the tree to insert the new node. Increment $x.size$ for each node $x$ on this path. The new node gets size 1. This adds $O(\\lg n)$ to the first phase. Phase 2 goes up, changing colors and performing at most two rotations. A rotation is a local operation. For `LEFT-ROTATE(T,x)` (where $y=x.right$), the sizes of $x$ and $y$ change. Update $y.size = x.size$ (original $x.size$), then $x.size = x.left.size + x.right.size + 1$. See [[PartV Advanced Data Structures Algorithms.md#C17.1 LEFT-ROTATE Subtree Size Update]]. This takes $O(1)$ per rotation. Total insertion time remains $O(\\lg n)$.\n\n**Deletion**: Deletion also has two phases. Phase 1 removes/moves nodes. Traverse a simple path from the lowest moved node up to the root, decrementing $size$ attributes. This takes $O(\\lg n)$. Phase 2 involves at most three rotations. Updates for rotations are $O(1)$ each. Total deletion time remains $O(\\lg n)$.\n\n### Exercises 17.1\nThis section includes exercises 17.1-1 through 17.1-8.\n\n## 17.2 How to augment a data structure\n\nThe process of augmenting a basic data structure involves four steps:\n1. Choose an underlying data structure.\n2. Determine additional information to maintain in the underlying data structure.\n3. Verify that the additional information can be maintained for the basic modifying operations on the underlying data structure (efficiently).\n4. Develop new operations.\n\nFor order-statistic trees: Step 1 chose red-black trees. Step 2 added the $size$ attribute. Step 3 verified $O(\\lg n)$ maintenance for insertion/deletion. Step 4 developed `OS-SELECT` and `OS-RANK`.\n\n### Augmenting red-black trees\n\n**Theorem 17.1 (Augmenting a red-black tree)**: Let $f$ be an attribute that augments a red-black tree $T$ of $n$ nodes. Suppose that the value of $f$ for each node $x$ depends only on information in nodes $x$, $x.left$, and $x.right$ (possibly including $x.left.f$ and $x.right.f$), and that $x.f$ can be computed in $O(1)$ time using this information. Then, insertion and deletion can maintain the values of $f$ in all nodes of $T$ without asymptotically affecting their $O(\\lg n)$ running times.\n\n*Proof Idea*: A change to an $f$ attribute in a node $x$ propagates only to ancestors of $x$. Updating $x.f$ might require $x.p.f$ to be updated, and so on, up to $T.root.f$. Since tree height is $O(\\lg n)$, this propagation costs $O(\\lg n)$.\nDuring insertion, Phase 1 involves adding a new node. Its $f$ value is computed. Changes propagate up, taking $O(\\lg n)$. Phase 2 involves at most two rotations. Each rotation might require $O(1)$ local attribute updates and then $O(\\lg n)$ propagation if the attribute computation isn't strictly local to children's $f$ values for the parent. However, the theorem's condition that $x.f$ depends only on $x, x.left, x.right$ and their $f$ values implies that after a rotation changes children for $x$ and $y$, their $f$ values can be recomputed in $O(1)$, and then this change propagates up. For attributes like `size` in order-statistic trees, the update after rotation is $O(1)$ because $x.size$ only depends on $x.left.size$ and $x.right.size$, not on further ancestor information. The crucial part is that $x.f$ can be computed in $O(1)$ from its children and itself. If a rotation modifies $x$ and $y$, their $f$ values can be recomputed. Then their parents' $f$ values can be recomputed, and so on. Since only nodes $x, y$ and their ancestors up to the root are affected by a rotation structurally or by attribute change propagation, and there are at most two rotations, insertion takes $O(\\lg n)$. Deletion is similar with at most three rotations.\nIf an attribute update after rotation costs $O(1)$ (like $size$), this is faster. The theorem allows for $O(1)$ computation of $f$ from local information, so it covers both scenarios well. After a rotation involving $x$ and $y$, $y.f$ is updated (if $y$ becomes $x$'s parent), then $x.f$ is updated. Then their new parent's $f$ is updated, and so on. Since the number of rotations is constant, the total time remains $O(\\lg n)$.\n\n### Exercises 17.2\nThis section includes exercises 17.2-1 through 17.2-3.\n\n## 17.3 Interval trees\n\nThis section shows how to augment red-black trees to support operations on dynamic sets of intervals. An interval $[t_1, t_2]$ is an object $i$ with attributes $i.low = t_1$ and $i.high = t_2$. Intervals $i$ and $i'$ overlap if $i.low \text{ \textless{}= } i'.high$ and $i'.low \text{ \textless{}= } i.high$.\nAny two intervals $i, i'$ satisfy the interval trichotomy: they overlap, $i$ is to the left of $i'$ ($i.high < i'.low$), or $i$ is to the right of $i'$ ($i'.high < i.low$).\n\nAn **interval tree** is a red-black tree that maintains a dynamic set of elements, with each element $x$ containing an interval $x.int$. Operations supported:\n- `INTERVAL-INSERT(T, x)`: Adds element $x$ (with interval $x.int$) to interval tree $T$.\n- `INTERVAL-DELETE(T, x)`: Removes element $x$ from $T$.\n- `INTERVAL-SEARCH(T, i)`: Returns a pointer to an element $x$ in $T$ such that $x.int$ overlaps interval $i$, or $T.nil$ if no such element exists.\n\n### Step 1: Underlying data structure\nA red-black tree where each node $x$ stores an interval $x.int$. The key of $x$ is $x.int.low$. An inorder walk lists intervals by sorted low endpoints.\n\n### Step 2: Additional information\nEach node $x$ contains a value $x.max$, which is the maximum value of any interval endpoint stored in the subtree rooted at $x$.\n\n### Step 3: Maintaining the information\n$x.max = \\max(x.int.high, x.left.max, x.right.max)$. (Assuming $T.nil.max = -\\infty$). This can be computed in $O(1)$ time. By Theorem 17.1, insertion and deletion take $O(\\lg n)$ time.\n\n### Step 4: Developing new operations\nThe main new operation is `INTERVAL-SEARCH(T, i)`. See [[PartV Advanced Data Structures Algorithms.md#C17.3 INTERVAL-SEARCH]].\nThe search starts at the root and proceeds downward. If the current node's interval $x.int$ overlaps query interval $i$, it's found. Otherwise, if $x.left$ is not $T.nil$ and $x.left.max \\ge i.low$, then an overlapping interval might be in the left subtree. If not, or if the left subtree cannot contain an overlapping interval (e.g., $x.left.max < i.low$), then search right (if $x.int$ doesn't overlap $i$ and the left subtree is ruled out, an overlap can only be in the right subtree if it exists).\nThe procedure takes $O(\\lg n)$ time.\n\n**Theorem 17.2**: Any execution of `INTERVAL-SEARCH(T, i)` either returns a node whose interval overlaps $i$, or it returns $T.nil$ and $T$ contains no such node.\n*Proof Idea*: The loop terminates if $x=T.nil$ or $i$ overlaps $x.int$. We need to show if it returns $T.nil$, no overlap was missed. This involves showing that if the search goes left (or right), it's because an overlap cannot exist in the other direction (or is guaranteed to be found if one exists in the chosen direction).\n- If search goes right (line 5): $x.left = T.nil$ or $x.left.max < i.low$. If $x.left=T.nil$, no interval in left subtree. If $x.left.max < i.low$, then for any $i'$ in left subtree, $i'.high \\le x.left.max < i.low$. By interval trichotomy, $i'$ and $i$ do not overlap. So, no overlap in left subtree.\n- If search goes left (line 4): $x.left \\ne T.nil$ and $x.left.max \\ge i.low$. If an interval $i^*$ in the left subtree overlaps $i$, then we are good. If not, we must show the right subtree does not contain an overlapping interval. If $x.int$ does not overlap $i$, and search goes left because $x.left.max \\ge i.low$: if $i.low > x.int.high$ ( $i$ is to the right of $x.int$), then any interval $i''$ in right subtree has $i''.low \\ge x.int.high$. This path is complex. The book focuses on: if left subtree does *not* contain an overlap for $i$, but search still went left, it means $x.left.max \text{ \textless{ } } i.low$ was false, so $x.left.max \text{ \textgreater{}= } i.low$. This implies there is an interval $i'$ in $x.left$ s.t. $i'.high = x.left.max$. If $i$ does not overlap $i'$, then $i.high < i'.low$. The keys (low endpoints) are ordered. Any interval $i''$ in $x.right$ has $x.int.low \text{ \textless{}= } i''.low$. The proof argues that $i.high < i'.low \text{ \textless{}= } x.int.low \text{ \textless{}= } i''.low$, thus $i$ cannot overlap $i''$.\n\n### Exercises 17.3\nThis section includes exercises 17.3-1 through 17.3-6.\n\n## Problems\nThis chapter includes problems 17-1 (Point of maximum overlap) and 17-2 (Josephus permutation).\n\n## Chapter notes\nPreparata and Shamos [364] describe interval trees, citing Edelsbrunner (1980) and McCreight (1981). McCreight's interval tree can enumerate $k$ overlapping intervals in $O(k + \\lg n)$ for a static database."
            },
            {
                "file": "PartV Advanced Data Structures/C18 B-Trees.md",
                "contents": "# 18 B-Trees\n\nB-trees are balanced search trees designed to work well on disk drives or other direct-access secondary storage devices. They are similar to red-black trees but are better at minimizing disk accesses. Many database systems use B-trees or variants.\n\nB-tree nodes can have many children (large \u201cbranching factor\u201d), depending on disk characteristics. Every $n$-node B-tree has height $O(\\lg n)$, but due to the larger branching factor, the base of the logarithm is larger, leading to considerably lower height than red-black trees. B-trees generalize binary search trees: an internal node $x$ with $x.n$ keys has $x.n+1$ children, and the keys serve as dividing points.\n\n## Data structures on secondary storage\n\nComputer systems use main memory (silicon chips, expensive, fast) and secondary storage (magnetic disks, SSDs; cheaper, higher capacity, slower). SSDs are faster than magnetic disks.\nMagnetic disk drives consist of platters rotating around a spindle. Heads at the end of arms read/write data. A track is the surface passing under a stationary head. Disk drives are much slower than main memory due to mechanical parts: platter rotation and arm movement (latency). For example, an 8.33ms rotation is $10^5$ times slower than 50ns memory access.\n\nTo amortize latency, disks access data in blocks (e.g., 512-4096 bytes). Performance analysis for disk-based structures considers:\n1.  The number of disk accesses.\n2.  The CPU (computing) time.\n\nDisk access count is a good first-order approximation of time. B-tree algorithms handle data larger than main memory by copying blocks to/from disk. They keep a constant number of blocks in memory. Operations `DISK-READ(x)` and `DISK-WRITE(x)` are used. A B-tree node is usually as large as a whole disk block. Large branching factors (e.g., 50 to 2000) reduce tree height and disk accesses. A B-tree of height 2 can store over a billion keys with a branching factor of 1001, requiring at most 2 disk accesses if the root is in memory.\n\n## 18.1 Definition of B-trees\n\nSatellite information associated with a key resides in the same node or is a pointer to another disk block. A B+-tree stores all satellite information in leaves, maximizing internal node branching factor.\n\nA B-tree $T$ is a rooted tree with root $T.root$ having the following properties:\n1.  Every node $x$ has attributes:\n    a.  $x.n$: the number of keys currently stored in node $x$.\n    b.  $x.key_1, x.key_2, \\dots, x.key_{x.n}$: the $x.n$ keys themselves, stored in monotonically increasing order.\n    c.  $x.leaf$: a boolean value that is TRUE if $x$ is a leaf and FALSE if $x$ is an internal node.\n2.  Each internal node $x$ also contains $x.n+1$ pointers $x.c_1, x.c_2, \\dots, x.c_{x.n+1}$ to its children. Leaf nodes have no children, so their $c_i$ attributes are undefined.\n3.  The keys $x.key_i$ separate the ranges of keys stored in each subtree: if $k_i$ is any key stored in the subtree with root $x.c_i$, then $k_1 \\le x.key_1 \\le k_2 \\le x.key_2 \\le \\dots \\le x.key_{x.n} \\le k_{x.n+1}$.\n4.  All leaves have the same depth, which is the tree's height $h$.\n5.  Nodes have lower and upper bounds on the number of keys they can contain, expressed in terms of a fixed integer $t \\ge 2$ called the **minimum degree** of the B-tree:\n    a.  Every node other than the root must have at least $t-1$ keys. Every internal node other than the root thus has at least $t$ children. If the tree is nonempty, the root must have at least one key.\n    b.  Every node may contain at most $2t-1$ keys. Therefore, an internal node may have at most $2t$ children. We say that a node is **full** if it contains exactly $2t-1$ keys.\n\nThe simplest B-tree occurs when $t=2$. Every internal node has 2, 3, or 4 children (a 2-3-4 tree). Larger $t$ values yield smaller heights.\n\n### The height of a B-tree\n\n**Theorem 18.1**: If $n \\ge 1$, then for any $n$-key B-tree $T$ of height $h$ and minimum degree $t \\ge 2$, $h \\le \\log_t \\frac{n+1}{2}$.\n*Proof*: The root has $\\ge 1$ key. It has $\\ge 2$ children (if $h>0$). Each node at depth 1 has $\\ge t-1$ keys and $\\ge t$ children. So, at depth $d$, there are at least $2t^{d-1}$ nodes. The number of keys $n$ satisfies $n \\ge 1 + (t-1) \\sum_{i=1}^{h} 2t^{i-1} = 1 + 2(t-1)(t^h-1)/(t-1) = 1 + 2(t^h-1) = 2t^h-1$. So $t^h \\le (n+1)/2$. Taking $\\log_t$ gives $h \\le \\log_t((n+1)/2)$.\nB-trees save a factor of about $\\lg t$ in node examinations (disk accesses) over red-black trees.\n\n### Exercises 18.1\nThis section includes exercises 18.1-1 through 18.1-5.\n\n## 18.2 Basic operations on B-trees\n\nConventions: The root is always in main memory. Any nodes passed as parameters have already had `DISK-READ` performed. Algorithms are \u201cone-pass,\u201d proceeding downward.\n\n### Searching a B-tree\nThis is like binary search tree search, but with $(x.n+1)$-way branching. Procedure `B-TREE-SEARCH(x, k)` returns $(y, i)$ if $y.key_i = k$, else NIL. See [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-SEARCH]].\nThis accesses $O(h) = O(\\log_t n)$ disk blocks. CPU time is $O(t)$ per node, total $O(th) = O(t \\log_t n)$.\n\n### Creating an empty B-tree\nProcedure `B-TREE-CREATE(T)` uses `ALLOCATE-NODE` (allocates one disk block, $O(1)$ time). See [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-CREATE]]. It requires $O(1)$ disk operations and $O(1)$ CPU time.\n\n### Inserting a key into a B-tree\nInsertion is more complex than for BSTs. We search for the leaf position. Instead of creating a new node, we insert into an existing leaf. If a node is full ($2t-1$ keys), it must be split. A full node $y$ is split around its median key $y.key_t$. This median key moves into $y$'s parent. If $y$'s parent is also full, it must be split before $y.key_t$ can be inserted. This can propagate up to the root.\nTo avoid backing up, we split any full node encountered on the way down the tree. This ensures that when we need to split a child, its parent is not full. Insertion is a single pass.\n\n#### Splitting a node in a B-tree\nProcedure `B-TREE-SPLIT-CHILD(x, i)` takes a nonfull internal node $x$ and an index $i$ such that $x.c_i$ is a full child of $x$. See [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-SPLIT-CHILD]].\nIt splits $x.c_i$ (call it $y$) into two nodes, $y$ and $z$. $y$ gets the smallest $t-1$ keys, $z$ gets the largest $t-1$ keys. The median key $y.key_t$ moves up into $x$. $z$ becomes a new child of $x$, immediately after $y$. The tree grows taller only when the root is split.\n`B-TREE-SPLIT-CHILD` uses $\\Theta(t)$ CPU time (for loops copying keys/children) and $O(1)$ disk operations (writes $x, y, z$).\n\n#### Inserting a key into a B-tree in a single pass down the tree\nProcedure `B-TREE-INSERT(T, k)`. See [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-INSERT]].\nIf the root $r$ is full ($r.n = 2t-1$), it's split by `B-TREE-SPLIT-ROOT(T)` which creates a new root $s$ with $s.n=0$, makes old root $r$ a child of $s$, and then calls `B-TREE-SPLIT-CHILD(s,1)` to split $r$. See [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-SPLIT-ROOT]]. This is the only way tree height increases.\nThen, `B-TREE-INSERT-NONFULL(x, k)` is called on a nonfull node $x$. See [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-INSERT-NONFULL]].\nIf $x$ is a leaf, insert $k$ into $x$. If $x$ is internal, find child $x.c_i$ where $k$ should go. If $x.c_i$ is full, call `B-TREE-SPLIT-CHILD(x,i)`, then determine which of the two new children to descend into. Recurse on the appropriate child. The guarantee is that recursion never descends to a full node.\nTotal $O(h)$ disk accesses. CPU time $O(th) = O(t \\log_t n)$. `B-TREE-INSERT-NONFULL` can be implemented iteratively, using $O(1)$ blocks in memory.\n\n### Exercises 18.2\nThis section includes exercises 18.2-1 through 18.2-7.\n\n## 18.3 Deleting a key from a B-tree\nDeletion is analogous to insertion but more complex. A key can be deleted from any node, not just a leaf. Rearranging children may be necessary. We must ensure B-tree properties are maintained, preventing nodes from becoming too small (fewer than $t-1$ keys, unless it's the root).\nProcedure `B-TREE-DELETE` combines search and deletion in a single downward pass. It guarantees that whenever it recursively calls itself on a node $x$, $x.n \\ge t$ (the minimum degree, one more key than the usual minimum $t-1$). This condition might require moving a key from a parent into a child (or merging) before recursion.\n\nConsider deleting key $k$ from subtree rooted at $x$, where $x.n \\ge t$ (unless $x$ is root).\n1.  **Case 1**: $k$ is in node $x$, and $x$ is a leaf. Simply remove $k$ from $x$.\n2.  **Case 2**: $k$ is in node $x$, and $x$ is an internal node ($k=x.key_j$).\n    a.  If child $y=x.c_j$ (preceding $k$) has $\\ge t$ keys: Find predecessor $k'$ of $k$ in subtree $y$. Recursively delete $k'$ from $y$. Replace $k$ by $k'$ in $x$. ($k'$ is largest key in $y$).\n    b.  If child $z=x.c_{j+1}$ (following $k$) has $\\ge t$ keys: Symmetric to 2a. Find successor $k'$ of $k$ in $z$. Recursively delete $k'$ from $z$. Replace $k$ by $k'$ in $x$. ($k'$ is smallest key in $z$).\n    c.  If both $y$ and $z$ have $t-1$ keys: Merge $k$ and all of $z$ into $y$. Node $x$ loses $k$ and pointer to $z$. Node $y$ now has $2t-1$ keys. Free $z$. Recursively delete $k$ from $y$.\n3.  **Case 3**: $k$ is not in internal node $x$. Determine child $x.c_j$ that must contain $k$. If $x.c_j$ has only $t-1$ keys, it needs one more key before recursion.\n    a.  If $x.c_j$ has an immediate sibling $s$ with $\\ge t$ keys: Move a key from $x$ down into $x.c_j$. Move a key from $s$ up into $x$. Move an appropriate child pointer from $s$ to $x.c_j$. Now $x.c_j$ has $t$ keys. Then recurse on $x.c_j$.\n    b.  If $x.c_j$ and all its immediate siblings have $t-1$ keys: Merge $x.c_j$ with one sibling. This involves moving a key from $x$ down into the new merged node (becoming its median key). The merged node has $2t-1$ keys. Then recurse on this merged node.\n\nIf the root node $x$ becomes empty (0 keys) as a result of Case 2c or 3b, then $x$ is deleted and its only child becomes the new root. This decreases tree height by one and preserves the property that root has at least one key (unless tree is empty).\nDeletion involves $O(h)$ disk operations and $O(th) = O(t \\log_t n)$ CPU time.\n\n### Exercises 18.3\nThis section includes exercises 18.3-1 and 18.3-2.\n\n## Problems\nThis chapter includes problems 18-1 (Stacks on secondary storage) and 18-2 (Joining and splitting 2-3-4 trees).\n\n## Chapter notes\nKnuth, Aho, Hopcroft, Ullman, Sedgewick, and Wayne provide further discussions. Comer [99] gives a comprehensive survey. Guibas and Sedgewick [202] discuss relations among balanced-tree schemes. Hopcroft invented 2-3 trees (precursor to B-trees) in 1970. Bayer and McCreight [39] introduced B-trees in 1972. Bender, Demaine, and Farach-Colton [47] studied cache-oblivious B-trees."
            },
            {
                "file": "PartV Advanced Data Structures/C19 Data Structures for Disjoint Sets.md",
                "contents": "# 19 Data Structures for Disjoint Sets\n\nSome applications involve grouping $n$ distinct elements into a collection of disjoint sets. Key operations are finding the unique set containing a given element and uniting two sets.\n\n## 19.1 Disjoint-set operations\nA disjoint-set data structure maintains a collection $S = \\{S_1, S_2, \\dots, S_k\\}$ of disjoint dynamic sets. Each set is identified by a **representative**, which is some member of the set.\nOperations:\n- `MAKE-SET(x)`: Creates a new set whose only member (and representative) is $x$. $x$ must not already be in another set.\n- `UNION(x, y)`: Unites the dynamic sets $S_x$ and $S_y$ (that contain $x$ and $y$) into a new set $S_x \\cup S_y$. $S_x$ and $S_y$ are destroyed. The representative is typically chosen from $S_x$ or $S_y$.\n- `FIND-SET(x)`: Returns a pointer to the representative of the (unique) set containing $x$.\n\nAnalysis is in terms of $n$ (number of `MAKE-SET` operations) and $m$ (total number of operations). $m \\ge n$. Since sets are disjoint, each `UNION` reduces set count by 1. At most $n-1$ `UNION` operations can occur.\n\n### An application of disjoint-set data structures\nDetermining connected components of an undirected graph $G=(V,E)$.\n`CONNECTED-COMPONENTS(G)` (see [[PartV Advanced Data Structures Algorithms.md#C19.1 CONNECTED-COMPONENTS]]): Initially, each vertex $v \\in V$ is in its own set. For each edge $(u,v) \\in E$, if $u$ and $v$ are in different sets, unite their sets.\n`SAME-COMPONENT(u,v)` (see [[PartV Advanced Data Structures Algorithms.md#C19.1 SAME-COMPONENT]]): Returns TRUE if $u$ and $v$ are in the same set (i.e., `FIND-SET(u) == FIND-SET(v)`), FALSE otherwise.\nAfter processing all edges, two vertices are in the same connected component if and only if they are in the same set. This approach is useful if edges are added dynamically.\n\n### Exercises 19.1\nThis section includes exercises 19.1-1 through 19.1-3.\n\n## 19.2 Linked-list representation of disjoint sets\nEach set is represented by its own linked list. Each set object has `head` (to first object) and `tail` (to last object). Each list object has a member, a `next` pointer, and a pointer back to the set object. The representative is the member in the `head` object.\n`MAKE-SET(x)`: Create new list with $x$. $O(1)$ time.\n`FIND-SET(x)`: Follow pointer from $x$ to set object, return member at `head`. $O(1)$ time.\n\n### A simple implementation of union\n`UNION(x, y)`: Append $y$'s list to $x$'s list. The representative of $x$'s list becomes the new representative. Update pointers to set object for all members originally in $y$'s list. This takes time linear in length of $y$'s list.\nA sequence of $m=2n-1$ operations can take $\\Theta(n^2)$ time, or $\\Theta(n)$ amortized time per operation.\n\n### A weighted-union heuristic\nAlways append the shorter list onto the longer list (breaking ties arbitrarily). Each list stores its length. A single `UNION` can still take $\\Omega(n)$ time.\n**Theorem 19.1**: A sequence of $m$ MAKE-SET, UNION, and FIND-SET operations, $n$ of which are MAKE-SET, using linked-list representation and weighted-union heuristic, takes $O(m + n \\lg n)$ time.\n*Proof Idea*: An object $x$'s pointer to its set object is updated only when $x$ is in the smaller list. Each update means the size of $x$'s new set is at least double its old set size. So, $x$'s pointer is updated at most $\\lceil \\lg n \\rceil$ times. Total time for pointer updates across all $n$ objects is $O(n \\lg n)$. MAKE-SET and FIND-SET are $O(1)$. Updating tail pointers and list lengths is $O(1)$ per UNION. Total time $O(m)$ for non-pointer-update work plus $O(n \\lg n)$ for pointer updates.\n\n### Exercises 19.2\nThis section includes exercises 19.2-1 through 19.2-6.\n\n## 19.3 Disjoint-set forests\nSets are represented by rooted trees. Each node contains one member. Each tree represents one set. Each member points only to its parent. The root of each tree is the representative and is its own parent.\n`MAKE-SET(x)`: Creates a tree with a single node $x$.\n`FIND-SET(x)`: Follows parent pointers from $x$ to the root. The path traversed is the **find path**.\n`UNION(x,y)`: Makes the root of one tree point to the root of the other.\n\n### Heuristics to improve the running time\n1.  **Union by rank**: Each node $x$ maintains $x.rank$, an upper bound on height of $x$. When uniting two trees, make the root with smaller rank point to the root with larger rank. If ranks are equal, arbitrarily choose one as parent and increment its rank.\n2.  **Path compression**: During `FIND-SET(x)`, make every node on the find path from $x$ to the root point directly to the root. This does not change ranks.\n\n### Pseudocode for disjoint-set forests\nSee [[PartV Advanced Data Structures Algorithms.md#C19.3 MAKE-SET]], [[PartV Advanced Data Structures Algorithms.md#C19.3 UNION]], [[PartV Advanced Data Structures Algorithms.md#C19.3 LINK]], [[PartV Advanced Data Structures Algorithms.md#C19.3 FIND-SET]].\n`FIND-SET` is a two-pass method: one pass up to find root, second pass down (as recursion unwinds) to update parent pointers.\n\n### Effect of the heuristics on the running time\n- Union by rank alone: $O(m \\lg n)$.\n- Path compression alone: $\\Theta(n + f \\cdot (1 + \\log_{2+f/n} n))$ for $n$ MAKE-SETs and $f$ FIND-SETs.\n- Union by rank and path compression combined: $O(m \\alpha(n))$, where $\\alpha(n)$ is a very slowly growing function (practically $\\le 4$).\n\n### Exercises 19.3\nThis section includes exercises 19.3-1 through 19.3-5.\n\n## 19.4 Analysis of union by rank with path compression\nThis section proves the $O(m \\alpha(n))$ bound using amortized analysis.\n\n### A very quickly growing function and its very slowly growing inverse\nFor integers $j,k \\ge 0$, define $A_k(j)$ as:\n$A_k(j) = j+1$ if $k=0$.\n$A_k(j) = A_{k-1}^{(j+1)}(j)$ if $k \\ge 1$, where $A_{k-1}^{(i)}(j)$ is $A_{k-1}$ applied $i$ times to $j$. The parameter $k$ is the **level**.\n**Lemma 19.2**: For $j \\ge 1$, $A_1(j) = 2j+1$.\n**Lemma 19.3**: For $j \\ge 1$, $A_2(j) = 2^{j+1}(j+1)-1$.\n$A_k(j)$ grows very rapidly. $A_0(1)=2, A_1(1)=3, A_2(1)=7, A_3(1)=2047, A_4(1) > 10^{80}$.\nThe inverse function is $\\alpha(n) = \\min \\{k : A_k(1) \\ge n\\}$.\nSo, $\\alpha(n)=0$ for $0 \\le n \\le 2$; $\\alpha(n)=1$ for $n=3$; $\\alpha(n)=2$ for $4 \\le n \\le 7$; $\\alpha(n)=3$ for $8 \\le n \\le 2047$; $\\alpha(n)=4$ for $2048 \\le n \\le A_4(1)$. For practical purposes, $\\alpha(n) \\le 4$.\n\n### Properties of ranks\n**Lemma 19.4**: For all nodes $x$, $x.rank \\le x.p.rank$, with strict inequality if $x \\ne x.p$. $x.rank$ is initially 0, increases until $x \\ne x.p$, then does not change. $x.p.rank$ monotonically increases over time.\n**Corollary 19.5**: On any simple path from a node upward to a root, node ranks strictly increase.\n**Lemma 19.6**: Every node has rank at most $n-1$. (A tighter bound is $\\lfloor \\lg n \\rfloor$.)\n\n### Proving the time bound\nWe use the potential method. Assume calls are to `MAKE-SET`, `LINK`, `FIND-SET`.\n**Lemma 19.7**: If sequence $S'$ of $m'$ ops (incl. `UNION`) is converted to $S$ of $m$ ops (incl. `LINK`, `FIND-SET`s for `UNION`), $m' \\le m \\le 3m'$. If $S$ runs in $O(m \\alpha(n))$, $S'$ runs in $O(m' \\alpha(n))$.\n\n#### Potential function\nLet $\\phi_q(x)$ be potential of node $x$ after $q$ operations. $\\Phi_q = \\sum_x \\phi_q(x)$. $\\Phi_0=0$.\nIf $x$ is a root or $x.rank=0$, then $\\phi_q(x) = \\alpha(n) \\cdot x.rank$.\nIf $x$ is not a root and $x.rank \\ge 1$:\nDefine $level(x) = \\max \\{k : x.p.rank \\ge A_k(x.rank)\\}$. We have $0 \\le level(x) < \\alpha(n)$.\nDefine $iter(x) = \\max \\{i : x.p.rank \\ge A_{level(x)}^{(i)}(x.rank)\\}$. We have $1 \\le iter(x) \\le x.rank$ (when $x.rank \\ge 1$).\nThen $\\phi_q(x) = (\\alpha(n) - level(x)) \\cdot x.rank - iter(x)$.\n\n**Lemma 19.8**: For every node $x$, $0 \\le \\phi_q(x) \\le \\alpha(n) \\cdot x.rank$.\n**Corollary 19.9**: If $x$ is not a root and $x.rank > 0$, then $\\phi_q(x) < \\alpha(n) \\cdot x.rank$.\n\n#### Potential changes and amortized costs of operations\n**Lemma 19.10**: Let $x$ be a non-root. After a `LINK` or `FIND-SET` (q-th op), $\\phi_q(x) \\le \\phi_{q-1}(x)$. If $x.rank>0$ and $level(x)$ or $iter(x)$ changes, then $\\phi_q(x) \\le \\phi_{q-1}(x)-1$. (Potential of non-root nodes does not increase; it decreases if their structural relation to parent changes meaningfully w.r.t $level/iter$.)\n\n**Lemma 19.11**: Amortized cost of `MAKE-SET` is $O(1)$.\n*Proof*: Actual cost $O(1)$. New node $x$ has $x.rank=0$, so $\\phi_q(x)=0$. No change in total potential. Amortized cost = $O(1)+0 = O(1)$.\n\n**Lemma 19.12**: Amortized cost of `LINK` is $O(\\alpha(n))$.\n*Proof*: Actual cost $O(1)$. Suppose `LINK(x,y)` makes $y$ parent of $x$. Potentials of children of $y$ (other than $x$) do not increase. $x$'s potential decreases or stays 0. $y$'s potential (it remains a root) might increase if its rank increases (by 1, if $x.rank=y.rank$). Max increase is $\\alpha(n)$. Amortized cost $O(1) + \\alpha(n) = O(\\alpha(n))$.\n\n**Lemma 19.13**: Amortized cost of `FIND-SET` is $O(\\alpha(n))$.\n*Proof Sketch*: Actual cost $O(s)$ for find path of length $s$. No node's potential increases. At least $\\max\\{0, s - (\\alpha(n)+2)\\}$ nodes on find path have their potential decrease by at least 1. (These are nodes $x$ where $x.rank>0$ and $level(x)$ is preserved by some other node $y$ further up the path before compression, allowing $iter(x)$ to increase after compression). Total potential change is $\\le -(\\max\\{0, s - (\\alpha(n)+2)\\})$. Amortized cost $\\hat{c}_q = c_q + \\Delta \\Phi_q \\le O(s) - (s - (\\alpha(n)+2)) = O(\\alpha(n)+2) = O(\\alpha(n))$, after scaling potential units.\n\n**Theorem 19.14**: A sequence of $m$ `MAKE-SET`, `UNION`, `FIND-SET` operations ($n$ `MAKE-SET`s) on a disjoint-set forest with union by rank and path compression takes $O(m \\alpha(n))$ time.\n\n### Exercises 19.4\nThis section includes exercises 19.4-1 through 19.4-7.\n\n## Problems\nThis chapter includes problems:\n- 19-1 Offline minimum\n- 19-2 Depth determination\n- 19-3 Tarjan's offline lowest-common-ancestors algorithm\n\n## Chapter notes\nKey results by Tarjan. Original bound was in terms of $\\hat{\\alpha}(m,n)$, inverse of Ackermann's function. Hopcroft and Ullman gave $O(m \\lg^* n)$. Current analysis (Section 19.4) adapted from Tarjan [431] based on Kozen [270]. Tarjan and van Leeuwen [432] discuss one-pass path compression. Fredman and Saks [155] showed $\\Omega(m \\hat{\\alpha}(m,n))$ lower bound for memory accesses."
            }
        ]
    }
]