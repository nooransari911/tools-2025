[
        {
            "file": "./backend/requirements.txt",
            "content": "fastapi\nuvicorn[standard]\npython-dotenv\npython-multipart\ngoogle-generativeai\npydantic\n# Add the path to your original project's requirements if they are not covered above\n# Example: numpy, pandas (if used indirectly by your scripts)"
        },
        {
            "file": "./backend/.env",
            "content": "# --- Gemini API Keys ---\n# You MUST provide at least one key corresponding to the API_KEY_TYPE set below\nAPI_KEY_PAID=\"YOUR_PAID_API_KEY_HERE\"\nAPI_KEY_FREE=\"YOUR_FREE_API_KEY_HERE\"\n\n# --- Model Names (Make sure these match models your keys have access to) ---\nGEMINI_20_PRO=\"gemini-1.5-pro-latest\"\nGEMINI_20_PRO_EXP=\"gemini-1.5-pro-latest\" # Or the specific free/experimental pro model if different\nGEMINI_20_FL=\"gemini-1.5-flash-latest\"\n\n# --- Backend Configuration ---\n# Set this to 'paid' or 'free' to determine which API key the backend uses by default\n# The frontend's model selection (pro/flash) will be combined with this key type.\nBACKEND_API_KEY_TYPE=\"paid\"\n\n# --- Paths to Instructions (Relative to the location of the *original* script's execution, or absolute) ---\n# These paths need to be accessible from where the backend process runs.\n# Adjust these paths based on the *relative location* of your original 'Knowledge Base' project\n# from the 'AI_doc_processor_webapp/backend' directory, OR use absolute paths.\n# Example relative path if 'Knowledge Base' is a sibling to 'AI_doc_processor_webapp':\n# SYSTEM_INSTRUCTIONS_PATH=\"../Knowledge Base/data/system_instructions.txt\"\n# SYSTEM_INSTRUCTIONS_STRUCTURED_PATH=\"../Knowledge Base/data/system_instructions_structured.txt\"\n# --- !! IMPORTANT !! Adjust these paths based on your actual directory structure --- \nSYSTEM_INSTRUCTIONS_PATH=\"./path/to/your/original/project/data/system_instructions.txt\"\nSYSTEM_INSTRUCTIONS_STRUCTURED_PATH=\"./path/to/your/original/project/data/system_instructions_structured.txt\"\n\n# --- Path to the original AI processing script's directory (relative or absolute) ---\n# Used by the backend to add the 'src' and 'data' directories to sys.path\n# Example relative path if 'Knowledge Base' is a sibling to 'AI_doc_processor_webapp':\n# ORIGINAL_PROJECT_PATH=\"../Knowledge Base\"\n# --- !! IMPORTANT !! Adjust this path based on your actual directory structure --- \nORIGINAL_PROJECT_PATH=\"./path/to/your/original/project\" \n\n# --- Logging ---\nLOG_FILE=\"./backend_app.log\"\nLOG_LEVEL=\"INFO\""
        },
        {
            "file": "./backend/main.py",
            "content": "import os\nimport sys\nimport logging\nimport tempfile\nimport pathlib\nimport mimetypes\nfrom typing import Optional, List, Dict, Any, Tuple\n\nfrom fastapi import FastAPI, File, UploadFile, Form, HTTPException, Request\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom dotenv import load_dotenv\nfrom pydantic import BaseModel, Field\nimport google.api_core.exceptions\nfrom google.generativeai import types as genai_types # Use alias to avoid conflict\n\n# --- Environment & Path Setup --- \n\n# Load environment variables from .env file in the backend directory\nload_dotenv()\n\n# Get the path to the original project from .env\nORIGINAL_PROJECT_ROOT = os.getenv(\"ORIGINAL_PROJECT_PATH\")\nif not ORIGINAL_PROJECT_ROOT or not os.path.isdir(ORIGINAL_PROJECT_ROOT):\n    print(f\"Error: ORIGINAL_PROJECT_PATH environment variable not set or invalid: {ORIGINAL_PROJECT_ROOT}\", file=sys.stderr)\n    sys.exit(1)\n\n# Add the 'src' and 'data' directories from the original project to sys.path\nSRC_PATH = os.path.join(ORIGINAL_PROJECT_ROOT, 'src')\nDATA_PATH = os.path.join(ORIGINAL_PROJECT_ROOT, 'data')\n\nif not os.path.isdir(SRC_PATH):\n    print(f\"Error: Cannot find 'src' directory at: {SRC_PATH}\", file=sys.stderr)\n    sys.exit(1)\n# Data path might not be strictly necessary for imports but good practice if used directly\n# if not os.path.isdir(DATA_PATH):\n#     print(f\"Warning: Cannot find 'data' directory at: {DATA_PATH}\", file=sys.stderr)\n\nsys.path.insert(0, ORIGINAL_PROJECT_ROOT) # Add project root first\nsys.path.insert(1, SRC_PATH)              # Then src\n\n# --- Import from Original Project --- \ntry:\n    from utils import gemini_utils\n    # Schemas are registered when gemini_utils imports data modules\n    # Ensure the schema files exist and are importable relative to src_path\nexcept ImportError as e:\n    print(f\"Error importing modules from '{SRC_PATH}'. Check paths and dependencies. Error: {e}\", file=sys.stderr)\n    sys.exit(1)\nexcept Exception as e:\n    print(f\"An unexpected error occurred during initial imports: {e}\", file=sys.stderr)\n    sys.exit(1)\n\n# --- Logging Setup --- \nLOG_FILE = os.getenv(\"LOG_FILE\", \"./backend_app.log\")\nLOG_LEVEL = os.getenv(\"LOG_LEVEL\", \"INFO\").upper()\nlogging.basicConfig(\n    level=getattr(logging, LOG_LEVEL, logging.INFO),\n    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n    handlers=[\n        logging.FileHandler(LOG_FILE, encoding='utf-8'),\n        logging.StreamHandler(sys.stdout) # Also log to console\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# --- Backend Configuration --- \nBACKEND_API_KEY_TYPE = os.getenv(\"BACKEND_API_KEY_TYPE\", \"free\").lower()\nif BACKEND_API_KEY_TYPE not in ['free', 'paid']:\n    logger.warning(f\"Invalid BACKEND_API_KEY_TYPE '{BACKEND_API_KEY_TYPE}'. Defaulting to 'free'.\")\n    BACKEND_API_KEY_TYPE = 'free'\nlogger.info(f\"Backend configured to use API Key Type: {BACKEND_API_KEY_TYPE}\")\n\n# --- FastAPI App Setup --- \napp = FastAPI(title=\"AI Document Processor API\")\n\n# --- CORS Middleware --- \n# Allow requests from your frontend development server and production domain\n# Adjust origins as needed for your deployment\norigins = [\n    \"http://localhost:3000\", # Default React dev server port\n    \"http://127.0.0.1:3000\",\n    # Add your production frontend URL here if deploying\n    # \"https://your-frontend-domain.com\",\n]\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=origins,\n    allow_credentials=True,\n    allow_methods=[\"*\"], # Allows all methods (GET, POST, etc.)\n    allow_headers=[\"*\"], # Allows all headers\n)\n\n# --- Pydantic Models for API --- \nclass SchemaListResponse(BaseModel):\n    schemas: List[str]\n\nclass UsageMetadata(BaseModel):\n    prompt_token_count: Optional[int] = None\n    candidates_token_count: Optional[int] = None\n    total_token_count: Optional[int] = None\n\nclass ProcessResponse(BaseModel):\n    status: str = Field(..., description=\"'success' or 'error'\")\n    raw_output: Optional[str] = Field(None, description=\"The raw text output from the AI model.\")\n    structured_output: Optional[Any] = Field(None, description=\"Parsed JSON output if a schema was used and parsing succeeded.\")\n    schema_used: Optional[str] = Field(None, description=\"Name of the schema requested, or indication if none was used.\")\n    usage_metadata: Optional[UsageMetadata] = Field(None, description=\"Token usage information.\")\n    error_message: Optional[str] = Field(None, description=\"Details if status is 'error' or for non-fatal warnings.\")\n\n# --- Helper Function for Processing (Synchronous) --- \ndef run_single_processing(\n    input_file_path: str,\n    prompt: str,\n    schema_name: Optional[str],\n    model_type: str, # 'pro' or 'flash' from frontend\n    api_key_type: str # Determined by backend (e.g., from env)\n) -> Dict[str, Any]:\n    \"\"\"Synchronously processes a single document using Gemini utils.\"\"\"\n    result = {\n        \"status\": \"error\",\n        \"raw_output\": None,\n        \"structured_output\": None,\n        \"schema_used\": schema_name if schema_name else \"(No Schema - Plain Text)\",\n        \"usage_metadata\": None,\n        \"error_message\": None,\n    }\n    try:\n        logger.info(f\"Starting processing for file: {os.path.basename(input_file_path)}, model: {model_type}, schema: {schema_name}\")\n\n        # 1. Configure Gemini Client (using imported function)\n        client, model_name_resolved = gemini_utils.configure_gemini(model_type, api_key_type)\n        logger.info(f\"Using Gemini Model: {model_name_resolved}\")\n\n        # 2. Load Input Content (Handling different types)\n        input_part = None\n        p_file_path = pathlib.Path(input_file_path)\n        file_size = p_file_path.stat().st_size\n        logger.info(f\"Input file size: {file_size} bytes\")\n\n        # Add more robust mime type guessing if needed\n        mime_type, _ = mimetypes.guess_type(input_file_path)\n        logger.info(f\"Guessed MIME type: {mime_type}\")\n\n        # Define explicitly supported text-based extensions\n        text_extensions = ['.txt', '.md', '.json', '.html', '.css', '.js', '.py', '.java', '.c', '.cpp', '.h', '.hpp', '.xml', '.csv', '.log']\n\n        if mime_type == 'application/pdf':\n            input_part = genai_types.Part.from_bytes(data=p_file_path.read_bytes(), mime_type='application/pdf')\n        elif mime_type and mime_type.startswith('image/'):\n            input_part = genai_types.Part.from_bytes(data=p_file_path.read_bytes(), mime_type=mime_type)\n        elif (mime_type and mime_type.startswith('text/')) or \\\n             mime_type in ['application/json', 'application/markdown'] or \\\n             p_file_path.suffix.lower() in text_extensions:\n            try:\n                input_part = p_file_path.read_text(encoding='utf-8')\n            except UnicodeDecodeError:\n                logger.warning(f\"UTF-8 decoding failed for {input_file_path}. Trying latin-1.\")\n                try:\n                    input_part = p_file_path.read_text(encoding='latin-1')\n                except Exception as enc_err:\n                     logger.error(f\"Failed to read text file {input_file_path} with fallback encoding: {enc_err}\")\n                     raise ValueError(f\"Could not read text file: {os.path.basename(input_file_path)}\")\n            except Exception as read_err:\n                 logger.error(f\"Error reading text file {input_file_path}: {read_err}\")\n                 raise ValueError(f\"Error reading file: {os.path.basename(input_file_path)}\")\n        else:\n            logger.warning(f\"Unsupported file type based on MIME '{mime_type}' or extension '{p_file_path.suffix}'. Rejecting.\")\n            raise ValueError(f\"Unsupported file type: {mime_type or p_file_path.suffix}\")\n\n        if not input_part:\n            raise ValueError(\"Could not read or load input file content after processing type.\")\n\n        contents = [input_part, prompt]\n        logger.info(f\"Prepared content for Gemini: type={type(input_part)}, prompt length={len(prompt)}\")\n\n        # 3. Determine Structured Mode & Schema Class\n        is_structured_mode = bool(schema_name)\n        schema_class = None\n        if is_structured_mode:\n            schema_class = gemini_utils.resolve_schema_class(schema_name)\n            if not schema_class:\n                logger.warning(f\"Schema '{schema_name}' selected but not resolved by gemini_utils. Proceeding without structure.\")\n                is_structured_mode = False\n                result[\"schema_used\"] = f\"{schema_name} (Not Found)\"\n                # Add a non-fatal warning to the response message\n                result[\"error_message\"] = f\"Warning: Schema '{schema_name}' could not be loaded. Output is raw text.\"\n\n        # 4. Load System Instructions\n        sys_instructions = gemini_utils.load_system_instructions(is_structured_mode)\n        logger.info(f\"System instructions loaded (length: {len(sys_instructions)}). Structured Mode: {is_structured_mode}\")\n\n        # 5. Prepare Generation Config\n        gen_config_args = {\"system_instruction\": sys_instructions, \"max_output_tokens\": 8192}\n        if is_structured_mode and schema_class:\n            gen_config_args[\"response_mime_type\"] = \"application/json\"\n            gen_config_args[\"response_schema\"] = schema_class\n            logger.info(f\"Requesting JSON output with schema: {schema_class.__name__}\")\n        else:\n            logger.info(\"Requesting plain text output.\")\n\n        generation_config = genai_types.GenerateContentConfig(**gen_config_args)\n\n        # 6. Call Gemini API (Synchronously via the utility function)\n        logger.info(\"Sending request to Gemini API...\")\n        start_time = time.time()\n        response = gemini_utils.generate_gemini_content(\n            genai_client=client,\n            model_name=model_name_resolved,\n            contents=contents,\n            generation_config=generation_config,\n            is_structured_mode=is_structured_mode\n        )\n        end_time = time.time()\n        logger.info(f\"Gemini API call finished in {end_time - start_time:.2f} seconds.\")\n\n        # 7. Process Response\n        result[\"status\"] = \"success\"\n        result[\"raw_output\"] = response.text\n\n        if response.usage_metadata:\n            result[\"usage_metadata\"] = {\n                \"prompt_token_count\": response.usage_metadata.prompt_token_count,\n                \"candidates_token_count\": response.usage_metadata.candidates_token_count,\n                \"total_token_count\": response.usage_metadata.total_token_count,\n            }\n            logger.info(f\"Usage - In: {result['usage_metadata']['prompt_token_count']}, Out: {result['usage_metadata']['candidates_token_count']}, Total: {result['usage_metadata']['total_token_count']}\")\n        else:\n            logger.warning(\"No usage metadata received from Gemini API.\")\n\n        # 8. Attempt to parse structured output if applicable\n        if is_structured_mode:\n            logger.info(\"Attempting to parse structured JSON from response.\")\n            parsed_json = gemini_utils.parse_json_from_response_text(response.text)\n            if parsed_json is not None:\n                result[\"structured_output\"] = parsed_json\n                logger.info(\"Successfully parsed structured JSON.\")\n            else:\n                logger.warning(\"Failed to parse structured JSON from the response text.\")\n                # Append warning if structure was expected but not parsed\n                existing_warning = result[\"error_message\"] or \"\"\n                result[\"error_message\"] = (existing_warning + \" Warning: Failed to parse structured JSON from the AI response.\").strip()\n\n        logger.info(\"Processing successful.\")\n\n    except google.api_core.exceptions.GoogleAPIError as e:\n        logger.error(f\"Gemini API Error during processing: {e}\", exc_info=False) # Log full trace if needed\n        result[\"status\"] = \"error\"\n        # Provide a slightly more user-friendly message\n        error_detail = str(e)\n        if hasattr(e, 'message'):\n             error_detail = e.message\n        result[\"error_message\"] = f\"Gemini API Error: {error_detail}\" # e.g., Resource exhausted, Invalid API Key\n    except ValueError as e:\n        logger.error(f\"Value Error during processing step: {e}\", exc_info=True)\n        result[\"status\"] = \"error\"\n        result[\"error_message\"] = f\"Processing Error: {e}\" # e.g., Unsupported file type\n    except FileNotFoundError as e:\n         logger.error(f\"File Not Found Error: {e}\", exc_info=True)\n         result[\"status\"] = \"error\"\n         result[\"error_message\"] = f\"Server Error: Could not find temporary file.\"\n    except PermissionError as e:\n         logger.error(f\"Permission Error accessing file: {e}\", exc_info=True)\n         result[\"status\"] = \"error\"\n         result[\"error_message\"] = f\"Server Error: File permission issue.\"\n    except Exception as e:\n        logger.exception(\"An unexpected error occurred during processing.\") # Log full trace for unexpected errors\n        result[\"status\"] = \"error\"\n        result[\"error_message\"] = f\"An unexpected server error occurred. Please check server logs.\" # Keep it generic for user\n\n    return result\n\n# --- API Endpoints --- \n\n@app.get(\"/api/schemas\", response_model=SchemaListResponse)\nasync def get_schemas():\n    \"\"\"Returns a list of available schema names.\"\"\"\n    logger.info(\"Request received for /api/schemas\")\n    try:\n        # Access the registry populated by gemini_utils imports\n        schema_names = list(gemini_utils.SCHEMA_REGISTRY.keys())\n        logger.info(f\"Found schemas: {schema_names}\")\n        # Ensure the default schema is listed if registry isn't empty\n        if schema_names and gemini_utils.DEFAULT_SCHEMA_NAME not in schema_names:\n             logger.warning(f\"Default schema '{gemini_utils.DEFAULT_SCHEMA_NAME}' missing from registry keys.\")\n        # You might want to always ensure the default schema is present or handle its absence\n        return SchemaListResponse(schemas=schema_names)\n    except Exception as e:\n        logger.exception(\"Error fetching schemas from registry.\")\n        raise HTTPException(status_code=500, detail=\"Failed to load available schemas.\")\n\n@app.post(\"/api/process\", response_model=ProcessResponse)\ndef process_document(\n    file: UploadFile = File(...),\n    prompt: str = Form(...),\n    schema_name: Optional[str] = Form(None), # Allow schema to be optional\n    model_type: str = Form(...) # 'pro' or 'flash'\n):\n    \"\"\"Receives a document, prompt, schema, and model type, processes it, and returns results.\"\"\"\n    logger.info(f\"Request received for /api/process: filename='{file.filename}', schema='{schema_name}', model='{model_type}'\")\n\n    # Input validation\n    if not file.filename:\n         logger.error(\"File upload missing filename.\")\n         raise HTTPException(status_code=400, detail=\"No file name provided.\")\n    if model_type not in ['pro', 'flash']:\n         logger.error(f\"Invalid model_type received: {model_type}\")\n         raise HTTPException(status_code=400, detail=\"Invalid model type specified. Must be 'pro' or 'flash'.\")\n    if not prompt.strip():\n        logger.error(\"Empty prompt received.\")\n        raise HTTPException(status_code=400, detail=\"Prompt cannot be empty.\")\n\n    # Use a temporary file to store the upload\n    temp_file_path = None\n    try:\n        # Create a temporary directory if it doesn't exist\n        temp_dir = pathlib.Path(\"./temp_uploads\")\n        temp_dir.mkdir(exist_ok=True)\n\n        # Sanitize filename slightly (more robust sanitization might be needed)\n        safe_filename = pathlib.Path(file.filename).name\n        temp_file_path = temp_dir / safe_filename\n\n        logger.info(f\"Saving uploaded file to temporary path: {temp_file_path}\")\n        with open(temp_file_path, \"wb\") as buffer:\n            buffer.write(file.file.read())\n        logger.info(f\"Temporary file saved successfully.\")\n\n        # Call the synchronous processing function\n        # The API endpoint remains synchronous ('def' not 'async def')\n        # FastAPI handles running this in a thread pool\n        import time # Import time inside function if not already global\n        processing_start_time = time.time()\n        result_dict = run_single_processing(\n            input_file_path=str(temp_file_path),\n            prompt=prompt,\n            schema_name=schema_name if schema_name else None, # Pass None if empty string\n            model_type=model_type,\n            api_key_type=BACKEND_API_KEY_TYPE # Use backend-configured key type\n        )\n        processing_end_time = time.time()\n        logger.info(f\"run_single_processing completed in {processing_end_time - processing_start_time:.2f} seconds.\")\n\n        # Return the result using the Pydantic model for validation and serialization\n        # If result_dict structure matches ProcessResponse, this works directly\n        # Handle potential internal server errors during processing explicitly\n        if result_dict['status'] == 'error' and not result_dict.get('error_message'):\n             result_dict['error_message'] = 'An internal server error occurred during processing.'\n\n        # Return the result dictionary directly, FastAPI will serialize it based on response_model\n        # Use JSONResponse only if you need custom status codes based on result['status']\n        status_code = 200 if result_dict['status'] == 'success' else 500 # Or 4xx for specific client errors caught inside\n        # If using specific HTTP status codes based on processing outcome:\n        # return JSONResponse(content=result_dict, status_code=status_code)\n        # Otherwise, letting FastAPI handle it with response_model is cleaner:\n        return result_dict\n\n    except HTTPException as http_exc:\n        # Re-raise client-side errors (4xx)\n        raise http_exc\n    except Exception as e:\n        logger.exception(\"Unhandled error in /api/process endpoint wrapper.\")\n        # Return a generic 500 error response\n        return JSONResponse(\n             content=ProcessResponse(\n                 status=\"error\",\n                 error_message=\"An unexpected error occurred on the server while handling the request.\"\n             ).model_dump(), # Use model_dump() for Pydantic v2\n             status_code=500\n         )\n    finally:\n        # --- Cleanup: Remove the temporary file --- \n        if temp_file_path and os.path.exists(temp_file_path):\n            try:\n                os.remove(temp_file_path)\n                logger.info(f\"Successfully removed temporary file: {temp_file_path}\")\n            except OSError as e:\n                logger.error(f\"Error removing temporary file {temp_file_path}: {e}\")\n        # Clean up the temporary file object if needed\n        if file:\n            try:\n                 file.file.close()\n            except Exception: pass # Ignore errors during close\n\n\n# --- Root Endpoint (Optional) --- \n@app.get(\"/\")\nasync def read_root():\n    return {\"message\": \"AI Document Processor API is running.\"}\n\n# --- Run with Uvicorn (for local development) --- \nif __name__ == \"__main__\":\n    import uvicorn\n    logger.info(\"Starting Uvicorn server for local development...\")\n    # Ensure necessary env vars are loaded before configure_gemini is potentially called\n    # (already done by load_dotenv() at the top)\n    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000, reload=True)"
        }
]
