[
    {
        "items": [
            {
                "file": "PartV Advanced Data Structures/PartV Introduction.md",
                "contents": "# Part V Advanced Data Structures\n\n## Introduction\nThis part returns to studying data structures that support operations on dynamic sets, but at a more advanced level than Part III. One of the chapters, for example, makes extensive use of the amortized analysis techniques from Chapter 16.\n\nChapter 17 shows how to augment red-black trees\u2014adding additional information in each node\u2014to support dynamic-set operations in addition to those covered in Chapters 12 and 13. The first example augments red-black trees to dynamically maintain order statistics for a set of keys. Another example augments them in a different way to maintain intervals of real numbers. Chapter 17 includes a theorem giving sufficient conditions for when a red-black tree can be augmented while maintaining the $O(\\lg n)$ running times for insertion and deletion.\n\nChapter 18 presents B-trees, which are balanced search trees specifically designed to be stored on disks. Since disks operate much more slowly than random-access memory, B-tree performance depends not only on how much computing time the dynamic-set operations consume but also on how many disk accesses they perform. For each B-tree operation, the number of disk accesses increases with the height of the B-tree, but B-tree operations keep the height low.\n\nChapter 19 examines data structures for disjoint sets. Starting with a universe of $n$ elements, each initially in its own singleton set, the operation UNION unites two sets. At all times, the $n$ elements are partitioned into disjoint sets, even as calls to the UNION operation change the members of a set dynamically. The query FIND-SET identifies the unique set that contains a given element at the moment. Representing each set as a simple rooted tree yields surprisingly fast operations: a sequence of $m$ operations runs in $O(m \\alpha(n))$ time, where $\\alpha(n)$ is an incredibly slowly growing function\u2014$\\alpha(n)$ is at most 4 in any conceivable application. The amortized analysis that proves this time bound is as complex as the data structure is simple.\n\n## Other Advanced Data Structures\nThe topics covered in this part are by no means the only examples of \u201cadvanced\" data structures. Other advanced data structures include the following:\n\n-   **Fibonacci heaps** [156] implement mergeable heaps (see Problem 10-2 on page 268) with the operations INSERT, MINIMUM, and UNION taking only $O(1)$ actual and amortized time, and the operations EXTRACT-MIN and DELETE taking $O(\\lg n)$ amortized time. The most significant advantage of these data structures, however, is that DECREASE-KEY takes only $O(1)$ amortized time. **Strict Fibonacci heaps** [73], developed later, made all of these time bounds actual. Because the DECREASE-KEY operation takes constant amortized time, (strict) Fibonacci heaps constitute key components of some of the asymptotically fastest algorithms to date for graph problems.\n\n-   **Dynamic trees** [415, 429] maintain a forest of disjoint rooted trees. Each edge in each tree has a real-valued cost. Dynamic trees support queries to find parents, roots, edge costs, and the minimum edge cost on a simple path from a node up to a root. Trees may be manipulated by cutting edges, updating all edge costs on a simple path from a node up to a root, linking a root into another tree, and making a node the root of the tree it appears in. One implementation of dynamic trees gives an $O(\\lg n)$ amortized time bound for each operation, while a more complicated implementation yields $O(\\lg n)$ worst-case time bounds. Dynamic trees are used in some of the asymptotically fastest network-flow algorithms.\n\n-   **Splay trees** [418, 429] are a form of binary search tree on which the standard search-tree operations run in $O(\\lg n)$ amortized time. One application of splay trees simplifies dynamic trees.\n\n-   **Persistent data structures** allow queries, and sometimes updates as well, on past versions of a data structure. For example, linked data structures can be made persistent with only a small time and space cost [126]. Problem 13-1 gives a simple example of a persistent dynamic set.\n\n-   Several data structures allow a faster implementation of dictionary operations (INSERT, DELETE, and SEARCH) for a restricted universe of keys. By taking advantage of these restrictions, they are able to achieve better worst-case asymptotic running times than comparison-based data structures. If the keys are unique integers drawn from the set \\{0, 1, 2, ..., $u-1$\\}, where $u$ is an exact power of 2, then a recursive data structure known as a **van Emde Boas tree** [440, 441] supports each of the operations SEARCH, INSERT, DELETE, MINIMUM, MAXIMUM, SUCCESSOR, and PREDECESSOR in $O(\\lg \\lg u)$ time. **Fusion trees** [157] were the first data structure to allow faster dictionary operations when the universe is restricted to integers, implementing these operations in $O(\\lg n / \\lg \\lg n)$ time. Several subsequent data structures, including **exponential search trees** [17], have also given improved bounds on some or all of the dictionary operations and are mentioned in the chapter notes throughout this book.\n\n-   **Dynamic graph data structures** support various queries while allowing the structure of a graph to change through operations that insert or delete vertices or edges. Examples of the queries that they support include vertex connectivity [214], edge connectivity, minimum spanning trees [213], biconnectivity, and transitive closure [212].\n\nChapter notes throughout this book mention additional data structures."
            },
            {
                "file": "PartV Advanced Data Structures Algorithms.md",
                "contents": "# C17 Augmenting Data Structures Algorithms\n\n## 17.1 OS-SELECT(x, i)\n```\nOS-SELECT(x, i)\n  1  r = x.left.size + 1  // rank of x within the subtree rooted at x\n  2  if i == r\n  3      return x\n  4  elseif i < r\n  5      return OS-SELECT(x.left, i)\n  6  else return OS-SELECT(x.right, i - r)\n```\n\n## 17.1 OS-RANK(T, x)\n```\nOS-RANK(T, x)\n  1  r = x.left.size + 1  // rank of x within the subtree rooted at x\n  2  y = x                // root of subtree being examined\n  3  while y != T.root\n  4      if y == y.p.right\n  5          r = r + y.p.left.size + 1  // if root of a right subtree ...\n                                     // ... add in parent and its left subtree\n  6      y = y.p            // move y toward the root\n  7  return r\n```\n\n## 17.1 LEFT-ROTATE Size Update\nTo be added to `LEFT-ROTATE(T, x)` from page 336:\n```\n13  y.size = x.size\n14  x.size = x.left.size + x.right.size + 1\n```\n(A corresponding update is needed for `RIGHT-ROTATE`.)\n\n## 17.3 INTERVAL-SEARCH(T, i)\n```\nINTERVAL-SEARCH(T, i)\n  1  x = T.root\n  2  while x != T.nil and i does not overlap x.int\n  3      if x.left != T.nil and x.left.max >= i.low\n  4          x = x.left  // overlap in left subtree or no overlap in right subtree\n  5      else x = x.right // no overlap in left subtree\n  6  return x\n```\n\n# C18 B-Trees Algorithms\n\n## Conceptual DISK-READ(x) and DISK-WRITE(x)\n- `DISK-READ(x)`: Reads the block containing object `x` from disk into main memory. If `x` is already in main memory, it's a no-op.\n- `DISK-WRITE(x)`: Writes the block containing object `x` (with any modifications) from main memory back to disk.\n\n## 18.2 B-TREE-SEARCH(x, k)\n```\nB-TREE-SEARCH(x, k)\n  1  i = 1\n  2  while i <= x.n and k > x.key_i\n  3      i = i + 1\n  4  if i <= x.n and k == x.key_i\n  5      return (x, i)\n  6  elseif x.leaf\n  7      return NIL\n  8  else DISK-READ(x.c_i)\n  9      return B-TREE-SEARCH(x.c_i, k)\n```\n\n## 18.2 B-TREE-CREATE(T)\n```\nB-TREE-CREATE(T)\n  1  x = ALLOCATE-NODE()\n  2  x.leaf = TRUE\n  3  x.n = 0\n  4  DISK-WRITE(x)\n  5  T.root = x\n```\n\n## 18.2 B-TREE-SPLIT-CHILD(x, i)\n(x is a nonfull internal node, x.c_i is a full child of x)\n```\nB-TREE-SPLIT-CHILD(x, i)\n  1  y = x.c_i\n  2  z = ALLOCATE-NODE()\n  3  z.leaf = y.leaf\n  4  z.n = t - 1\n  5  for j = 1 to t - 1\n  6      z.key_j = y.key_{j+t}\n  7  if not y.leaf\n  8      for j = 1 to t\n  9          z.c_j = y.c_{j+t}\n 10  y.n = t - 1\n 11  for j = x.n + 1 downto i + 1\n 12      x.c_{j+1} = x.c_j\n 13  x.c_{i+1} = z\n 14  for j = x.n downto i\n 15      x.key_{j+1} = x.key_j\n 16  x.key_i = y.key_t\n 17  x.n = x.n + 1\n 18  DISK-WRITE(y)\n 19  DISK-WRITE(z)\n 20  DISK-WRITE(x)\n```\n\n## 18.2 B-TREE-INSERT(T, k)\n```\nB-TREE-INSERT(T, k)\n  1  r = T.root\n  2  if r.n == 2*t - 1\n  3      s = ALLOCATE-NODE()\n  4      T.root = s\n  5      s.leaf = FALSE\n  6      s.n = 0\n  7      s.c_1 = r\n  8      B-TREE-SPLIT-CHILD(s, 1) // Note: CLRS text has B-TREE-SPLIT-ROOT(T) which implicitly does this\n                                 // and then calls B-TREE-INSERT-NONFULL on the new root. Original text in OCR is:\n                                 // s = B-TREE-SPLIT-ROOT(T)\n                                 // B-TREE-INSERT-NONFULL(s,k)\n                                 // I will use the OCR version's logic for B-TREE-SPLIT-ROOT directly\n  // Corrected structure based on B-TREE-SPLIT-ROOT usage:\n  1  r = T.root\n  2  if r.n == 2*t - 1\n  3      s = B-TREE-SPLIT-ROOT(T) // s is the new root\n  4      B-TREE-INSERT-NONFULL(s, k)\n  5  else B-TREE-INSERT-NONFULL(r, k)\n```\n\n## 18.2 B-TREE-SPLIT-ROOT(T)\n```\nB-TREE-SPLIT-ROOT(T)\n  1  s = ALLOCATE-NODE()\n  2  s.leaf = FALSE\n  3  s.n = 0\n  4  s.c_1 = T.root\n  5  T.root = s\n  6  B-TREE-SPLIT-CHILD(s, 1)\n  7  return s\n```\n\n## 18.2 B-TREE-INSERT-NONFULL(x, k)\n```\nB-TREE-INSERT-NONFULL(x, k)\n  1  i = x.n\n  2  if x.leaf\n  3      while i >= 1 and k < x.key_i\n  4          x.key_{i+1} = x.key_i\n  5          i = i - 1\n  6      x.key_{i+1} = k\n  7      x.n = x.n + 1\n  8      DISK-WRITE(x)\n  9  else while i >= 1 and k < x.key_i\n 10          i = i - 1\n 11      i = i + 1\n 12      DISK-READ(x.c_i)\n 13      if x.c_i.n == 2*t - 1\n 14          B-TREE-SPLIT-CHILD(x, i)\n 15          if k > x.key_i\n 16              i = i + 1\n 17      B-TREE-INSERT-NONFULL(x.c_i, k)\n```\n\n# C19 Data Structures for Disjoint Sets Algorithms\n\n## 19.1 CONNECTED-COMPONENTS(G)\n```\nCONNECTED-COMPONENTS(G)\n  1  for each vertex v in G.V\n  2      MAKE-SET(v)\n  3  for each edge (u,v) in G.E\n  4      if FIND-SET(u) != FIND-SET(v)\n  5          UNION(u,v)\n```\n\n## 19.1 SAME-COMPONENT(u,v)\n```\nSAME-COMPONENT(u,v)\n  1  if FIND-SET(u) == FIND-SET(v)\n  2      return TRUE\n  3  else return FALSE\n```\n\n## 19.2 MAKE-SET Linked-List(x)\n(Conceptual, creates a new list for x)\n\n## 19.2 FIND-SET Linked-List(x)\n(Conceptual, returns representative from x's set object)\n\n## 19.2 UNION Linked-List(x,y)\n(Conceptual, appends y's list to x's, updates pointers in y's list. With weighted-union, appends shorter to longer.)\n\n## 19.3 MAKE-SET Forest(x)\n```\nMAKE-SET(x)\n  1  x.p = x\n  2  x.rank = 0\n```\n\n## 19.3 UNION Forest(x,y)\n```\nUNION(x,y)\n  1  LINK(FIND-SET(x), FIND-SET(y))\n```\n\n## 19.3 LINK Forest(x,y)\n(x and y are roots)\n```\nLINK(x,y)\n  1  if x.rank > y.rank\n  2      y.p = x\n  3  else x.p = y\n  4      if x.rank == y.rank\n  5          y.rank = y.rank + 1\n```\n\n## 19.3 FIND-SET Forest(x)\n```\nFIND-SET(x)\n  1  if x != x.p\n  2      x.p = FIND-SET(x.p)\n  3  return x.p\n```\n\n## C19.P1 OFFLINE-MINIMUM(m,n)\n(K_j are sets of keys)\n```\nOFFLINE-MINIMUM(m,n)\n  1  for i = 1 to n\n  2      determine j such that i is in K_j\n  3      if j != m + 1\n  4          extracted[j] = i\n  5          let l be the smallest value greater than j for which set K_l exists\n  6          K_l = K_j U K_l, destroying K_j  // (This is a conceptual UNION of sets of numbers)\n  7  return extracted\n```\n\n## C19.P3 LCA(u)\n(Assumes FIND-SET(u).ancestor attribute on set representatives)\n```\nLCA(u)\n  1  MAKE-SET(u)\n  2  FIND-SET(u).ancestor = u\n  3  for each child v of u in T\n  4      LCA(v)\n  5      UNION(u,v)\n  6      FIND-SET(u).ancestor = u\n  7  u.color = BLACK\n  8  for each node v such that {u,v} is in P // P is the set of pairs\n  9      if v.color == BLACK\n 10          print \"The lowest common ancestor of\" u \"and\" v \"is\" FIND-SET(v).ancestor\n```"
            },
            {
                "file": "PartV Advanced Data Structures/C17 Augmenting Data Structures.md",
                "contents": "# 17 Augmenting Data Structures\n\nSome solutions require no more than a \u201ctextbook\u201d data structure, but many others require creativity. Rarely will you need to create an entirely new type of data structure. More often, you can augment a textbook data structure by storing additional information in it. You can then program new operations for the data structure to support your application. Augmenting a data structure is not always straightforward, however, since the added information must be updated and maintained by the ordinary operations on the data structure.\n\nThis chapter discusses two data structures based on red-black trees that are augmented with additional information. Section 17.1 describes a data structure that supports general order-statistic operations on a dynamic set: quickly finding the $i$-th smallest number or the rank of a given element. Section 17.2 abstracts the process of augmenting a data structure and provides a theorem for augmenting red-black trees. Section 17.3 uses this theorem to help design a data structure for maintaining a dynamic set of intervals, such as time intervals, allowing quick discovery of an interval that overlaps a given query interval.\n\n## 17.1 Dynamic order statistics\nChapter 9 introduced the notion of an order statistic. Specifically, the $i$-th order statistic of a set of $n$ elements, where $i \\in \\{1, 2, ..., n\\}$, is simply the element in the set with the $i$-th smallest key. This section shows how to modify red-black trees so that you can determine any order statistic for a dynamic set in $O(\\lg n)$ time and also compute the rank of an element\u2014its position in the linear order of the set\u2014in $O(\\lg n)$ time.\n\nAn **order-statistic tree** $T$ is a red-black tree with additional information stored in each node. Each node $x$ contains the usual red-black tree attributes ($x.key, x.color, x.p, x.left, x.right$) along with a new attribute, $x.size$. This attribute contains the number of internal nodes in the subtree rooted at $x$ (including $x$ itself, but not including any sentinels). If we define the sentinel's size to be 0 (i.e., $T.nil.size = 0$), then we have the identity: $x.size = x.left.size + x.right.size + 1$.\n\nKeys need not be distinct. When equal keys are present, the rank of an element is defined as the position at which it would be printed in an inorder walk of the tree. Figure 17.1 in the book shows an example.\n\n### Retrieving the element with a given rank\nThe procedure [[PartV Advanced Data Structures Algorithms.md#C17.1 OS-SELECT]] returns a pointer to the node containing the $i$-th smallest key in the subtree rooted at $x$. To find the $i$-th smallest key in tree $T$, call `OS-SELECT(T.root, i)`.\nThe procedure works by computing $r = x.left.size + 1$, which is the rank of node $x$ within its own subtree. If $i=r$, $x$ is the desired node. If $i < r$, the $i$-th smallest element is in $x$'s left subtree. If $i > r$, the $i$-th smallest element is the $(i-r)$-th smallest element in $x$'s right subtree.\nSince each recursive call goes down one level, the total time for `OS-SELECT` is $O(h)$, where $h$ is the height of the tree. For a red-black tree, this is $O(\\lg n)$.\n\n### Determining the rank of an element\nThe procedure [[PartV Advanced Data Structures Algorithms.md#C17.1 OS-RANK]] returns the position of $x$ in the linear order determined by an inorder tree walk of $T$.\nThe rank of $x$ is its initial rank within its own subtree ($r = x.left.size + 1$) plus the sum of sizes of all left subtrees and parent nodes that precede $x$ in the inorder walk as we ascend towards the root. The algorithm iteratively moves from $y=x$ up to $T.root$. If $y$ is a right child of $y.p$, then all nodes in $y.p$'s left subtree, plus $y.p$ itself, precede $x$. So, $r$ is incremented by $y.p.left.size + 1$.\n*Loop Invariant:* At the start of each iteration of the `while` loop (lines 3-6), $r$ is the rank of $x.key$ in the subtree rooted at node $y$.\n-   **Initialization:** Before the first iteration, line 1 sets $r$ to be $x.key$'s rank in the subtree at $x$. Line 2 sets $y=x$, making the invariant true.\n-   **Maintenance:** If $y$ is a left child, its parent $y.p$ and $y.p$'s right subtree do not precede $x.key$ relative to the subtree rooted at $y.p$. If $y$ is a right child, $y.p$ and all nodes in $y.p$'s left subtree precede $x.key$. Line 5 adds $y.p.left.size + 1$ to $r$. Moving $y$ to $y.p$ maintains the invariant for the next iteration.\n-   **Termination:** The loop terminates when $y = T.root$. At this point, $r$ is the rank of $x.key$ in the entire tree.\nEach iteration takes $O(1)$ time, and $y$ goes up one level. Total time is $O(h) = O(\\lg n)$.\n\n### Maintaining subtree sizes\nTo efficiently maintain the $size$ attribute during insertion and deletion without affecting the $O(\\lg n)$ running time:\n\n**Insertion:**\n1.  **First phase (going down):** Increment $x.size$ for each node $x$ on the path from the root to the leaves where the new node is added. The new node gets $size=1$. This adds $O(\\lg n)$ to the first phase.\n2.  **Second phase (going up, rotations):** Rotations are local operations. Only two nodes have their $size$ attributes invalidated. For `LEFT-ROTATE(T,x)` (where $y=x.right$ becomes the new root of the subproblem), update sizes as: $y.size = x.size$ (before $x$ changes), then $x.size = x.left.size + x.right.size + 1$. (See [[PartV Advanced Data Structures Algorithms.md#C17.1 LEFT-ROTATE Size Update]]). This takes $O(1)$ per rotation. Since at most two rotations occur, this phase adds $O(1)$.\nTotal insertion time remains $O(\\lg n)$.\n\n**Deletion:**\n1.  **First phase (search and remove/move):** This phase may remove one node and move at most two others. To update subtree sizes, traverse a simple path from the lowest moved/removed node up to the root, decrementing $size$ attributes. This path has length $O(\\lg n)$, costing $O(\\lg n)$.\n2.  **Second phase (rotations):** At most three rotations occur. Updates are $O(1)$ per rotation. Total $O(1)$.\nTotal deletion time remains $O(\\lg n)$.\n\nExercises 17.1-1 to 17.1-8 are presented.\n\n## 17.2 How to augment a data structure\nThe process of augmenting a basic data structure can be broken into four steps:\n1.  Choose an underlying data structure.\n2.  Determine additional information to maintain in the underlying data structure.\n3.  Verify that you can maintain the additional information for the basic modifying operations on the underlying data structure.\n4.  Develop new operations.\n\nThese steps were followed for order-statistic trees: Red-black trees were chosen (Step 1). The $size$ attribute was added (Step 2). Maintenance of $size$ during insertion and deletion was verified to be $O(\\lg n)$ (Step 3). `OS-SELECT` and `OS-RANK` were developed (Step 4).\n\n### Augmenting red-black trees\n**Theorem 17.1 (Augmenting a red-black tree):** Let $f$ be an attribute that augments a red-black tree $T$ of $n$ nodes. Suppose that the value of $f$ for each node $x$ depends only on the information in nodes $x$, $x.left$, and $x.right$ (possibly including $x.left.f$ and $x.right.f$), and that the value of $x.f$ can be computed from this information in $O(1)$ time. Then, the insertion and deletion operations can maintain the values of $f$ in all nodes of $T$ without asymptotically affecting the $O(\\lg n)$ running times of these operations.\n\n*Proof idea:* A change to an $f$ attribute in a node $x$ propagates only to ancestors of $x$. Updating $T.root.f$ terminates the process. Since tree height is $O(\\lg n)$, changing $x.f$ costs $O(\\lg n)$ to update ancestors.\n-   **Insertion:** Phase 1 (node insertion) computes $x.f$ in $O(1)$ (children are $T.nil$). Propagation up takes $O(\\lg n)$. Phase 2 (rotations): At most two rotations. Each rotation might cause changes propagating up to the root, taking $O(\\lg n)$ per rotation. Total $O(\\lg n)$.\n-   **Deletion:** Phase 1 (node removal/movement) causes local changes. Propagation up takes $O(\\lg n)$. Phase 2 (rotations): At most three rotations. Each takes $O(\\lg n)$ for propagation. Total $O(\\lg n)$.\n\nIf updating $f$ after a rotation is $O(1)$ (like $size$ attribute), the cost per rotation is $O(1)$.\n\nExercises 17.2-1 to 17.2-3 are presented.\n\n## 17.3 Interval trees\nThis section shows how to augment red-black trees to support operations on dynamic sets of intervals. Intervals are assumed to be closed $[t_1, t_2]$. An interval $i$ has attributes $i.low = t_1$ and $i.high = t_2$. Intervals $i$ and $i'$ overlap if $i.low \text{ } \text{<} \text{=} \text{ } i'.high$ and $i'.low \text{ } \text{<} \text{=} \text{ } i.high$.\nAny two intervals $i, i'$ satisfy the **interval trichotomy**: 1. $i$ and $i'$ overlap, 2. $i$ is to the left of $i'$ ($i.high < i'.low$), or 3. $i$ is to the right of $i'$ ($i'.high < i.low$). (See Figure 17.3 in the book).\n\nAn **interval tree** is a red-black tree that maintains a dynamic set of elements, with each element $x$ containing an interval $x.int$. Operations:\n-   `INTERVAL-INSERT(T, x)`: Adds element $x$ (with $x.int$) to interval tree $T$.\n-   `INTERVAL-DELETE(T, x)`: Removes element $x$ from $T$.\n-   `INTERVAL-SEARCH(T, i)`: Returns a pointer to an element $x$ in $T$ such that $x.int$ overlaps interval $i$, or $T.nil$ if no such element exists.\n\nDesign steps (Figure 17.4 in the book for an example):\n\n**Step 1: Underlying data structure**\nA red-black tree where each node $x$ stores an interval $x.int$. The key of $x$ is $x.int.low$. An inorder walk lists intervals by sorted low endpoints.\n\n**Step 2: Additional information**\nEach node $x$ contains $x.max$, the maximum value of any interval endpoint stored in the subtree rooted at $x$.\n\n**Step 3: Maintaining the information**\n$x.max = \text{max}(x.int.high, x.left.max, x.right.max)$ (assuming $T.nil.max = -\\infty$).\nThis can be computed in $O(1)$ time given children's max values. By Theorem 17.1, insertion and deletion run in $O(\\lg n)$ time. Updating $max$ attributes after rotation can be done in $O(1)$ (Exercise 17.2-3 or 17.3-1).\n\n**Step 4: Developing new operations**\nThe procedure is [[PartV Advanced Data Structures Algorithms.md#C17.3 INTERVAL-SEARCH]]. It searches for an interval $i$ starting from $T.root$. It terminates when an overlapping interval is found or $T.nil$ is reached. Each iteration is $O(1)$, so total time is $O(\\lg n)$.\n\n*Correctness of INTERVAL-SEARCH:* Theorem 17.2 states that `INTERVAL-SEARCH(T, i)` either returns a node whose interval overlaps $i$, or it returns $T.nil$ and $T$ contains no such node.\n*Proof idea:* If $x.int$ does not overlap $i$, the search proceeds safely.\n1.  If search goes left (line 4: $x.left \neq T.nil$ and $x.left.max \text{ } \text{>}\text{=} \text{ } i.low$): Either an overlapping interval is in the left subtree, or if not, the right subtree contains no interval overlapping $i$. This is because if the left subtree contains no overlap, then for any interval $i'$ in the left subtree with $i'.high = x.left.max$, we have $i.high < i'.low$. Since tree is ordered by low endpoints, any interval $i''$ in the right subtree has $x.int.low \text{ } \text{<}\text{=} \text{ } i''.low$. Thus $i.high < i'.low \text{ } \text{<}\text{=} \text{ } x.int.low \text{ } \text{<}\text{=} \text{ } i''.low$, so $i$ cannot overlap $i''$. (See Figure 17.5b in the book).\n2.  If search goes right (line 5): The left subtree $x.left$ contains no interval overlapping $i$. This is because either $x.left = T.nil$ or $x.left.max < i.low$. If $x.left.max < i.low$, then for any interval $i'$ in $x.left$, $i'.high \text{ } \text{<}\text{=} \text{ } x.left.max < i.low$. By interval trichotomy, $i'$ and $i$ do not overlap. (See Figure 17.5a in the book).\n\nExercises 17.3-1 to 17.3-6 are presented.\nProblems 17-1 (Point of maximum overlap) and 17-2 (Josephus permutation) are described. Chapter notes reference works by Edelsbrunner and McCreight."
            },
            {
                "file": "PartV Advanced Data Structures/C18 B-Trees.md",
                "contents": "# 18 B-Trees\n\nB-trees are balanced search trees designed to work well on disk drives or other direct-access secondary storage devices. They are similar to red-black trees but are better at minimizing disk accesses. Many database systems use B-trees or variants.\nB-tree nodes can have many children (large branching factor), depending on disk characteristics. An $n$-node B-tree has height $O(\\lg n)$, but the base of the logarithm is larger than in red-black trees, leading to lower height.\nB-trees generalize binary search trees. An internal node $x$ with $x.n$ keys has $x.n+1$ children. Keys in $x$ act as dividing points for subranges handled by children. (See Figure 18.1 in the book for an example).\n\n## Data structures on secondary storage\nComputer systems use main memory (silicon chips, expensive) and secondary storage (SSDs, magnetic disks; cheaper, higher capacity, slower).\nMagnetic disk drives consist of platters rotating around a spindle. Arms with read/write heads move across platters. A track is the surface passing under a stationary head. (See Figure 18.2 in the book).\nDisk drives are much slower than main memory due to mechanical parts (platter rotation, arm movement - latency). Information is accessed in blocks. Accessing a block takes longer than processing its information.\nRunning time components for disk-based structures:\n1.  Number of disk accesses.\n2.  CPU (computing) time.\nDisk accesses are measured by blocks read/written. Typical B-tree applications handle data too large for main memory. Algorithms copy selected blocks to main memory and write back changed blocks. [[PartV Advanced Data Structures Algorithms.md#Conceptual DISK-READ(x) and DISK-WRITE(x)]] are operations for this. B-tree nodes are usually as large as a disk block. Large branching factors (e.g., 50-2000) reduce tree height and disk accesses. A B-tree of height 2 can store over a billion keys (Figure 18.3 in the book).\n\n## 18.1 Definition of B-trees\nAssume satellite information associated with a key resides in the same node. A **B+-tree** is a variant storing all satellite information in leaves, and only keys and child pointers in internal nodes, maximizing internal node branching factor.\nA **B-tree** $T$ is a rooted tree with root $T.root$ having the following properties:\n1.  Every node $x$ has attributes:\n    a.  $x.n$: number of keys currently stored in node $x$.\n    b.  $x.key_1, x.key_2, ..., x.key_{x.n}$: the $x.n$ keys, stored in nondecreasing order.\n    c.  $x.leaf$: a boolean value, TRUE if $x$ is a leaf, FALSE if $x$ is an internal node.\n2.  Each internal node $x$ also contains $x.n+1$ pointers $x.c_1, x.c_2, ..., x.c_{x.n+1}$ to its children. Leaf nodes have no children, so their $c_i$ attributes are undefined.\n3.  The keys $x.key_i$ separate the ranges of keys stored in each subtree: if $k_i$ is any key stored in the subtree with root $x.c_i$, then $k_1 \\le x.key_1 \\le k_2 \\le x.key_2 \\le \\dots \\le x.key_{x.n} \\le k_{x.n+1}$.\n4.  All leaves have the same depth, which is the tree\u2019s height $h$.\n5.  Nodes have lower and upper bounds on the number of keys they can contain, in terms of a fixed integer $t \\ge 2$ called the **minimum degree** of the B-tree:\n    a.  Every node other than the root must have at least $t-1$ keys. Every internal node other than the root thus has at least $t$ children. If the tree is nonempty, the root must have at least one key.\n    b.  Every node may contain at most $2t-1$ keys. Therefore, an internal node may have at most $2t$ children. A node is **full** if it contains exactly $2t-1$ keys.\n\nThe simplest B-tree has $t=2$. Internal nodes have 2, 3, or 4 children (a 2-3-4 tree). Larger $t$ yield smaller height.\n\n### The height of a B-tree\n**Theorem 18.1:** If $n \\ge 1$, then for any $n$-key B-tree $T$ of height $h$ and minimum degree $t \\ge 2$, $h \\le \\log_t \\frac{n+1}{2}$.\n*Proof:* Root has $\\ge 1$ key. Other nodes have $\\ge t-1$ keys. Number of nodes at depth $d$: root (1), depth 1 ($\\{\\ge 2\\}$ children), depth 2 ($\\{\\ge 2t\\}$ children), ..., depth $h$ ($\\{\\ge 2t^{h-1}\\}$ nodes). (See Figure 18.4 in the book).\nThe number of keys $n \\ge 1 + (t-1) \\sum_{i=1}^h 2t^{i-1} = 1 + 2(t-1) \\frac{t^h-1}{t-1} = 2t^h - 1$. So $t^h \\le (n+1)/2$. Taking $\\log_t$ gives the bound.\nB-trees save a factor of about $\\lg t$ in nodes examined (disk accesses) compared to red-black trees.\n\nExercises 18.1-1 to 18.1-5 are presented.\n\n## 18.2 Basic operations on B-trees\nConventions: root of B-tree always in main memory (no `DISK-READ` on root, but `DISK-WRITE` if changed). Any nodes passed as parameters already had `DISK-READ` performed.\nProcedures are \u201cone-pass\u201d algorithms (downward from root).\n\n### Searching a B-tree\nProcedure [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-SEARCH]] generalizes binary tree search, making an $(x.n+1)$-way branching decision at each internal node $x$.\nInput: root $x$ of subtree, key $k$. Returns $(y,i)$ if $y.key_i = k$, else NIL.\nAlgorithm finds smallest $i$ such that $k \\le x.key_i$. If $k = x.key_i$, found. Else if $x$ is leaf, not found. Else, recurse on $x.c_i$ (after `DISK-READ(x.c_i)`).\nDisk accesses: $O(h) = O(\\log_t n)$. CPU time: $O(t)$ per node search. Total CPU $O(th) = O(t \\log_t n)$.\n\n### Creating an empty B-tree\nProcedure [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-CREATE]] uses `ALLOCATE-NODE` (allocates one disk block for new node in $O(1)$ time, no `DISK-READ` needed). Sets new node as leaf, $n=0$, `DISK-WRITE`, sets $T.root$.\nRequires $O(1)$ disk operations and $O(1)$ CPU time.\n\n### Inserting a key into a B-tree\nMore complex than BSTs. Search for leaf position. Cannot simply create new leaf. Insert into existing leaf. If leaf is full, it must be split.\nA full node $y$ (with $2t-1$ keys) is split around its median key $y.key_t$ into two nodes with $t-1$ keys each. Median key moves to $y$'s parent. If parent is full, it too must be split. Splitting can propagate up to root.\nTo avoid backing up, split any full node encountered on the way down. Parent of a node to be split is guaranteed not full.\nInsertion requires a single pass down.\n\n**Splitting a node in a B-tree**\nProcedure [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-SPLIT-CHILD]]. Input: nonfull internal node $x$, index $i$ such that $x.c_i$ is a full child of $x$. (Assumes $x$ and $x.c_i$ in memory).\nSplits $y=x.c_i$ into $y$ and a new node $z$. $y$ gets $t-1$ smallest keys, $z$ gets $t-1$ largest keys. Median key $y.key_t$ moves into $x$. $z$ becomes child of $x$ after $y$. (See Figure 18.5 in the book).\nCPU time $\\Theta(t)$ (due to loops for keys/children). $O(1)$ disk operations (3 writes: $y, z, x$).\nTree grows in height only by splitting the root. This increases height by 1.\n\n**Inserting a key in a single pass down the tree**\nProcedure [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-INSERT]]. Input: tree $T$, key $k$. Height $h$. $O(h)$ disk accesses. CPU time $O(th) = O(t \\log_t n)$.\nIf root $r$ is full ($r.n = 2t-1$), call [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-SPLIT-ROOT]]. This creates a new root $s$ with $s.n=0$, $s.leaf=$FALSE, $s.c_1 = r$. Then it calls `B-TREE-SPLIT-CHILD(s,1)` to split the old root $r$. The new root $s$ (returned by `B-TREE-SPLIT-ROOT`) is then used for insertion. (See Figure 18.6 in the book).\nThen, call [[PartV Advanced Data Structures Algorithms.md#C18.2 B-TREE-INSERT-NONFULL]] on the (possibly new) nonfull root.\n\n`B-TREE-INSERT-NONFULL(x,k)` assumes $x$ is nonfull.\n- If $x$ is a leaf: Insert $k$ into $x$ at correct position. Increment $x.n$. `DISK-WRITE(x)`.\n- If $x$ is internal: Find child $x.c_i$ where $k$ should go. `DISK-READ(x.c_i)`. If $x.c_i$ is full, call `B-TREE-SPLIT-CHILD(x,i)`. This may change which child of $x$ is correct for $k$. Then, recurse `B-TREE-INSERT-NONFULL` on the appropriate (nonfull) child.\n(See Figure 18.7 in the book for examples).\nSince `B-TREE-INSERT-NONFULL` is tail-recursive, it can be implemented with a `while` loop, requiring $O(1)$ blocks in memory.\n\nExercises 18.2-1 to 18.2-7 are presented.\n\n## 18.3 Deleting a key from a B-tree\nDeletion is analogous to insertion but more complex. Key can be deleted from any node. If key is from internal node, children must be rearranged. Must guard against underfull nodes (fewer than $t-1$ keys, root can have $<t-1$ keys).\n`B-TREE-DELETE(T,k)` combines search and deletion. Guarantees that whenever it recursively calls itself on node $x$, $x.n \\ge t$ (minimum degree, not $t-1$), unless $x$ is root. (See Figure 18.8 for examples).\n\nThree main cases when deleting key $k$ from subtree rooted at $x$ (where $x.n \\ge t$ unless $x$ is root):\n\n**Case 1: Key $k$ is in node $x$, and $x$ is a leaf.**\nDelete $k$ from $x$. No children to worry about.\n\n**Case 2: Key $k$ is in node $x$, and $x$ is an internal node.** (Let $k = x.key_j$ for some $j$)\n    a.  If child $y = x.c_j$ (preceding $k$) has at least $t$ keys: Find predecessor $k'$ of $k$ in subtree rooted at $y$. Recursively delete $k'$, replace $k$ with $k'$ in $x$. ($k'$ is largest key in $y$'s subtree, found by descending rightmost path from $y$).\n    b.  Symmetrically, if child $z = x.c_{j+1}$ (following $k$) has at least $t$ keys: Find successor $k'$ of $k$ in subtree $z$. Recursively delete $k'$, replace $k$ with $k'$ in $x$.\n    c.  If both $y$ and $z$ have only $t-1$ keys: Merge $k$ and all of $z$ into $y$. Node $x$ loses $k$ and pointer to $z$. Node $y$ now contains $2t-1$ keys. Free $z$. Recursively delete $k$ from $y$.\n\n**Case 3: Key $k$ is not in internal node $x$.**\nDetermine child $x.c_j$ that must contain $k$. If $x.c_j$ has only $t-1$ keys, it needs one more key to ensure $x.c_j.n \\ge t$ before recursing. Two subcases:\n    a.  If $x.c_j$ has an immediate sibling (say $x.c_{j+1}$) with at least $t$ keys: Move a key from $x$ down into $x.c_j$. Move a key from $x.c_{j+1}$ up into $x$. Move appropriate child pointer from $x.c_{j+1}$ to $x.c_j$. (Symmetric if left sibling $x.c_{j-1}$ is used).\n    b.  If $x.c_j$ and both its immediate siblings (or one, if it's an end child) have $t-1$ keys: Merge $x.c_j$ with one sibling. This involves moving a key from $x$ down into the new merged node (becoming its median key).\nAfter ensuring $x.c_j$ has at least $t$ keys (possibly by merging it into a new node which is now $x.c_j$), recurse on $x.c_j$.\n\nIf root becomes empty (e.g. after Case 2c or 3b on root's only child), the old root is deleted and its only child becomes the new root, decreasing tree height by 1.\n\nAnalysis: $O(h)$ disk operations, $O(th) = O(t \\log_t n)$ CPU time. Most keys are in leaves, so deletion often from leaves (Case 1).\n\nExercises 18.3-1 and 18.3-2 are presented.\nProblems 18-1 (Stacks on secondary storage) and 18-2 (Joining and splitting 2-3-4 trees) are described. Chapter notes mention Knuth, Aho, Hopcroft, Ullman, Sedgewick, Comer, Guibas, Bayer, McCreight, Bender, Demaine, Farach-Colton."
            },
            {
                "file": "PartV Advanced Data Structures/C19 Data Structures for Disjoint Sets.md",
                "contents": "# 19 Data Structures for Disjoint Sets\n\nSome applications involve grouping $n$ distinct elements into a collection of disjoint sets. Key operations are finding the unique set containing an element and uniting two sets. This chapter explores methods for maintaining such a data structure.\n\n## 19.1 Disjoint-set operations\nA disjoint-set data structure maintains a collection $S = \\{S_1, S_2, ..., S_k\\}$ of disjoint dynamic sets. Each set is identified by a **representative**, which is some member of the set.\nOperations:\n-   `MAKE-SET(x)`: Creates a new set whose only member (and representative) is $x$. Assumes $x$ is not already in another set.\n-   `UNION(x,y)`: Unites the dynamic sets containing $x$ and $y$, say $S_x$ and $S_y$, into a new set $S_x \\cup S_y$. $S_x$ and $S_y$ are destroyed. The representative of the new set can be any member, often chosen from $S_x$ or $S_y$'s representative.\n-   `FIND-SET(x)`: Returns a pointer to the representative of the unique set containing $x$.\n\nAnalysis is in terms of $n$ (number of `MAKE-SET` operations) and $m$ (total number of operations). $m \\ge n$. At most $n-1$ `UNION` operations can occur, as each reduces the number of sets by 1.\n\n### An application of disjoint-set data structures\nDetermining connected components of an undirected graph $G=(V,E)$. (See Figure 19.1a in the book).\nProcedure [[PartV Advanced Data Structures Algorithms.md#C19.1 CONNECTED-COMPONENTS]] uses disjoint-set operations. Initially, each vertex $v \\in G.V$ is in its own set. For each edge $(u,v) \\in G.E$, if $u$ and $v$ are in different sets, their sets are united. After processing all edges, two vertices are in the same connected component if and only if they are in the same set (Exercise 19.1-2).\nProcedure [[PartV Advanced Data Structures Algorithms.md#C19.1 SAME-COMPONENT]] queries if two vertices are in the same component. (See Figure 19.1b in the book for an example trace).\nFor static graphs, DFS is faster. For dynamic graphs (edges added over time), this approach can be more efficient.\n\nExercises 19.1-1 to 19.1-3 are presented.\n\n## 19.2 Linked-list representation of disjoint sets\nEach set is represented by its own linked list. (See Figure 19.2a in the book).\n-   Set object attributes: `head` (points to first object in list), `tail` (points to last object).\n-   List object attributes: set member, `next` pointer, pointer back to set object.\n-   Representative: member in the first object of the list.\n\n`MAKE-SET(x)`: Create new list with $x$. $O(1)$ time.\n`FIND-SET(x)`: Follow pointer from $x$ to set object, return member at `head`. $O(1)$ time.\n\n### A simple implementation of union\n`UNION(x,y)`: Append $y$'s list to $x$'s list. (See Figure 19.2b in the book). Representative of $x$'s list becomes representative of new set. `tail` pointer of $x$'s list used for append. All objects originally in $y$'s list must update their pointer to the set object, now pointing to $x$'s set object. This takes time linear in length of $y$'s list.\nWorst case: A sequence of $m$ operations ($n$ `MAKE-SET`, $n-1$ `UNION`s, $m=2n-1$) can take $\\Theta(n^2)$ time. (See Figure 19.3 in the book). Amortized time per operation is $\\Theta(n)$.\n\n### A weighted-union heuristic\nAlways append the shorter list onto the longer list (breaking ties arbitrarily). Each list must store its length.\nA single `UNION` can still take $\\Omega(n)$ time.\n**Theorem 19.1:** Using linked-list representation and weighted-union heuristic, a sequence of $m$ operations ($n$ `MAKE-SET`s) takes $O(m + n \\lg n)$ time.\n*Proof idea:* Bound total pointer updates. An object $x$'s set pointer is updated only if $x$ starts in smaller set. Resulting set size at least doubles. For $k \\le n$, after $x$'s pointer updated $\\lceil \\lg k \\rceil$ times, resulting set has $\\ge k$ members. Max set size $n$, so pointer updated $\\le \\lceil \\lg n \\rceil$ times. Total pointer updates $O(n \\lg n)$. Other costs per UNION are $O(1)$. `MAKE-SET` and `FIND-SET` are $O(1)$. Total $O(m + n \\lg n)$.\n\nExercises 19.2-1 to 19.2-6 are presented.\n\n## 19.3 Disjoint-set forests\nSets are represented by rooted trees. Each node contains one member. Each tree represents one set. (See Figure 19.4a in the book).\nEach member points only to its parent. Root contains representative and is its own parent.\n-   `MAKE-SET(x)`: Creates a tree with one node $x$. [[PartV Advanced Data Structures Algorithms.md#C19.3 MAKE-SET Forest]]\n-   `FIND-SET(x)`: Follows parent pointers from $x$ to the root. Path traversed is the **find path**. [[PartV Advanced Data Structures Algorithms.md#C19.3 FIND-SET Forest]]\n-   `UNION(x,y)`: Causes root of one tree to point to root of other. [[PartV Advanced Data Structures Algorithms.md#C19.3 UNION Forest]] which calls [[PartV Advanced Data Structures Algorithms.md#C19.3 LINK Forest]]. (See Figure 19.4b in the book).\n\nWithout heuristics, $n-1$ UNIONs could create a linear chain of $n$ nodes.\nTwo heuristics improve running time significantly:\n1.  **Union by rank:** Similar to weighted-union. Each node $x$ maintains $x.rank$, an upper bound on height of $x$. `LINK(x,y)` (where $x,y$ are roots) makes root with smaller rank point to root with larger rank. If ranks are equal, one arbitrarily becomes parent, and its rank increments by 1.\n2.  **Path compression:** Used during `FIND-SET(x)`. Makes each node on find path from $x$ to root point directly to root. (See Figure 19.5 in the book). Does not change ranks.\n\n*Pseudocode for disjoint-set forests:*\n-   `MAKE-SET(x)`: $x.p = x$, $x.rank = 0$.\n-   `UNION(x,y)`: Calls `LINK(FIND-SET(x), FIND-SET(y))`. \n-   `LINK(x,y)`: If $x.rank > y.rank$, $y.p = x$. Else $x.p = y$; if $x.rank == y.rank$, then $y.rank = y.rank + 1$.\n-   `FIND-SET(x)` (recursive, with path compression): If $x \ne x.p$, then $x.p = \text{FIND-SET}(x.p)$. Return $x.p$.\nThis is a two-pass method: one pass up to find root, second pass down to update pointers.\n\n*Effect of heuristics:* Union by rank alone: $O(m \\lg n)$. Path compression alone: $\\Theta(n + f \tcdot (1 + \\log_{2+f/n} n))$ for $f$ FIND-SETs. Combined: $O(m \\alpha(n))$, where $\\alpha(n)$ is very slowly growing inverse Ackermann-like function.\n\nExercises 19.3-1 to 19.3-5 are presented.\n\n## 19.4 Analysis of union by rank with path compression\nThis section proves the $O(m \\alpha(n))$ bound using amortized analysis (potential method).\n\n### A very quickly growing function and its very slowly growing inverse\nDefine $A_k(j)$ for integers $j, k \\ge 0$:\n$A_k(j) = \\begin{cases} j+1 & \\text{if } k=0 \\\\ A_{k-1}^{(j+1)}(j) & \\text{if } k \\ge 1 \\end{cases}$\nwhere $A_{k-1}^{(j+1)}(j)$ uses functional iteration: $A^{(0)}(x)=x$, $A^{(i)}(x) = A(A^{(i-1)}(x))$.\n\n**Lemma 19.2:** For any integer $j \\ge 1$, $A_1(j) = 2j+1$.\n*Proof:* $A_0^{(i)}(j) = j+i$. So $A_1(j) = A_0^{(j+1)}(j) = j+(j+1) = 2j+1$.\n\n**Lemma 19.3:** For any integer $j \\ge 1$, $A_2(j) = 2^{j+1}(j+1)-1$.\n*Proof:* (By induction on $i$ for $A_1^{(i)}(j)$ then using $A_2(j)=A_1^{(j+1)}(j)$)\n\n$A_k(j)$ grows very quickly. Example values: $A_0(1)=2, A_1(1)=3, A_2(1)=7, A_3(1)=2047, A_4(1) > 10^{80}$.\n\nDefine inverse $\\alpha(n) = \\min\\{k : A_k(1) \\ge n\\}$.\n$\\{\\alpha(n) \\le 4\\}$ for all practical purposes (i.e., for $n \\le A_4(1)$).\n\n### Properties of ranks\n**Lemma 19.4:** For all nodes $x$, $x.rank \\le x.p.rank$, with strict inequality if $x \ne x.p$. $x.rank$ is initially 0, increases until $x \ne x.p$, then doesn't change. $x.p.rank$ monotonically increases over time.\n**Corollary 19.5:** On simple path from any node to root, ranks strictly increase.\n**Lemma 19.6:** Every node has rank at most $n-1$. (Tighter bound is $\\lfloor \\lg n \\rfloor$, see Exercise 19.4-2).\n\n### Proving the time bound\nAssume UNION is replaced by two FIND-SETs then one LINK (Lemma 19.7 shows this doesn't change asymptotic bound for $m'$ original operations vs $m$ converted operations).\n**Potential function** $\\Phi_q$ after $q$ operations. For node $x$:\n- If $x$ is a root or $x.rank=0$: $\\phi_q(x) = \\alpha(n) \tcdot x.rank$.\n- If $x$ is not a root and $x.rank \\ge 1$: $\\phi_q(x) = (\\alpha(n) - \\text{level}(x)) \tcdot x.rank - \\text{iter}(x)$.\n  - $\\text{level}(x) = \\max\\{k : x.p.rank \\ge A_k(x.rank)\\}$. $0 \\le \\text{level}(x) < \\alpha(n)$.\n  - $\\text{iter}(x) = \\max\\{i : x.p.rank \\ge A_{\\text{level}(x)}^{(i)}(x.rank)\\}$. $1 \\le \\text{iter}(x) \\le x.rank$.\n\n**Lemma 19.8:** For every node $x$, $0 \\le \\phi_q(x) \\le \\alpha(n) \tcdot x.rank$.\n**Corollary 19.9:** If $x$ is not root and $x.rank > 0$, then $\\phi_q(x) < \\alpha(n) \tcdot x.rank$.\n\n**Lemma 19.10:** Let $x$ be a non-root node. After a LINK or FIND-SET, $\\phi_q(x) \\le \\phi_{q-1}(x)$. If $x.rank > 0$ and $\\text{level}(x)$ or $\\text{iter}(x)$ changes, $\\phi_q(x) \\le \\phi_{q-1}(x) - 1$.\n\n*Amortized costs:*\n**Lemma 19.11:** Amortized cost of `MAKE-SET` is $O(1)$.\n*Proof:* Creates node $x$ with $x.rank=0$, so $\\phi_q(x)=0$. No other potentials change. $\\Delta\\Phi = 0$. Actual cost $O(1)$.\n\n**Lemma 19.12:** Amortized cost of `LINK` is $O(\\alpha(n))$.\n*Proof:* Actual cost $O(1)$. Suppose `LINK(x,y)` makes $y$ parent of $x$. Potentials of $y$'s children (before link) do not increase (Lemma 19.10). Potential of $x$ (old root) decreases (Corollary 19.9). Potential of $y$ (new root of combined tree) can increase if its rank increases by 1 (from $y.rank$ to $y.rank+1$). Increase is $\\alpha(n)$. Total $\\Delta\\Phi \\le \\alpha(n)$.\n\n**Lemma 19.13:** Amortized cost of `FIND-SET` is $O(\\alpha(n))$.\n*Proof:* Actual cost $O(s)$ for find path of $s$ nodes. No node's potential increases. At least $\\max\\{0, s - (\\alpha(n)+2)\\}$ nodes on find path have their potential decrease by at least 1. These are nodes $x$ where $x.rank>0$ and it's followed by non-root $y$ with $\\text{level}(y) = \\text{level}(x)$ before path compression, leading to $\\text{iter}(x)$ increasing. Total potential decrease is at least $\\max\\{0, s - (\\alpha(n)+2)\\}$. Amortized cost $\\hat{c} \\le O(s) - (s - (\\alpha(n)+2)) = O(\\alpha(n))$ (after scaling potential units).\n\n**Theorem 19.14:** A sequence of $m$ `MAKE-SET`, `UNION`, `FIND-SET` operations ($n$ `MAKE-SET`s) on disjoint-set forest with union by rank and path compression takes $O(m \\alpha(n))$ time.\n*Proof:* Immediate from Lemmas 19.7, 19.11, 19.12, 19.13.\n\nExercises 19.4-1 to 19.4-7 are presented.\nProblems 19-1 (Offline minimum), 19-2 (Depth determination), and 19-3 (Tarjan's offline lowest-common-ancestors algorithm) are described. Chapter notes mention Tarjan, Hopcroft, Ullman, Kozen, Harfst, Reingold, van Leeuwen, Goel, Gabow, Fredman, Saks."
            }
        ]
    },
    {
        "items": [
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C14 Dynamic Programming.md",
                "contents": "# 14 Dynamic Programming\n\nPart IV of the book covers three important techniques used in designing and analyzing efficient algorithms: dynamic programming (this chapter), greedy algorithms (Chapter 15), and amortized analysis (Chapter 16). These techniques are somewhat more sophisticated than earlier ones like divide-and-conquer but are applicable to many computational problems.\n\nDynamic programming, like the divide-and-conquer method, solves problems by combining solutions to subproblems. (\"Programming\" in this context refers to a tabular method, not writing computer code.) Divide-and-conquer algorithms partition the problem into disjoint subproblems, solve them recursively, and then combine their solutions. In contrast, dynamic programming applies when subproblems overlap\u2014that is, when subproblems share sub-subproblems. In this context, a divide-and-conquer algorithm does more work than necessary, repeatedly solving common sub-subproblems. A dynamic-programming algorithm solves each sub-subproblem just once and then saves its answer in a table, avoiding recomputation.\n\nDynamic programming typically applies to optimization problems, where many possible solutions exist, each with a value, and we want to find a solution with the optimal (minimum or maximum) value. An optimal solution might not be unique.\n\nTo develop a dynamic-programming algorithm, follow a sequence of four steps:\n1. Characterize the structure of an optimal solution.\n2. Recursively define the value of an optimal solution.\n3. Compute the value of an optimal solution, typically in a bottom-up fashion.\n4. Construct an optimal solution from computed information.\nSteps 1\u20133 form the basis of a dynamic-programming solution. Step 4 can be omitted if only the value of an optimal solution is needed. When performing step 4, it often helps to maintain additional information during step 3.\n\n## 14.1 Rod cutting\n\nThis section introduces a simple problem in deciding where to cut steel rods to maximize revenue.\n\n**Problem definition**: Given a rod of length $n$ inches and a table of prices $p_i$ for $i = 1, 2, \n\n\n, n$, determine the maximum revenue $r_n$ obtainable by cutting up the rod and selling the pieces. If the price $p_n$ is large enough, an optimal solution might require no cutting at all.\n\nSerling Enterprises can cut up a rod of length $n$ in $2^{n-1}$ different ways, since there's an independent option of cutting or not cutting at $n-1$ possible locations.\n\n**Optimal substructure**: If an optimal solution cuts the rod into $k$ pieces, for some $1 \\le k \\le n$, as $n = i_1 + i_2 + \n\n\n + i_k$, then the revenue is $r_n = p_{i_1} + p_{i_2} + \n\n\n + p_{i_k}$. We can express the optimal revenue $r_n$ for $n \\ge 1$ in terms of optimal revenues from shorter rods:\n$r_n = \\max (p_n, r_1 + r_{n-1}, r_2 + r_{n-2}, \n\n\n, r_{n-1} + r_1)$. (14.1)\nThe first argument $p_n$ corresponds to making no cuts. The other $n-1$ arguments correspond to making an initial cut into two pieces of size $i$ and $n-i$, and then optimally cutting up those pieces further.\n\nThis problem exhibits optimal substructure: optimal solutions to a problem incorporate optimal solutions to related subproblems, which may be solved independently.\n\nA simpler way to arrange a recursive structure is to view a decomposition as a first piece of length $i$ cut off the left-hand end, and then a right-hand remainder of length $n-i$. Only the remainder may be further divided. With $r_0 = 0$, this yields:\n$r_n = \\max_{1 \\le i \\le n} (p_i + r_{n-i})$. (14.2)\nAn optimal solution embodies the solution to only one related subproblem (the remainder).\n\n**Recursive top-down implementation**:\nThe procedure [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 CUT-ROD]] implements equation (14.2). Its running time $T(n)$ is exponential in $n$, specifically $T(n) = 2^n$. This is because it repeatedly solves the same subproblems. The recursion tree for $n=4$ (Figure 14.3 in the book) shows $2^{n-1}$ leaves, each corresponding to one way to cut the rod.\n\n**Using dynamic programming for optimal rod cutting**:\nInstead of solving subproblems repeatedly, solve each subproblem only once. The first time, save its solution. If needed again, look it up. This is a time-memory trade-off.\n\nThere are two main approaches:\n1.  **Top-down with memoization**: Write the procedure recursively, but modify it to save the result of each subproblem. The procedure first checks if it has previously solved the subproblem. If so, it returns the saved value. Otherwise, it computes and saves the value. The procedure is *memoized*.\n    The procedures [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 MEMOIZED-CUT-ROD]] and [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 MEMOIZED-CUT-ROD-AUX]] implement this. `MEMOIZED-CUT-ROD` initializes an auxiliary array $r$ with a value indicating \"unknown\" and calls `MEMOIZED-CUT-ROD-AUX`. The auxiliary procedure is the memoized version of `CUT-ROD`. It checks if the value is known; if so, it returns it. Otherwise, it computes, saves, and returns it.\n\n2.  **Bottom-up method**: This approach depends on a natural notion of subproblem \"size\", solving smaller subproblems first. Solve subproblems in order of size, storing solutions. When solving a subproblem, solutions to its prerequisite smaller subproblems are already available.\n    The procedure [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 BOTTOM-UP-CUT-ROD]] implements this. It uses an array $r$ to save results. It solves for sizes $j = 0, 1, \n\n\n, n$ in order. To solve for size $j$, it uses previously computed results $r[j-i]$.\n\nBoth top-down memoized and bottom-up approaches have an asymptotic running time of $\\Theta(n^2)$ for the rod-cutting problem. The bottom-up approach often has better constant factors due to less overhead for procedure calls. `BOTTOM-UP-CUT-ROD` uses a doubly nested loop structure; the inner loop's iterations form an arithmetic series, summing to $\\Theta(n^2)$. For `MEMOIZED-CUT-ROD-AUX`, each subproblem (for sizes $0, 1, \n\n\n, n$) is solved once. Solving subproblem size $n$ involves a loop iterating $n$ times. The total iterations across all recursive calls also form an arithmetic series, totaling $\\Theta(n^2)$.\n\n**Subproblem graphs**:\nA subproblem graph for a problem visualizes the set of subproblems and their dependencies. It's a directed graph with one vertex for each distinct subproblem. An edge $(x, y)$ exists if solving subproblem $x$ involves directly considering a solution for subproblem $y$. For rod cutting with length $n=4$ (Figure 14.4 in the book), vertices are $0, 1, 2, 3, 4$. An edge $(i, j)$ means that to solve for rod length $i$, we might make a first cut of length $i-j$, leaving a remainder of length $j$. This graph is a reduced version of the recursion tree.\nThe bottom-up method processes vertices in a reverse topological sort of the subproblem graph. The top-down memoized method is like a depth-first search of this graph.\nThe size of the subproblem graph $G=(V,E)$ helps determine running time. Typically, it's the sum of times to solve each subproblem. If solving a subproblem takes time proportional to the number of outgoing edges, the total time is linear in $|V| + |E|$.\n\n**Reconstructing a solution**:\nTo get the actual list of piece sizes, not just the optimal value, modify the procedures to store choices. [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 EXTENDED-BOTTOM-UP-CUT-ROD]] computes, for each rod size $j$, not only $r_j$ but also $s_j$, the optimal size of the first piece to cut off. [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.1 PRINT-CUT-ROD-SOLUTION]] then uses this information to print the cuts.\n\n## 14.2 Matrix-chain multiplication\n\nThis problem involves finding the optimal way to parenthesize a chain of matrices to minimize scalar multiplications.\n\n**Problem definition**: Given a chain $\\langle A_1, A_2, \n\n\n, A_n \\rangle$ of $n$ matrices, where matrix $A_i$ has dimension $p_{i-1} \\times p_i$, fully parenthesize the product $A_1 A_2 \n\n\n A_n$ to minimize the number of scalar multiplications. The input is the sequence of dimensions $\\langle p_0, p_1, \n\n\n, p_n \\rangle$.\n\nMultiplying a $p \\times q$ matrix by a $q \\times r$ matrix using the standard algorithm ([[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.2 RECTANGULAR-MATRIX-MULTIPLY]]) takes $pqr$ scalar multiplications.\nParenthesization matters. For $A_1 (10 \\times 100)$, $A_2 (100 \\times 5)$, $A_3 (5 \\times 50)$:\n- $((A_1 A_2) A_3)$: $(10 \\cdot 100 \\cdot 5) + (10 \\cdot 5 \\cdot 50) = 5000 + 2500 = 7500$ multiplications.\n- $(A_1 (A_2 A_3))$: $(100 \\cdot 5 \\cdot 50) + (10 \\cdot 100 \\cdot 50) = 25000 + 50000 = 75000$ multiplications.\n\n**Counting the number of parenthesizations**:\nThe number of alternative parenthesizations $P(n)$ for $n$ matrices is given by the recurrence:\n$P(n) = 1$ if $n=1$\n$P(n) = \\sum_{k=1}^{n-1} P(k)P(n-k)$ if $n \\ge 2$. (14.6)\nThis recurrence's solution is related to Catalan numbers, growing as $\\Omega(2^n)$. Brute-force checking is too slow.\n\n**Applying dynamic programming**:\n\n**Step 1: The structure of an optimal parenthesization**\nLet $A_{i \n\n j}$ denote the matrix resulting from $A_i A_{i+1} \n\n\n A_j$. If $i < j$, an optimal parenthesization must split the product between $A_k$ and $A_{k+1}$ for some $i \\le k < j$. The cost is the cost of computing $A_{i \n\n k}$ plus the cost of computing $A_{k+1 \n\n j}$ plus the cost of multiplying these two resultant matrices.\nOptimal substructure: If the optimal split is at $k$, then the parenthesization of the prefix $A_i \n\n\n A_k$ within this optimal parenthesization must be an optimal parenthesization of $A_i \n\n\n A_k$. Similarly for $A_{k+1} \n\n\n A_j$.\n\n**Step 2: A recursive solution**\nLet $m[i, j]$ be the minimum number of scalar multiplications needed to compute $A_{i \n\n j}$. Goal is $m[1, n]$.\nIf $i=j$, $m[i, i] = 0$ (single matrix).\nIf $i < j$, suppose the optimal split is between $A_k$ and $A_{k+1}$. Computing $A_{i \n\n k}$ and $A_{k+1 \n\n j}$ results in matrices of dimensions $p_{i-1} \\times p_k$ and $p_k \\times p_j$. Multiplying them costs $p_{i-1} p_k p_j$.\nSo, $m[i, j] = m[i, k] + m[k+1, j] + p_{i-1} p_k p_j$.\nSince $k$ is unknown, we must check all possibilities $i \\le k < j$:\n$m[i, j] = \\min_{i \\le k < j} \\{m[i, k] + m[k+1, j] + p_{i-1} p_k p_j\\}$ for $i < j$. (14.7)\nLet $s[i, j]$ store the value of $k$ that achieves this minimum.\n\n**Step 3: Computing the optimal costs**\nThere are $\\binom{n}{2} + n = \\Theta(n^2)$ distinct subproblems $m[i, j]$.\nThe procedure [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.2 MATRIX-CHAIN-ORDER]] computes these costs bottom-up. It uses an auxiliary table $m[1 \n\n n, 1 \n\n n]$ for costs and $s[1 \n\n n-1, 2 \n\n n]$ for optimal split points $k$.\nSubproblems are defined by chain length $l$. It computes for $l=1, 2, \n\n\n, n$. For length $l$, $j = i+l-1$.\n- Chains of length 1 ($l=1$): $m[i, i] = 0$.\n- For $l = 2, \n\n\n, n$: iterate $i$ from $1$ to $n-l+1$. Let $j = i+l-1$. Compute $m[i, j]$ using (14.7).\nRunning time is $O(n^3)$ due to three nested loops. Space is $\\Theta(n^2)$.\nFigure 14.5 in the book illustrates the tables $m$ and $s$.\n\n**Step 4: Constructing an optimal solution**\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.2 PRINT-OPTIMAL-PARENS]] uses the $s$ table to print the optimal parenthesization recursively.\n\n## 14.3 Elements of dynamic programming\n\nThis section examines the two key ingredients for dynamic programming to apply: optimal substructure and overlapping subproblems.\n\n**Optimal substructure**:\nA problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems.\nDiscovering optimal substructure typically follows a pattern:\n1. Show that a solution involves making a choice, leaving one or more subproblems.\n2. Assume you are given the choice that leads to an optimal solution.\n3. Determine the resulting subproblems.\n4. Use a \"cut-and-paste\" argument: if a subproblem solution within the assumed optimal solution were not optimal, you could cut it out, paste in a better one, and get a better overall solution, a contradiction.\n\nThe space of subproblems should be kept simple but sufficient. For rod cutting, subproblems of cutting a rod of length $i$ worked. For matrix-chain, subproblems $A_{i \n\n j}$ (varying at both ends) were needed.\n\nOptimal substructure varies in two ways:\n1. How many subproblems are used in an optimal solution (e.g., one in rod cutting, two in matrix-chain).\n2. How many choices in determining which subproblem(s) to use (e.g., $n$ choices for $i$ in rod cutting, $j-i$ choices for $k$ in matrix-chain).\n\nRunning time informally: (number of subproblems) $\\times$ (number of choices per subproblem).\n- Rod cutting: $\\Theta(n)$ subproblems, $O(n)$ choices $\\implies O(n^2)$ time.\n- Matrix-chain: $\\Theta(n^2)$ subproblems, $O(n)$ choices $\\implies O(n^3)$ time.\n\nDP uses optimal substructure bottom-up. Greedy algorithms (Chapter 15) also use optimal substructure but make a greedy choice first.\n\n**Subtleties**:\nNot all problems with apparent optimal substructure are suitable for DP. Consider:\n-   **Unweighted shortest path**: Find a path from $u$ to $v$ with fewest edges. If path $u \\leadsto w \\leadsto v$ is a shortest path, then $u \\leadsto w$ must be a shortest path from $u$ to $w$, and $w \\leadsto v$ must be a shortest path from $w$ to $v$. This exhibits optimal substructure. Subproblems are independent.\n-   **Unweighted longest simple path**: Find a simple path from $u$ to $v$ with most edges. If $u \\leadsto w \\leadsto v$ is a longest simple path, $u \\leadsto w$ is *not necessarily* a longest simple path from $u$ to $w$. (Figure 14.6 in the book shows an example). Subproblems are not independent: choosing a path for $u \\leadsto w$ might use vertices needed for an optimal $w \\leadsto v$ path, making the combination non-simple or suboptimal.\nSubproblems are *independent* if the solution to one does not affect the solution to another. DP relies on independent subproblems.\n\n**Overlapping subproblems**:\nThe space of subproblems must be small (typically polynomial in input size), and a recursive algorithm for the problem must solve the same subproblems repeatedly.\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.3 RECURSIVE-MATRIX-CHAIN]] for matrix-chain multiplication has exponential running time because it recomputes solutions to subproblems. Figure 14.7 in the book shows the recursion tree. $T(n) \\ge 2 \\sum_{i=1}^{n-1} T(i) + n$, so $T(n) = \\Omega(2^n)$.\nDP solves each subproblem once and stores the result.\n\n**Reconstructing an optimal solution**:\nOften, a separate table (like $s[i,j]$ in matrix-chain) stores choices made at each subproblem. This avoids recomputing choices from the cost table, which can be slow if many choices were possible.\n\n**Memoization**:\nAn alternative DP approach that maintains a top-down strategy. Memoize a natural recursive algorithm.\n- Maintain a table for subproblem solutions, initialized to a value indicating \"not yet computed.\"\n- When a subproblem is first encountered, compute its solution and store it.\n- Subsequent encounters just look up the stored value.\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.3 MEMOIZED-MATRIX-CHAIN]] with helper [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.3 LOOKUP-CHAIN]] illustrates this for matrix-chain. It runs in $O(n^3)$ time. There are $\\Theta(n^2)$ calls of the first type (computing and storing), and $O(n^3)$ calls of the second type (lookup). Each first type call takes $O(n)$ plus recursive calls.\nMemoization turns an $\\Omega(2^n)$ algorithm into $O(n^3)$.\nBottom-up DP usually outperforms memoized versions by a constant factor (no recursion overhead, better table access patterns sometimes). However, if some subproblems in the space are not needed, memoization solves only those that are definitely required.\n\n## 14.4 Longest common subsequence\n\nUsed in comparing DNA sequences, for example.\nA *subsequence* of $X = \\langle x_1, \n\n\n, x_m \\rangle$ is $X$ with 0 or more elements left out. E.g., $\\langle B, C, D, B \\rangle$ is a subsequence of $\\langle A, B, C, B, D, A, B \\rangle$.\nA *common subsequence* (CS) of $X$ and $Y$ is a subsequence of both.\nA *longest common subsequence* (LCS) is a CS of maximum length.\n\n**Problem**: Given two sequences $X = \\langle x_1, \n\n\n, x_m \\rangle$ and $Y = \\langle y_1, \n\n\n, y_n \\rangle$, find an LCS of $X$ and $Y$.\nBrute force (enumerating all $2^m$ subsequences of $X$ and checking) is exponential.\n\n**Step 1: Characterizing a longest common subsequence**\nLet $X_i = \\langle x_1, \n\n\n, x_i \\rangle$ be the $i$-th prefix of $X$.\n**Theorem 14.1 (Optimal substructure of an LCS)**\nLet $X=\\langle x_1, \n\n\n, x_m \\rangle$ and $Y=\\langle y_1, \n\n\n, y_n \\rangle$. Let $Z=\\langle z_1, \n\n\n, z_k \\rangle$ be any LCS of $X$ and $Y$.\n1. If $x_m = y_n$, then $z_k = x_m = y_n$, and $Z_{k-1}$ is an LCS of $X_{m-1}$ and $Y_{n-1}$.\n2. If $x_m \\ne y_n$ and $z_k \\ne x_m$, then $Z$ is an LCS of $X_{m-1}$ and $Y$.\n3. If $x_m \\ne y_n$ and $z_k \\ne y_n$, then $Z$ is an LCS of $X$ and $Y_{n-1}$.\nProof involves cut-and-paste arguments.\n\n**Step 2: A recursive solution**\nLet $c[i,j]$ be the length of an LCS of $X_i$ and $Y_j$.\n$c[i,j] = 0$ if $i=0$ or $j=0$.\n$c[i,j] = c[i-1, j-1] + 1$ if $i,j > 0$ and $x_i = y_j$.\n$c[i,j] = \\max(c[i, j-1], c[i-1, j])$ if $i,j > 0$ and $x_i \\ne y_j$. (14.9)\n\n**Step 3: Computing the length of an LCS**\nThere are $\\Theta(mn)$ distinct subproblems. [[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.4 LCS-LENGTH]] computes $c[i,j]$ values bottom-up (row-major order). It also maintains $b[i,j]$ to help construct the LCS. Running time is $\\Theta(mn)$.\nFigure 14.8 in the book shows tables $c$ and $b$.\n\n**Step 4: Constructing an LCS**\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.4 PRINT-LCS]] uses table $b$ to trace back and print the LCS elements in forward order. Time $O(m+n)$.\n\n**Improving the code**:\nTable $b$ can be eliminated; $c[i,j]$'s computation path can be reconstructed from $c[i-1,j-1], c[i-1,j], c[i,j-1]$ in $O(1)$ time.\nSpace for table $c$ can be reduced to $O(\\min(m,n))$ if only length is needed, as each entry depends only on the previous row/column. If reconstructing LCS, this is harder but sometimes possible with more advanced techniques.\n\n## 14.5 Optimal binary search trees\n\n**Problem**: Given a sequence $K = \\langle k_1, \n\n\n, k_n \\rangle$ of $n$ distinct keys in sorted order ($k_1 < k_2 < \n\n\n < k_n$), and probabilities $p_i$ that a search is for key $k_i$, and probabilities $q_i$ that a search is for a value in $(k_i, k_{i+1})$ (represented by dummy key $d_i$; $d_0$ for values $<k_1$, $d_n$ for values $>k_n$). Build a binary search tree (BST) whose expected search cost is minimized.\n$\\sum p_i + \\sum q_i = 1$.\nExpected search cost in a tree $T$: $E[\\text{search cost in } T] = \\sum_{i=1}^n (\text{depth}_T(k_i)+1)p_i + \\sum_{i=0}^n (\text{depth}_T(d_i)+1)q_i$. (14.10 simplified)\nThis can be rewritten as $1 + \\sum_{i=1}^n \text{depth}_T(k_i)p_i + \\sum_{i=0}^n \text{depth}_T(d_i)q_i$. (14.11)\nAn optimal BST is not necessarily the most balanced one. The key with highest probability is not always at the root. Number of BSTs is Catalan, so brute force is infeasible.\n\n**Step 1: The structure of an optimal binary search tree**\nConsider any subtree of a BST. It must contain keys in a contiguous range $k_i, \n\n\n, k_j$, and dummy keys $d_{i-1}, \n\n\n, d_j$. If an optimal BST $T$ has $k_r$ as its root, its left subtree contains $k_i, \n\n\n, k_{r-1}$ and dummy keys $d_{i-1}, \n\n\n, d_{r-1}$. Its right subtree contains $k_{r+1}, \n\n\n, k_j$ and dummy keys $d_r, \n\n\n, d_j$.\nOptimal substructure: If $T$ is an optimal BST for keys $k_i, \n\n\n, k_j$ and $k_r$ is its root, then the left subtree of $k_r$ must be an optimal BST for its keys and dummy keys, and similarly for the right subtree.\n\n**Step 2: A recursive solution**\nLet $e[i,j]$ be the expected cost of searching an optimal BST containing keys $k_i, \n\n\n, k_j$. Goal is $e[1,n]$.\nIf $j = i-1$, only dummy key $d_{i-1}$ exists. $e[i, i-1] = q_{i-1}$.\nIf $j \\ge i$, choose a root $k_r$ ($i \\le r \\le j$). Left subtree has keys $k_i, \n\n\n, k_{r-1}$. Right subtree has $k_{r+1}, \n\n\n, k_j$. Let $w(i,j) = \\sum_{l=i}^j p_l + \\sum_{l=i-1}^j q_l$ be sum of probabilities in the subtree. (14.12)\nWhen a subtree becomes child of a node, depths of its nodes increase by 1. Its expected cost increases by $w(\text{subtree})$.\n$e[i,j] = p_r + (e[i, r-1] + w(i, r-1)) + (e[r+1, j] + w(r+1, j))$.\nSince $w(i,j) = w(i, r-1) + p_r + w(r+1, j)$, this simplifies to:\n$e[i,j] = e[i, r-1] + e[r+1, j] + w(i,j)$. (14.13)\nMinimizing over all possible roots $k_r$:\n$e[i,j] = q_{i-1}$ if $j=i-1$.\n$e[i,j] = \\min_{i \\le r \\le j} \\{e[i, r-1] + e[r+1, j] + w(i,j)\\}$ if $i \\le j$. (14.14)\nLet $\text{root}[i,j]$ store the $r$ that minimizes this cost.\n\n**Step 3: Computing the expected search cost of an optimal binary search tree**\nUse table $e[1 \n\n n+1, 0 \n\n n]$ for costs and $\text{root}[1 \n\n n, 1 \n\n n]$.\nTable $w[1 \n\n n+1, 0 \n\n n]$ stores $w(i,j)$ values. $w[i,i-1] = q_{i-1}$. For $j \\ge i$, $w[i,j] = w[i, j-1] + p_j + q_j$. (14.15) This allows $O(1)$ computation of $w[i,j]$ after $O(n^2)$ precomputation.\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C14.5 OPTIMAL-BST]] computes $e, w, \text{root}$ tables. Loop structure similar to matrix-chain, by increasing length $l$ of key range. Time $\\Theta(n^3)$, space $\\Theta(n^2)$.\nFigure 14.10 in the book shows example tables.\nConstruction of tree from $\text{root}$ table is an exercise (14.5-1).\nKnuth showed that $\text{root}[i,j-1] \\le \text{root}[i,j] \\le \text{root}[i+1,j]$, which can be used to reduce time to $\\Theta(n^2)$ (Exercise 14.5-4)."
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C15 Greedy Algorithms.md",
                "contents": "# 15 Greedy Algorithms\n\nGreedy algorithms are used for optimization problems. They make a sequence of choices, and at each step, make the choice that looks best at the moment\u2014a locally optimal choice\u2014hoping it leads to a globally optimal solution. This chapter explores problems where this strategy works. This chapter builds upon dynamic programming concepts from Chapter 14.\n\n## 15.1 An activity-selection problem\n\n**Problem definition**: Schedule a resource among several competing activities. Given a set $S = \\{a_1, a_2, \n\n\n, a_n\\}$ of $n$ proposed activities that wish to use a common resource, which can serve only one activity at a time. Each activity $a_i$ has a start time $s_i$ and a finish time $f_i$, with $0 \\le s_i < f_i < \\infty$. If selected, $a_i$ takes place during $[s_i, f_i)$. Activities $a_i$ and $a_j$ are *compatible* if $[s_i, f_i)$ and $[s_j, f_j)$ do not overlap (i.e., $s_i \\ge f_j$ or $s_j \\ge f_i$). Goal: select a maximum-size subset of mutually compatible activities.\nAssume activities are sorted by monotonically increasing finish times:\n$f_1 \\le f_2 \\le \n\n\n \\le f_n$. (15.1)\n\n**The optimal substructure of the activity-selection problem**:\nLet $S_{ij}$ be the set of activities that start after $a_i$ finishes and finish before $a_j$ starts. If an optimal solution $A_{ij}$ to this subproblem includes activity $a_k$, then it also includes optimal solutions to subproblems $S_{ik}$ and $S_{kj}$.\nLet $c[i,j]$ be the size of an optimal solution for $S_{ij}$.\n$c[i,j] = 0$ if $S_{ij} = \\emptyset$.\n$c[i,j] = \\max_{a_k \\in S_{ij}} \\{c[i,k] + c[k,j] + 1\\}$ if $S_{ij} \\ne \\emptyset$. (15.2)\n(To handle the full problem, add fictitious activities $a_0$ with $f_0=0$ and $a_{n+1}$ with $s_{n+1}=\\infty$, then find optimal solution for $S_{0,n+1}$.)\n\n**Making the greedy choice**:\nInstead of solving all subproblems, intuition suggests choosing an activity that leaves the resource available for as many other activities as possible. Choose the activity in $S$ with the earliest finish time. This is $a_1$ (due to sorting by finish times). This is the greedy choice.\nOnce $a_1$ is chosen, the only remaining subproblem is to find activities that start after $a_1$ finishes. Let $S_k = \\{a_i \\in S : s_i \\ge f_k\\}$. If $a_1$ is chosen, the subproblem is $S_1$.\n\n**Theorem 15.1**\nConsider any nonempty subproblem $S_k$, and let $a_m$ be an activity in $S_k$ with the earliest finish time. Then $a_m$ is included in some maximum-size subset of mutually compatible activities of $S_k$.\nProof: Let $A_k$ be a maximum-size subset of $S_k$. Let $a_j$ be the activity in $A_k$ with the earliest finish time. If $a_j = a_m$, we are done. If $a_j \\ne a_m$, consider $A'_k = (A_k - \\{a_j\\}) \\cup \\{a_m\\}$. Since $f_m \\le f_j$ and activities in $A_k$ are compatible with $a_j$ (and thus start after $f_j$, or finish before $s_j$), activities in $A_k - \\{a_j\\}$ are compatible with $a_m$. $A'_k$ is a set of mutually compatible activities, and $|A'_k| = |A_k|$. Thus $A'_k$ is also a maximum-size set and includes $a_m$.\nThis theorem implies we can repeatedly choose the activity with the earliest finish time that is compatible with previously chosen activities.\n\n**A recursive greedy algorithm**:\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15.1 RECURSIVE-ACTIVITY-SELECTOR]] takes sorted arrays $s, f$, index $k$ (defining $S_k$), and $n$. It returns a maximum-size set of compatible activities in $S_k$.\nInitial call: `RECURSIVE-ACTIVITY-SELECTOR(s, f, 0, n)` (with $f_0=0$).\nThe algorithm finds the first activity $a_m$ in $S_k$ that finishes (i.e., $s_m \\ge f_k$). It then recursively calls itself for $S_m$. Time: $\\Theta(n)$ if activities are already sorted, as each activity is examined once across all calls.\n\n**An iterative greedy algorithm**:\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15.1 GREEDY-ACTIVITY-SELECTOR]] implements the strategy iteratively. It selects $a_1$, then scans for the next activity $a_m$ compatible with $a_1$ (i.e., $s_m \\ge f_1$), adds it, and repeats with $a_m$ as the last-added activity. Variable $k$ tracks index of last activity added. Time: $\\Theta(n)$ if sorted.\n\n## 15.2 Elements of the greedy strategy\n\nA greedy algorithm makes a sequence of locally optimal choices.\nSteps typically followed (more direct than the detailed DP-to-greedy conversion):\n1. Cast the optimization problem as one where a choice is made, leaving one subproblem.\n2. Prove that there is always an optimal solution that makes the greedy choice (safety of greedy choice).\n3. Demonstrate optimal substructure: show that after making the greedy choice, the remaining subproblem, if solved optimally and combined with the greedy choice, yields an optimal solution to the original problem.\n\n**Greedy-choice property**:\nA globally optimal solution can be arrived at by making a locally optimal (greedy) choice. Unlike DP, where choices depend on subproblem solutions, a greedy choice is made *before* solving subproblems. Greedy algorithms are often top-down.\nProving this property typically involves showing that a greedy choice can be substituted into an optimal solution without worsening it.\n\n**Optimal substructure**:\nAn optimal solution to the problem contains optimal solutions to subproblems. This is key for both greedy and DP. For greedy, usually one assumes the greedy choice was made to arrive at a subproblem, and then argues that an optimal solution to this subproblem, combined with the greedy choice, yields an optimal solution to the original problem.\n\n**Greedy versus dynamic programming**:\nBoth exploit optimal substructure.\n-   **0-1 knapsack problem**: $n$ items, item $i$ worth $v_i$, weighs $w_i$. Knapsack capacity $W$. Choose items to maximize total value without exceeding $W$. Each item either taken or not. Requires DP.\n-   **Fractional knapsack problem**: Same, but can take fractions of items. Solvable by a greedy strategy: compute $v_i/w_i$ (value per pound) for each item. Take as much as possible of item with highest $v_i/w_i$. If capacity remains, take next highest, etc. Time $O(n \text{lg} n)$ for sorting.\nThe 0-1 knapsack problem does not have the greedy-choice property. Taking the item with highest $v_i/w_i$ might not lead to an optimal solution because it might not fill the knapsack efficiently with remaining capacity. (Figure 15.3 in book).\n\n## 15.3 Huffman codes\n\nHuffman codes are used for data compression, achieving savings of 20-90%.\n**Problem**: Given a set of characters and their frequencies, find a binary character code (unique binary string for each character) that minimizes the total bits for a file.\n-   **Fixed-length code**: If $n$ characters, $\\lceil \\lg n \\rceil$ bits per character.\n-   **Variable-length code**: Frequent characters get short codewords, infrequent ones get long ones.\n\n**Prefix-free codes** (or prefix codes):\nNo codeword is a prefix of another. Simplifies decoding: identify initial codeword, translate, repeat on remainder. E.g., if 'a'=0, 'b'=10, 'c'=11, then 01011 decodes to 'abc'.\nPrefix-free codes can be represented by full binary trees, where leaves are characters. Codeword is path from root (0 for left, 1 for right). Optimal codes correspond to full binary trees.\nCost of a tree $T$: $B(T) = \\sum_{c \\in C} c.\text{freq} \\cdot d_T(c)$, where $d_T(c)$ is depth of leaf $c$. (15.4)\n\n**Constructing a Huffman code**:\nHuffman's greedy algorithm builds an optimal prefix-free code.\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C15.3 HUFFMAN]] procedure:\nInput: Set $C$ of $n$ characters, each $c$ with $c.\text{freq}$.\n1. Create a min-priority queue $Q$ of characters, keyed by frequency.\n2. For $i=1$ to $n-1$:\n   a. Allocate a new node $z$.\n   b. $z.\text{left} = x = \text{EXTRACT-MIN}(Q)$.\n   c. $z.\text{right} = y = \text{EXTRACT-MIN}(Q)$.\n   d. $z.\text{freq} = x.\text{freq} + y.\text{freq}$.\n   e. INSERT$(Q,z)$.\n3. Return EXTRACT-MIN$(Q)$ (the root of the tree).\nProcess is bottom-up merging. $n-1$ merges. If $Q$ is a binary min-heap, total time is $O(n \\lg n)$.\n(Figure 15.6 in the book shows steps for an example).\n\n**Correctness of Huffman's algorithm**:\nRelies on greedy-choice property and optimal substructure.\n\n**Lemma 15.2 (Greedy-choice property for Huffman codes)**\nLet $C$ be an alphabet, each $c \\in C$ has frequency $c.\text{freq}$. Let $x, y$ be two characters in $C$ with lowest frequencies. Then there exists an optimal prefix-free code for $C$ in which codewords for $x, y$ have same length and differ only in last bit.\nProof idea: Take any optimal tree $T$. Let $a,b$ be sibling leaves of maximum depth. Assume $x.\text{freq} \\le a.\text{freq}$ and $y.\text{freq} \\le b.\text{freq}$ (WLOG, after possible swaps of $x,y$ and $a,b$). Swap $x$ with $a$ to get $T'$, then $y$ with $b$ in $T'$ to get $T''$. Cost $B(T'') \\le B(T') \\le B(T)$. Since $T$ is optimal, $B(T'')=B(T)$. $T''$ is optimal and has $x,y$ as deep siblings.\nThis lemma implies that merging the two lowest-frequency characters is a safe first step.\n\n**Lemma 15.3 (Optimal-substructure property for Huffman codes)**\nLet $C, x, y$ be as above. Let $C' = (C - \\{x,y\\}) \\cup \\{z\\}$, where $z$ is a new character with $z.\text{freq} = x.\text{freq} + y.\text{freq}$. Let $T'$ be any tree representing an optimal prefix-free code for $C'$. Then the tree $T$, formed by replacing leaf $z$ in $T'$ with an internal node having $x, y$ as children, represents an optimal prefix-free code for $C$.\nProof idea: Show $B(T) = B(T') + x.\text{freq} + y.\text{freq}$. If $T$ were not optimal for $C$, let $T''$ be an optimal tree for $C$. By Lemma 15.2, $T''$ has $x,y$ as siblings. Construct $T'''$ from $T''$ by replacing $x,y$ and their parent with leaf $z$. Then $B(T''') = B(T'') - x.\text{freq} - y.\text{freq} < B(T) - x.\text{freq} - y.\text{freq} = B(T')$, contradicting optimality of $T'$.\n\n**Theorem 15.4**\nProcedure HUFFMAN produces an optimal prefix-free code.\nProof follows from Lemmas 15.2 and 15.3 by induction.\n\n## 15.4 Offline caching\n\n**Problem**: Managing a cache of $k$ blocks to minimize cache misses for a known sequence of $n$ memory requests $b_1, b_2, \n\n\n, b_n$. This is the *offline* version.\nWhen a requested block $b_i$ is not in cache (a miss) and cache is full, a block must be evicted.\nGreedy strategy: **furthest-in-future**. Evict the block in cache whose *next* access in the request sequence is furthest in the future. If a block in cache is never requested again, it's an ideal candidate.\n\n**Optimal substructure of offline caching**:\nLet subproblem $(C, i)$ be processing requests $b_i, \n\n\n, b_n$ with initial cache configuration $C$. An optimal solution $S$ for $(C, i)$ makes a decision for $b_i$. If $C'$ is the cache after processing $b_i$, the remaining decisions $S'$ must be optimal for $(C', i+1)$.\nLet $\text{miss}(C,i)$ be min misses for subproblem $(C,i)$.\nIf $b_i \\in C$ (hit): $\text{miss}(C,i) = \text{miss}(C, i+1)$.\nIf $b_i \\notin C$ (miss):\n  If $|C| < k$: $\text{miss}(C,i) = 1 + \text{miss}(C \\cup \\{b_i\\}, i+1)$.\n  If $|C| = k$: $\text{miss}(C,i) = 1 + \\min_{x \\in C} \\{\text{miss}((C - \\{x\\}) \\cup \\{b_i\\}, i+1)\\}$.\nBase cases for $i=n$: $\text{miss}(C,n)=0$ if $b_n \\in C$, else $1$.\n\n**Greedy-choice property for offline caching**:\n\n**Theorem 15.5**\nConsider subproblem $(C,i)$ where cache $C$ is full ($|C|=k$) and $b_i \\notin C$. Let $z$ be the block in $C$ whose next access is furthest in the future. Evicting $z$ is part of some optimal strategy.\nProof idea: Let $S$ be an optimal strategy that evicts block $x \\ne z$ when $b_i$ is requested. Construct strategy $S'$ that evicts $z$ instead. We need to show $S'$ is no worse than $S$.\n- Up to the first request after $b_i$ for either $x$ or $z$, $S'$ can mimic $S$ or do better.\n- If $b_j$ is the next request for $x$ ($j>i$): $S$ has $x$. $S'$ has $z$. If $b_j=x$, $S$ hits. $S'$ misses. $S'$ must evict something. If $S'$ evicts $z$, then $S, S'$ caches become same. If $S'$ evicts other $w$, $S, S'$ caches differ by $w, z$. If $b_j \ne x$, then if $S$ evicts $y \ne z$, $S'$ can evict $y$. If $S$ evicts $z$, $S'$ hits on $z$. This part of the proof is complex, involving showing $S'$ makes no more misses than $S$ by careful exchange argument based on when $x$ and $z$ are next requested. The key is that $z$ is needed later than $x$ (or not at all), so keeping $x$ over $z$ is not better than keeping $z$ over $x$ if an eviction is forced for $b_i$ and $x$ is not $b_i$.\nThe detailed proof shows that any divergence in cache contents between $S$ and $S'$ can be managed such that $S'$ has at most as many misses as $S$ until their cache states realign, or $S'$ performs strictly better if $S$ ever evicts $z$ when $S'$ still holds $z$ and has a hit.\n\nThe furthest-in-future strategy combined with optimal substructure ensures it minimizes cache misses."
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques/C16 Amortized Analysis.md",
                "contents": "# 16 Amortized Analysis\n\nAmortized analysis is a technique to average the time required to perform a sequence of data-structure operations over all operations performed. It guarantees the average performance of each operation in the worst case, without involving probability (unlike average-case analysis). Even if a single operation in a sequence is expensive, the average cost can be small.\nThis chapter covers three common techniques: aggregate analysis, the accounting method, and the potential method.\n\n## 16.1 Aggregate analysis\n\nIn aggregate analysis, we show that for all $n$, a sequence of $n$ operations takes worst-case time $T(n)$ in total. The average cost, or *amortized cost*, per operation is then $T(n)/n$. This amortized cost applies to each operation, even if there are different types of operations.\n\n**Stack operations**:\nConsider a stack with PUSH, POP (both $O(1)$ actual cost), and a new operation MULTIPOP$(S, k)$ which pops $k$ items (or all items if stack size $s < k$).\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16.1 MULTIPOP]] procedure:\nActual cost of MULTIPOP$(S,k)$ is $\\min(s,k)$ POP operations.\nIn a sequence of $n$ PUSH, POP, MULTIPOP operations on an initially empty stack:\nA single MULTIPOP can cost $O(n)$. Naive analysis: $n$ operations, each $O(n)$, so $O(n^2)$ total.\nAggregate analysis: An object can be popped only if it was pushed. Total POPs (including within MULTIPOPs) $\\le$ total PUSHs. Total PUSHs $\\le n$. So, total cost of all operations is $O(n)$.\nThe amortized cost per operation is $O(n)/n = O(1)$.\n\n**Incrementing a binary counter**:\nConsider a $k$-bit binary counter $A[0 \n\n k-1]$ counting up from 0. $A[0]$ is LSB.\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16.1 INCREMENT]] procedure:\nCost of INCREMENT is number of bits flipped.\nWorst case for one INCREMENT is $O(k)$ (e.g., $011\n\n1 \\to 100\n\n0$). Naive analysis for $n$ INCREMENTs: $O(nk)$.\nAggregate analysis: $A[0]$ flips $n$ times. $A[1]$ flips $\\lfloor n/2 \\rfloor$ times. $A[i]$ flips $\\lfloor n/2^i \\rfloor$ times.\nTotal flips for $n$ INCREMENTs starting from 0: $\\sum_{i=0}^{k-1} \\lfloor n/2^i \\rfloor < \\sum_{i=0}^{\\infty} n/2^i = n \\sum_{i=0}^{\\infty} (1/2)^i = 2n$.\nTotal cost is $O(n)$. Amortized cost per INCREMENT is $O(n)/n = O(1)$.\n\n## 16.2 The accounting method\n\nAssign different charges (amortized costs) to different operations. If amortized cost $\\hat{c}_i >$ actual cost $c_i$, the difference is stored as *credit* on specific objects in the data structure. Credit can pay for later operations where $\\hat{c}_j < c_j$.\nTotal amortized cost must be an upper bound on total actual cost for any sequence: $\\sum \\hat{c}_i \\ge \\sum c_i$. (16.1)\nTotal credit $(\\sum \\hat{c}_i - \\sum c_i)$ must always be non-negative.\n\n**Stack operations**:\nActual costs: PUSH 1, POP 1, MULTIPOP $\\min(s,k)$.\nAmortized costs: PUSH $2, POP $0, MULTIPOP $0.\n- PUSH: Charge $2. Actual cost is $1. Store $1 credit on the pushed item.\n- POP: Charge $0. Actual cost is $1. Use $1 credit from the popped item to pay.\n- MULTIPOP: Charge $0. Actual cost $\\min(s,k)$. Use $1 credit from each of the $\\min(s,k)$ items popped.\nSince each item on stack has $1 credit, and stack size is non-negative, total credit is non-negative. Total amortized cost for $n$ operations is $O(n)$, so total actual cost is $O(n)$.\n\n**Incrementing a binary counter**:\nCost is $1 per bit flip.\nAmortized cost for setting a bit from 0 to 1 is $2.\n- When bit is set 0 $\\to$ 1: Charge $2. Actual cost is $1. Store $1 credit on this bit.\n- When bit is reset 1 $\\to$ 0: Charge $0. Actual cost is $1. Use $1 credit from this bit to pay.\nINCREMENT operation: Resets $t$ bits (1 $\\to$ 0) and sets at most one bit (0 $\\to$ 1).\nCost of resetting $t$ bits is paid by credits on those bits. Cost of setting one bit is $2. So, amortized cost of INCREMENT $\\le 2$.\nTotal credit is number of 1s in counter $\\times$ $1, always non-negative. Total amortized cost for $n$ operations is $O(n)$, so total actual cost $O(n)$.\n\n## 16.3 The potential method\n\nPrepaid work is stored as *potential energy* of the data structure as a whole.\nLet $D_0$ be initial state. $D_i$ is state after $i$-th operation. $c_i$ is actual cost of $i$-th operation.\nPotential function $\\Phi: \\{\text{states}\\} \\to \\mathbb{R}$. $\\Phi(D_i)$ is potential of state $D_i$.\nAmortized cost $\\hat{c}_i$ of $i$-th operation: $\\hat{c}_i = c_i + \\Phi(D_i) - \\Phi(D_{i-1})$. (16.2)\nTotal amortized cost for $n$ operations: $\\sum_{i=1}^n \\hat{c}_i = \\sum_{i=1}^n (c_i + \\Phi(D_i) - \\Phi(D_{i-1})) = (\\sum_{i=1}^n c_i) + \\Phi(D_n) - \\Phi(D_0)$. (16.3)\nIf $\\Phi(D_n) \\ge \\Phi(D_0)$ for all $n$, then $\\sum \\hat{c}_i$ is an upper bound on $\\sum c_i$. Often, $\\Phi(D_0)=0$ and $\\Phi(D_i) \\ge 0$ for all $i$.\nIf $\\Phi(D_i) - \\Phi(D_{i-1}) > 0$, operation $i$ is overcharged, potential increases.\nIf $\\Phi(D_i) - \\Phi(D_{i-1}) < 0$, operation $i$ is undercharged, potential decrease pays for it.\n\n**Stack operations**:\n$\\Phi(\text{stack}) =$ number of objects on stack. Initially empty, $\\Phi(D_0)=0$. Since stack size $\\ge 0$, $\\Phi(D_i) \\ge 0$ always.\n- PUSH: Stack size $s$. $c_i=1$. $\\Phi(D_i) - \\Phi(D_{i-1}) = (s+1) - s = 1$. $\\hat{c}_i = 1+1=2$.\n- POP: Stack size $s$. $c_i=1$. $\\Phi(D_i) - \\Phi(D_{i-1}) = (s-1) - s = -1$. $\\hat{c}_i = 1-1=0$.\n- MULTIPOP$(S,k)$: Pops $k' = \\min(s,k)$ items. $c_i=k'$. $\\Phi(D_i) - \\Phi(D_{i-1}) = (s-k') - s = -k'$. $\\hat{c}_i = k' - k' = 0$.\nAll amortized costs $O(1)$. Total actual cost $O(n)$.\n\n**Incrementing a binary counter**:\n$\\Phi(\text{counter}) = b_i =$ number of 1s in counter after $i$-th operation. Initial counter is all 0s, $\\Phi(D_0)=0$. $b_i \\ge 0$ always.\n$i$-th INCREMENT resets $t_i$ bits (1 $\\to$ 0) and sets at most one bit (0 $\\to$ 1).\nActual cost $c_i \\le t_i + 1$.\nNumber of 1s after $i$-th op: $b_i \\le b_{i-1} - t_i + 1$.\nPotential change: $\\Phi(D_i) - \\Phi(D_{i-1}) = b_i - b_{i-1} \\le (b_{i-1} - t_i + 1) - b_{i-1} = 1 - t_i$.\nAmortized cost: $\\hat{c}_i = c_i + \\Phi(D_i) - \\Phi(D_{i-1}) \\le (t_i+1) + (1-t_i) = 2$.\nAmortized cost $O(1)$. Total actual cost $O(n)$.\nIf counter doesn't start at 0, $\\sum c_i = \\sum \\hat{c}_i - \\Phi(D_n) + \\Phi(D_0) \\le 2n - b_n + b_0$. If $b_0 \\le k$, this is $O(n)$ if $n = \\Omega(k)$.\n\n## 16.4 Dynamic tables\n\nProblem of expanding and contracting tables (arrays) when number of items changes.\nLoad factor $\\alpha(T) = T.\text{num} / T.\text{size}$. (Items / slots). Empty table size 0, $\\alpha=1$.\n\n**16.4.1 Table expansion**\nAssume only insertions. If table is full ($\\\text{num} == \\\text{size}$), expand by allocating a new table (usually double size), copying items, freeing old table.\n[[PartIV Advanced Design and Analysis Techniques Algorithms.md#C16.4 TABLE-INSERT]] procedure.\nCost $c_i$ of $i$-th insertion: $1$ if table not full. $i$ if table full (1 for new item, $i-1$ to copy old items, since old table had $i-1$ items and new size is $2(i-1)$).\nAn expansion occurs when $i-1$ is a power of 2 (if $T.\text{size}$ starts at 1 and doubles).\n\n*Aggregate analysis*:\nTotal cost $\\sum_{i=1}^n c_i \\le n + \\sum_{j=0}^{\\lfloor \\lg n \\rfloor} 2^j < n + 2n = 3n$. (Sum is over powers of 2 up to $n-1$. Cost of $2^j$-th expansion is $2^j$. This is cost when $i-1=2^j$, so $i=2^j+1$. Item $2^j+1$ is being inserted, table size was $2^j$, $2^j$ items copied, 1 item inserted.)\nThe cost of $i$-th operation if it causes expansion is $i$. Expansion happens for $i = 1, 2, 3, 5, 9, \n\n\n$, i.e., when $i-1$ is $0, 1, 2, 4, 8, \n\n\n$. So cost is $(i-1)$ to copy, $1$ to insert. Sum of costs is $\\sum_{i=1}^n 1 + \\sum_{j=0}^{\\lfloor \\lg(n-1) \\rfloor} 2^j \\le n + (2(n-1)+1) < 3n$.\nAmortized cost is $3n/n = O(1)$.\n\n*Accounting method*:\nCharge $3 amortized cost for each TABLE-INSERT.\n- $1 pays for elementary insertion of the item.\n- $1 is stored as credit on the item itself (to pay for moving it when table expands).\n- $1 is stored as credit on some item already in the table (to pay for moving that item when table expands).\nWhen table size is $m$ with $m/2$ items (just after expansion), no credit. After $m/2$ more insertions, table is full, size $m$, $m$ items. Each of these $m$ items has $1 credit. Expansion costs $m$ (to copy $m$ items). These credits pay for it. New table size $2m$, $m$ items. The new $m$ items copied over lose their credit. New potential for $m$ items is 0. The $m$ items inserted after the previous expansion have paid for themselves and one old item each.\n\n*Potential method*:\n$\\Phi(T) = 2 \n\n T.\text{num} - T.\text{size}$. (16.4) (This should be $2(T.\text{num} - T.\text{size}/2)$ to match $\\Phi=0$ when $T.\text{num} = T.\text{size}/2$). Let's assume $\\Phi(T) = 2 \n\n T.\text{num} - T.\text{size}$ when $T.\text{size} > 0$, and $\\Phi(T)=0$ if $T.\text{size}=0$.\nAssume $T.\text{size}$ is always a power of 2. Immediately after expansion, $T.\text{num} = T.\text{size}/2$. So $\\Phi(T) = 2(T.\text{size}/2) - T.\text{size} = 0$.\nIf $i$-th insertion does not trigger expansion: $T_i.\text{num} = T_{i-1}.\text{num} + 1$, $T_i.\text{size} = T_{i-1}.\text{size}$.\n$c_i=1$. $\\Delta\\Phi_i = \\Phi(T_i) - \\Phi(T_{i-1}) = (2 T_i.\text{num} - T_i.\text{size}) - (2 T_{i-1}.\text{num} - T_{i-1}.\text{size}) = 2(T_i.\text{num} - T_{i-1}.\text{num}) = 2$.\n$\\hat{c}_i = c_i + \\Delta\\Phi_i = 1+2=3$.\nIf $i$-th insertion triggers expansion: $T_{i-1}.\text{num} = T_{i-1}.\text{size} = \text{num}_{i-1}$. Cost $c_i = \text{num}_{i-1} + 1 = T_{i-1}.\text{num} + 1 = i$.\nOld potential $\\Phi(T_{i-1}) = 2 T_{i-1}.\text{num} - T_{i-1}.\text{size} = 2 \text{num}_{i-1} - \text{num}_{i-1} = \text{num}_{i-1} = i-1$.\nNew table $T_i$: $T_i.\text{size} = 2 T_{i-1}.\text{size} = 2 \text{num}_{i-1}$. $T_i.\text{num} = T_{i-1}.\text{num} + 1 = \text{num}_{i-1} + 1$.\n$T_i.\text{num} = (\text{num}_{i-1}+1)$. $T_i.\text{size} = 2 \text{num}_{i-1}$. So $T_i.\text{num} = T_i.\text{size}/2 + 1$.\nPotential after expansion and insertion: $\\Phi(T_i) = 2 T_i.\text{num} - T_i.\text{size} = 2(\text{num}_{i-1}+1) - 2\text{num}_{i-1} = 2$.\n$\\Delta\\Phi_i = \\Phi(T_i) - \\Phi(T_{i-1}) = 2 - (i-1) = 3-i$.\n$\\hat{c}_i = c_i + \\Delta\\Phi_i = i + (3-i) = 3$.\nAmortized cost per insertion is $O(1)$.\n\n**16.4.2 Table expansion and contraction**\nTo save space, contract table if load factor too small. Halving size when $\\alpha < 1/2$ can lead to thrashing (sequence of insert/delete near boundary causes repeated expand/contract, $O(n)$ per op).\nStrategy: Double size on insert if full ($\\\text{num} == \\\text{size}$). Halve size on delete if $\\alpha < 1/4$. So $\\alpha$ is always $\\ge 1/4$. After expansion or contraction, $\\alpha=1/2$.\nPotential function (Figure 16.6 in book):\n$\\Phi(T) = 2 \n\n T.\text{num} - T.\text{size}$ if $\\alpha(T) \\ge 1/2$.\n$\\Phi(T) = T.\text{size}/2 - T.\text{num}$ if $\\alpha(T) < 1/2$. (16.5, matching page 468 of CLRS 4th ed uses $T.\text{size}/2 - T.\text{num}$, not $T.\text{size} - 2 \n\n T.\text{num}$)\n$\\Phi(T)=0$ if $T.\text{size}=0$ or if $\\alpha(T)=1/2$.\n$\\Phi(T) \\ge 0$ always.\n\nAnalysis of $\\hat{c}_i = c_i + \\Delta\\Phi_i$ for various cases (no expand/contract, expand, contract, crossing $\\alpha=1/2$ boundary):\n- Insert, no expansion, $\\alpha_{i-1} \\ge 1/2$: $\\hat{c}_i=3$.\n- Insert, no expansion, $\\alpha_i < 1/2$: $\\hat{c}_i=0$.\n- Delete, no contraction, $\\alpha_{i-1} \\ge 1/2$: $\\hat{c}_i=-1$.\n- Delete, no contraction, $\\alpha_i < 1/2$: $\\hat{c}_i=2$.\n- Insert causing expansion (takes $\\alpha$ from 1 to 1/2): $\\hat{c}_i=3$.\n- Delete causing contraction (takes $\\alpha$ from 1/4 to 1/2): $\\hat{c}_i=1$.\n- Insert crossing $\\alpha=1/2$ (e.g. from $1/2-\\epsilon$ to $1/2$): $num_{i-1} = size/2 - 1$, $num_i = size/2$. $\\Phi_{i-1} = size/2 - (size/2-1) = 1$. $\\Phi_i = 2(size/2) - size = 0$. $\\Delta\\Phi_i = -1$. $c_i=1$. $\\hat{c}_i = 1-1=0$.\n- Delete crossing $\\alpha=1/2$ (e.g. from $1/2$ to $1/2-\\epsilon$): $num_{i-1} = size/2$, $num_i = size/2 - 1$. $\\Phi_{i-1} = 0$. $\\Phi_i = size/2 - (size/2-1) = 1$. $\\Delta\\Phi_i = 1$. $c_i=1$. $\\hat{c}_i = 1+1=2$.\nAll amortized costs are $O(1)$. Total time for $n$ operations is $O(n)$."
            },
            {
                "file": "PartIV Advanced Design and Analysis Techniques Algorithms.md",
                "contents": "# C14 Dynamic Programming Algorithms\n\n## 14.1 CUT-ROD\n```\nCUT-ROD(p, n)\n1  if n == 0\n2      return 0\n3  q = -\u221e\n4  for i = 1 to n\n5      q = max(q, p[i] + CUT-ROD(p, n - i))\n6  return q\n```\n\n## 14.1 MEMOIZED-CUT-ROD\n```\nMEMOIZED-CUT-ROD(p, n)\n1  let r[0..n] be a new array\n2  for i = 0 to n\n3      r[i] = -\u221e\n4  return MEMOIZED-CUT-ROD-AUX(p, n, r)\n```\n\n## 14.1 MEMOIZED-CUT-ROD-AUX\n```\nMEMOIZED-CUT-ROD-AUX(p, n, r)\n1  if r[n] \u2265 0\n2      return r[n]\n3  if n == 0\n4      q = 0\n5  else q = -\u221e\n6      for i = 1 to n\n7          q = max(q, p[i] + MEMOIZED-CUT-ROD-AUX(p, n - i, r))\n8  r[n] = q\n9  return q\n```\n\n## 14.1 BOTTOM-UP-CUT-ROD\n```\nBOTTOM-UP-CUT-ROD(p, n)\n1  let r[0..n] be a new array\n2  r[0] = 0\n3  for j = 1 to n\n4      q = -\u221e\n5      for i = 1 to j\n6          q = max(q, p[i] + r[j - i])\n7      r[j] = q\n8  return r[n]\n```\n\n## 14.1 EXTENDED-BOTTOM-UP-CUT-ROD\n```\nEXTENDED-BOTTOM-UP-CUT-ROD(p, n)\n1  let r[0..n] and s[1..n] be new arrays\n2  r[0] = 0\n3  for j = 1 to n\n4      q = -\u221e\n5      for i = 1 to j\n6          if q < p[i] + r[j-i]\n7              q = p[i] + r[j-i]\n8              s[j] = i\n9      r[j] = q\n10 return r and s\n```\n\n## 14.1 PRINT-CUT-ROD-SOLUTION\n```\nPRINT-CUT-ROD-SOLUTION(p, n)\n1  (r, s) = EXTENDED-BOTTOM-UP-CUT-ROD(p, n)\n2  while n > 0\n3      print s[n]\n4      n = n - s[n]\n```\n\n## 14.2 RECTANGULAR-MATRIX-MULTIPLY\n```\nRECTANGULAR-MATRIX-MULTIPLY(A, B, C, p, q, r)\n1  for i = 1 to p\n2      for j = 1 to r\n3          // C[i,j] = 0 // if computing C = A * B from scratch\n4          for k = 1 to q\n5              C[i,j] = C[i,j] + A[i,k] * B[k,j]\n```\n\n## 14.2 MATRIX-CHAIN-ORDER\n```\nMATRIX-CHAIN-ORDER(p, n)\n1  let m[1..n, 1..n] and s[1..n-1, 2..n] be new tables\n2  for i = 1 to n\n3      m[i,i] = 0\n4  for l = 2 to n  // l is the chain length\n5      for i = 1 to n - l + 1\n6          j = i + l - 1\n7          m[i,j] = \u221e\n8          for k = i to j - 1\n9              q = m[i,k] + m[k+1,j] + p[i-1]*p[k]*p[j]\n10             if q < m[i,j]\n11                m[i,j] = q\n12                s[i,j] = k\n13 return m and s\n```\n\n## 14.2 PRINT-OPTIMAL-PARENS\n```\nPRINT-OPTIMAL-PARENS(s, i, j)\n1  if i == j\n2      print \"A\"i\n3  else print \"(\"\n4      PRINT-OPTIMAL-PARENS(s, i, s[i,j])\n5      PRINT-OPTIMAL-PARENS(s, s[i,j] + 1, j)\n6      print \")\"\n```\n\n## 14.3 RECURSIVE-MATRIX-CHAIN\n```\nRECURSIVE-MATRIX-CHAIN(p, i, j)\n1  if i == j\n2      return 0\n3  m[i,j] = \u221e\n4  for k = i to j - 1\n5      q = RECURSIVE-MATRIX-CHAIN(p, i, k) + \n           RECURSIVE-MATRIX-CHAIN(p, k + 1, j) + \n           p[i-1]*p[k]*p[j]\n6      if q < m[i,j]\n7          m[i,j] = q\n8  return m[i,j]\n```\n\n## 14.3 MEMOIZED-MATRIX-CHAIN\n```\nMEMOIZED-MATRIX-CHAIN(p, n)\n1  let m[1..n, 1..n] be a new table\n2  for i = 1 to n\n3      for j = i to n\n4          m[i,j] = \u221e\n5  return LOOKUP-CHAIN(m, p, 1, n)\n```\n\n## 14.3 LOOKUP-CHAIN\n```\nLOOKUP-CHAIN(m, p, i, j)\n1  if m[i,j] < \u221e\n2      return m[i,j]\n3  if i == j\n4      m[i,j] = 0\n5  else for k = i to j - 1\n6          q = LOOKUP-CHAIN(m, p, i, k) + \n               LOOKUP-CHAIN(m, p, k + 1, j) + \n               p[i-1]*p[k]*p[j]\n7          if q < m[i,j]\n8              m[i,j] = q\n9  return m[i,j]\n```\n\n## 14.4 LCS-LENGTH\n```\nLCS-LENGTH(X, Y, m, n)\n1  let b[1..m, 1..n] and c[0..m, 0..n] be new tables\n2  for i = 1 to m\n3      c[i,0] = 0\n4  for j = 0 to n\n5      c[0,j] = 0\n6  for i = 1 to m\n7      for j = 1 to n\n8          if X[i] == Y[j]  // Assuming 1-based indexing for X, Y\n9              c[i,j] = c[i-1,j-1] + 1\n10             b[i,j] = \"\u2196\"\n11         elseif c[i-1,j] \u2265 c[i,j-1]\n12             c[i,j] = c[i-1,j]\n13             b[i,j] = \"\u2191\"\n14         else c[i,j] = c[i,j-1]\n15             b[i,j] = \"\u2190\"\n16 return c and b\n```\n\n## 14.4 PRINT-LCS\n```\nPRINT-LCS(b, X, i, j) // Assuming 1-based indexing for X\n1  if i == 0 or j == 0\n2      return\n3  if b[i,j] == \"\u2196\"\n4      PRINT-LCS(b, X, i-1, j-1)\n5      print X[i]\n6  elseif b[i,j] == \"\u2191\"\n7      PRINT-LCS(b, X, i-1, j)\n8  else PRINT-LCS(b, X, i, j-1)\n```\n\n## 14.5 OPTIMAL-BST\n```\nOPTIMAL-BST(p, q, n)\n1  let e[1..n+1, 0..n], w[1..n+1, 0..n], and root[1..n, 1..n] be new tables\n2  for i = 1 to n + 1\n3      e[i, i-1] = q[i-1]\n4      w[i, i-1] = q[i-1]\n5  for l = 1 to n\n6      for i = 1 to n - l + 1\n7          j = i + l - 1\n8          e[i,j] = \u221e\n9          w[i,j] = w[i,j-1] + p[j] + q[j]\n10         for r = i to j\n11             t = e[i,r-1] + e[r+1,j] + w[i,j]\n12             if t < e[i,j]\n13                e[i,j] = t\n14                root[i,j] = r\n15 return e and root\n```\n\n# C15 Greedy Algorithms Algorithms\n\n## 15.1 RECURSIVE-ACTIVITY-SELECTOR\n```\nRECURSIVE-ACTIVITY-SELECTOR(s, f, k, n)\n1  m = k + 1\n2  while m \u2264 n and s[m] < f[k]  // find the first activity in S_k to finish\n3      m = m + 1\n4  if m \u2264 n\n5      return {a_m} \u222a RECURSIVE-ACTIVITY-SELECTOR(s, f, m, n)\n6  else return \u00d8\n```\n\n## 15.1 GREEDY-ACTIVITY-SELECTOR\n```\nGREEDY-ACTIVITY-SELECTOR(s, f, n)\n1  A = {a_1}\n2  k = 1\n3  for m = 2 to n\n4      if s[m] \u2265 f[k]    // is a_m in S_k?\n5          A = A \u222a {a_m} // yes, so choose it\n6          k = m         // and continue from there\n7  return A\n```\n\n## 15.3 HUFFMAN\n```\nHUFFMAN(C)\n1  n = |C|\n2  Q = C  // min-priority queue initialized with characters in C\n3  for i = 1 to n - 1\n4      allocate a new node z\n5      x = EXTRACT-MIN(Q)\n6      y = EXTRACT-MIN(Q)\n7      z.left = x\n8      z.right = y\n9      z.freq = x.freq + y.freq\n10     INSERT(Q, z)\n11 return EXTRACT-MIN(Q) // the root of the tree is the only node left\n```\n\n# C16 Amortized Analysis Algorithms\n\n## 16.1 MULTIPOP\n```\nMULTIPOP(S, k)\n1  while not STACK-EMPTY(S) and k > 0\n2      POP(S)\n3      k = k - 1\n```\n\n## 16.1 INCREMENT\n```\nINCREMENT(A, k) // Array A is 0-indexed, k is number of bits\n1  i = 0\n2  while i < k and A[i] == 1\n3      A[i] = 0\n4      i = i + 1\n5  if i < k\n6      A[i] = 1\n```\n\n## 16.4 TABLE-INSERT\n```\nTABLE-INSERT(T, x)\n1  if T.size == 0\n2      allocate T.table with 1 slot\n3      T.size = 1\n4  if T.num == T.size\n5      allocate new-table with 2 * T.size slots\n6      insert all items in T.table into new-table\n7      free T.table\n8      T.table = new-table\n9      T.size = 2 * T.size\n10 insert x into T.table\n11 T.num = T.num + 1\n```"
            }
        ]
    }
]